[NbConvertApp] Searching ['/home/rain/exp_env/.venv/etc/jupyter', '/home/rain/.jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files
[NbConvertApp] Looking for jupyter_config in /etc/jupyter
[NbConvertApp] Looking for jupyter_config in /usr/local/etc/jupyter
[NbConvertApp] Looking for jupyter_config in /home/rain/.jupyter
[NbConvertApp] Looking for jupyter_config in /home/rain/exp_env/.venv/etc/jupyter
[NbConvertApp] Looking for jupyter_nbconvert_config in /etc/jupyter
[NbConvertApp] Looking for jupyter_nbconvert_config in /usr/local/etc/jupyter
[NbConvertApp] Looking for jupyter_nbconvert_config in /home/rain/.jupyter
[NbConvertApp] Looking for jupyter_nbconvert_config in /home/rain/exp_env/.venv/etc/jupyter
[NbConvertApp] Looping through config variables with prefix "JUPYTER_NBCONVERT"
[NbConvertApp] Converting notebook /home/rain/exp_env/src/work/translate/trans.ipynb to notebook
[NbConvertApp] Notebook name is 'trans'
[NbConvertApp] Applying preprocessor: ExecutePreprocessor
[NbConvertApp] Instantiating kernel 'Python 3 (ipykernel)' with kernel provisioner: local-provisioner
[NbConvertApp] Starting kernel: ['/home/rain/exp_env/.venv/bin/python', '-m', 'ipykernel_launcher', '-f', '/tmp/tmpay2io2jo.json', '--HistoryManager.hist_file=:memory:']
[NbConvertApp] Connecting to: tcp://127.0.0.1:58953
[NbConvertApp] connecting iopub channel to tcp://127.0.0.1:49111
[NbConvertApp] Connecting to: tcp://127.0.0.1:49111
[NbConvertApp] connecting shell channel to tcp://127.0.0.1:35167
[NbConvertApp] Connecting to: tcp://127.0.0.1:35167
[NbConvertApp] connecting stdin channel to tcp://127.0.0.1:38317
[NbConvertApp] Connecting to: tcp://127.0.0.1:38317
[NbConvertApp] connecting heartbeat channel to tcp://127.0.0.1:58673
[NbConvertApp] connecting control channel to tcp://127.0.0.1:58953
[NbConvertApp] Connecting to: tcp://127.0.0.1:58953
[NbConvertApp] Executing cell:
import pandas as pd
from exp_env.transformers.models import transformer
import sentencepiece as spm
import torch
from dataclasses import dataclass
import numpy as np
import gc
#default„Åßbf16„Çí‰Ωø„ÅÜ„Çà„ÅÜ„Å´„Åô„Çã
torch.set_default_dtype(torch.bfloat16)
torch.backends.cudnn.benchmark = True
# torch.backends.cudnn.allow_tf32 = True
dev_mode = False
import random
from exp_env.utils.utils import fix_seed
fix_seed(42)
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'import pandas as pd\nfrom exp_env.transformers.models import transformer\nimport sentencepiece as spm\nimport torch\nfrom dataclasses import dataclass\nimport numpy as np\nimport gc\n#default„Åßbf16„Çí‰Ωø„ÅÜ„Çà„ÅÜ„Å´„Åô„Çã\ntorch.set_default_dtype(torch.bfloat16)\ntorch.backends.cudnn.benchmark = True\n# torch.backends.cudnn.allow_tf32 = True\ndev_mode = False\nimport random\nfrom exp_env.utils.utils import fix_seed\nfix_seed(42)', 'execution_count': 1}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
from tokenizers.implementations import SentencePieceUnigramTokenizer
from transformers import PreTrainedTokenizerFast
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"
def load_tokenizer(model_name):
    tokenizer = SentencePieceUnigramTokenizer.from_spm(model_name)
    return PreTrainedTokenizerFast(tokenizer_object=tokenizer._tokenizer, pad_token='[PAD]', bos_token='[BOS]', eos_token='[EOS]', unk_token='[UNK]')

jp_tokenizer = load_tokenizer('tokenizer/jesc_jp.model')
en_tokenizer = load_tokenizer('tokenizer/jesc_en.model')
tokend=jp_tokenizer(text=['„Åì„Çì„Å´„Å°„ÅØüòÉ'],  truncation=True, padding='max_length' , max_length=10, return_tensors='pt')
print(jp_tokenizer.decode(tokend['input_ids'][0]))
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'from tokenizers.implementations import SentencePieceUnigramTokenizer\nfrom transformers import PreTrainedTokenizerFast\nimport os\nos.environ["TOKENIZERS_PARALLELISM"] = "false"\ndef load_tokenizer(model_name):\n    tokenizer = SentencePieceUnigramTokenizer.from_spm(model_name)\n    return PreTrainedTokenizerFast(tokenizer_object=tokenizer._tokenizer, pad_token=\'[PAD]\', bos_token=\'[BOS]\', eos_token=\'[EOS]\', unk_token=\'[UNK]\')\n\njp_tokenizer = load_tokenizer(\'tokenizer/jesc_jp.model\')\nen_tokenizer = load_tokenizer(\'tokenizer/jesc_en.model\')\ntokend=jp_tokenizer(text=[\'„Åì„Çì„Å´„Å°„ÅØüòÉ\'],  truncation=True, padding=\'max_length\' , max_length=10, return_tensors=\'pt\')\nprint(jp_tokenizer.decode(tokend[\'input_ids\'][0]))', 'execution_count': 2}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': '/home/rain/exp_env/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '„Åì„Çì„Å´„Å°„ÅØ<0xF0><0x9F><0x98><0x83>[PAD][PAD][PAD][PAD][PAD]\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
train_df = pd.read_table("/home/rain/exp_env/src/work/translate/data/all/concat",sep="\t",header=None)#.sample(frac=0.14)
if dev_mode:
    train_df = train_df.sample(2000)
train_x = train_df[1].values.tolist()
train_y = train_df[0].values.tolist()
train_y = ['[BOS] ' + text + ' [EOS]' for text in train_y]
test_df = pd.read_table("/home/rain/exp_env/src/work/translate/data/cleaned_jesc/test",sep="\t",header=None)
test_x = test_df[1].values.tolist()
test_y = test_df[0].values.tolist()
test_y = ['[BOS] ' + text + ' [EOS]' for text in test_y]
dev_df = pd.read_table("/home/rain/exp_env/src/work/translate/data/cleaned_jesc/dev",sep="\t",header=None)
dev_x = dev_df[1].values.tolist()
dev_y = dev_df[0].values.tolist()
dev_y = ['[BOS] ' + text + ' [EOS]' for text in dev_y]

test_x = test_x+dev_x
test_y = test_y+dev_y

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'train_df = pd.read_table("/home/rain/exp_env/src/work/translate/data/all/concat",sep="\\t",header=None)#.sample(frac=0.14)\nif dev_mode:\n    train_df = train_df.sample(2000)\ntrain_x = train_df[1].values.tolist()\ntrain_y = train_df[0].values.tolist()\ntrain_y = [\'[BOS] \' + text + \' [EOS]\' for text in train_y]\ntest_df = pd.read_table("/home/rain/exp_env/src/work/translate/data/cleaned_jesc/test",sep="\\t",header=None)\ntest_x = test_df[1].values.tolist()\ntest_y = test_df[0].values.tolist()\ntest_y = [\'[BOS] \' + text + \' [EOS]\' for text in test_y]\ndev_df = pd.read_table("/home/rain/exp_env/src/work/translate/data/cleaned_jesc/dev",sep="\\t",header=None)\ndev_x = dev_df[1].values.tolist()\ndev_y = dev_df[0].values.tolist()\ndev_y = [\'[BOS] \' + text + \' [EOS]\' for text in dev_y]\n\ntest_x = test_x+dev_x\ntest_y = test_y+dev_y\n', 'execution_count': 3}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
class Masker:
    def __init__(self, device):
        self.device = device
    @staticmethod
    @torch.jit.script
    def _subsequent_mask(x: torch.Tensor) -> torch.Tensor:
        batch_size = x.size(0)
        max_len = x.size(1)
        return torch.tril(torch.ones(batch_size, max_len, max_len)).eq(0).to('cpu')
    @staticmethod
    @torch.jit.script
    def _pad_mask(mask: torch.Tensor) -> torch.Tensor:
        return mask.eq(0).unsqueeze(1).repeat(1, mask.size(1), 1).to('cpu')
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': "class Masker:\n    def __init__(self, device):\n        self.device = device\n    @staticmethod\n    @torch.jit.script\n    def _subsequent_mask(x: torch.Tensor) -> torch.Tensor:\n        batch_size = x.size(0)\n        max_len = x.size(1)\n        return torch.tril(torch.ones(batch_size, max_len, max_len)).eq(0).to('cpu')\n    @staticmethod\n    @torch.jit.script\n    def _pad_mask(mask: torch.Tensor) -> torch.Tensor:\n        return mask.eq(0).unsqueeze(1).repeat(1, mask.size(1), 1).to('cpu')", 'execution_count': 4}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
from typing import Iterator
from exp_env.data.data_maker_base import DataMaker, DataMakerSpec

@dataclass
class SpmTokenizerSpec(DataMakerSpec):
    model_name: str
    max_length: int

class SpmTokenizer(DataMaker):
    def __init__(self, spec: SpmTokenizerSpec):
        self.tokenizer = load_tokenizer(spec.model_name)
        self.max_length = spec.max_length

    def make(self, data: list[str]) -> tuple[torch.Tensor, torch.Tensor]:
        tokenized=self.tokenizer(data,  truncation=True, padding='max_length' , max_length=self.max_length, return_tensors='pt')
        return tokenized['input_ids'], tokenized['attention_mask'] # type: ignore
    
class TransformerTrainDataMaker(DataMaker):
    def __init__(self, src_spec: SpmTokenizerSpec, tgt_spec: SpmTokenizerSpec, masker: Masker):
        self.src_tokenizer = SpmTokenizer(src_spec)
        self.tgt_tokenizer = SpmTokenizer(tgt_spec)
        self.masker = masker
    @torch.no_grad()
    def make(self, src_texts: list[str], tgt_texts: list[str],is_train:bool=False) -> tuple[Iterator[tuple[torch.Tensor, torch.Tensor]], Iterator[tuple[torch.Tensor, torch.Tensor]]]:
        src_ids, encoder_mask = self.src_tokenizer.make(src_texts)
        encoder_mask = self.masker._pad_mask(encoder_mask)
        tgt_ids, decoder_mask = self.tgt_tokenizer.make(tgt_texts)
        decoder_mask = self.masker._pad_mask(decoder_mask)
        if is_train:
            seq_mask = self.masker._subsequent_mask(tgt_ids)
            decoder_mask = torch.logical_or(
                seq_mask,
                decoder_mask,
            )
        src=(src_ids, encoder_mask)
        tgt=(tgt_ids, decoder_mask)
        return src, tgt # type: ignore
max_length = 256
maker=TransformerTrainDataMaker(SpmTokenizerSpec('tokenizer/jesc_jp.model', max_length), SpmTokenizerSpec('tokenizer/jesc_en.model', max_length), Masker('cpu'))
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': "from typing import Iterator\nfrom exp_env.data.data_maker_base import DataMaker, DataMakerSpec\n\n@dataclass\nclass SpmTokenizerSpec(DataMakerSpec):\n    model_name: str\n    max_length: int\n\nclass SpmTokenizer(DataMaker):\n    def __init__(self, spec: SpmTokenizerSpec):\n        self.tokenizer = load_tokenizer(spec.model_name)\n        self.max_length = spec.max_length\n\n    def make(self, data: list[str]) -> tuple[torch.Tensor, torch.Tensor]:\n        tokenized=self.tokenizer(data,  truncation=True, padding='max_length' , max_length=self.max_length, return_tensors='pt')\n        return tokenized['input_ids'], tokenized['attention_mask'] # type: ignore\n    \nclass TransformerTrainDataMaker(DataMaker):\n    def __init__(self, src_spec: SpmTokenizerSpec, tgt_spec: SpmTokenizerSpec, masker: Masker):\n        self.src_tokenizer = SpmTokenizer(src_spec)\n        self.tgt_tokenizer = SpmTokenizer(tgt_spec)\n        self.masker = masker\n    @torch.no_grad()\n    def make(self, src_texts: list[str], tgt_texts: list[str],is_train:bool=False) -> tuple[Iterator[tuple[torch.Tensor, torch.Tensor]], Iterator[tuple[torch.Tensor, torch.Tensor]]]:\n        src_ids, encoder_mask = self.src_tokenizer.make(src_texts)\n        encoder_mask = self.masker._pad_mask(encoder_mask)\n        tgt_ids, decoder_mask = self.tgt_tokenizer.make(tgt_texts)\n        decoder_mask = self.masker._pad_mask(decoder_mask)\n        if is_train:\n            seq_mask = self.masker._subsequent_mask(tgt_ids)\n            decoder_mask = torch.logical_or(\n                seq_mask,\n                decoder_mask,\n            )\n        src=(src_ids, encoder_mask)\n        tgt=(tgt_ids, decoder_mask)\n        return src, tgt # type: ignore\nmax_length = 256\nmaker=TransformerTrainDataMaker(SpmTokenizerSpec('tokenizer/jesc_jp.model', max_length), SpmTokenizerSpec('tokenizer/jesc_en.model', max_length), Masker('cpu'))", 'execution_count': 5}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
from torch.utils import data as data

@dataclass
class BaseTrainPipelineConfig:
    batch_size:int=64
    max_epochs:int|None=2
    skip=0
    is_running:bool=True
    debug:bool=False
@dataclass
class BaseTrainGenKeys:
    epoch='epoch'
    train_loss='train_loss'
    test_loss='test_loss'
    batch_x='batch_x'
    batch_y='batch_y'
    batch_step='batch_step'
    max_batch_step='max_batch_step'

class BaseTrainPipeline:
    def __init__(self,config:BaseTrainPipelineConfig):
        self.config=config
    def epoch_gen(self):
        i=0
        while True:
            yield {BaseTrainGenKeys.epoch:i}
            i+=1
            if self.config.max_epochs is not None:
                if i>=self.config.max_epochs:
                    break
            if not self.config.is_running:
                break

    def batch_gen(self,G,dataset,scale=1):
        dataloader=data.DataLoader(dataset,batch_size=self.config.batch_size*scale,shuffle=True,num_workers=6,pin_memory=True)
        all_step=0
        for g in G:
            for i,(batch_X,batch_y) in enumerate(dataloader):
                if all_step<self.config.skip:
                    all_step+=1
                    continue
                g[BaseTrainGenKeys.batch_x]=batch_X
                g[BaseTrainGenKeys.batch_y]=batch_y
                g[BaseTrainGenKeys.batch_step]=i+1
                g[BaseTrainGenKeys.max_batch_step]=len(dataloader)
                all_step+=1
                yield g
                if self.config.debug:
                    break
                gc.collect()
    
    def train_loop_gen(self,train_dataset):
        g=self.epoch_gen()
        g=self.batch_gen(g,train_dataset)
        return g
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': "from torch.utils import data as data\n\n@dataclass\nclass BaseTrainPipelineConfig:\n    batch_size:int=64\n    max_epochs:int|None=2\n    skip=0\n    is_running:bool=True\n    debug:bool=False\n@dataclass\nclass BaseTrainGenKeys:\n    epoch='epoch'\n    train_loss='train_loss'\n    test_loss='test_loss'\n    batch_x='batch_x'\n    batch_y='batch_y'\n    batch_step='batch_step'\n    max_batch_step='max_batch_step'\n\nclass BaseTrainPipeline:\n    def __init__(self,config:BaseTrainPipelineConfig):\n        self.config=config\n    def epoch_gen(self):\n        i=0\n        while True:\n            yield {BaseTrainGenKeys.epoch:i}\n            i+=1\n            if self.config.max_epochs is not None:\n                if i>=self.config.max_epochs:\n                    break\n            if not self.config.is_running:\n                break\n\n    def batch_gen(self,G,dataset,scale=1):\n        dataloader=data.DataLoader(dataset,batch_size=self.config.batch_size*scale,shuffle=True,num_workers=6,pin_memory=True)\n        all_step=0\n        for g in G:\n            for i,(batch_X,batch_y) in enumerate(dataloader):\n                if all_step<self.config.skip:\n                    all_step+=1\n                    continue\n                g[BaseTrainGenKeys.batch_x]=batch_X\n                g[BaseTrainGenKeys.batch_y]=batch_y\n                g[BaseTrainGenKeys.batch_step]=i+1\n                g[BaseTrainGenKeys.max_batch_step]=len(dataloader)\n                all_step+=1\n                yield g\n                if self.config.debug:\n                    break\n                gc.collect()\n    \n    def train_loop_gen(self,train_dataset):\n        g=self.epoch_gen()\n        g=self.batch_gen(g,train_dataset)\n        return g", 'execution_count': 6}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:


class TransformerTrainDataLoader(data.Dataset):
    def __init__(self, data_maker: TransformerTrainDataMaker, src_texts: list[str], tgt_texts: list[str],is_train:bool=False):
        self.data_maker = data_maker
        self.src_texts = src_texts
        self.tgt_texts = tgt_texts
        self.is_train=is_train

    def __len__(self):
        return len(self.src_texts)

    def __getitem__(self, idx):
        return self.data_maker.make(self.src_texts[idx], self.tgt_texts[idx],is_train=self.is_train)

train_dataset = TransformerTrainDataLoader(maker, train_x, train_y,is_train=True)
test_dataset = TransformerTrainDataLoader(maker, test_x, test_y,is_train=False)
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '\n\nclass TransformerTrainDataLoader(data.Dataset):\n    def __init__(self, data_maker: TransformerTrainDataMaker, src_texts: list[str], tgt_texts: list[str],is_train:bool=False):\n        self.data_maker = data_maker\n        self.src_texts = src_texts\n        self.tgt_texts = tgt_texts\n        self.is_train=is_train\n\n    def __len__(self):\n        return len(self.src_texts)\n\n    def __getitem__(self, idx):\n        return self.data_maker.make(self.src_texts[idx], self.tgt_texts[idx],is_train=self.is_train)\n\ntrain_dataset = TransformerTrainDataLoader(maker, train_x, train_y,is_train=True)\ntest_dataset = TransformerTrainDataLoader(maker, test_x, test_y,is_train=False)', 'execution_count': 7}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 7
[NbConvertApp] Executing cell:
len(train_x)//128
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'len(train_x)//128', 'execution_count': 8}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '79139'}, 'metadata': {}, 'execution_count': 8}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
import math


class CosineAnnealingLR(torch.optim.lr_scheduler._LRScheduler):
    def __init__(
        self,
        optimizer: torch.optim.Optimizer,
        warmup_epochs: int,
        max_epochs: int,
        warmup_start_lr: float = 0.00001,
        eta_min: float = 0.00001,
        last_epoch: int = -1,
    ):
        """
        Args:
            optimizer (torch.optim.Optimizer):
                ÊúÄÈÅ©ÂåñÊâãÊ≥ï„Ç§„É≥„Çπ„Çø„É≥„Çπ
            warmup_epochs (int):
                linear warmup„ÇíË°å„ÅÜepochÊï∞
            max_epochs (int):
                cosineÊõ≤Á∑ö„ÅÆÁµÇ‰∫Ü„Å´Áî®„ÅÑ„Çã Â≠¶Áøí„ÅÆepochÊï∞
            warmup_start_lr (float):
                linear warmup 0 epochÁõÆ„ÅÆÂ≠¶ÁøíÁéá
            eta_min (float):
                cosineÊõ≤Á∑ö„ÅÆ‰∏ãÈôê
            last_epoch (int):
                cosineÊõ≤Á∑ö„ÅÆ‰ΩçÁõ∏„Ç™„Éï„Çª„ÉÉ„Éà
        Â≠¶ÁøíÁéá„Çímax_epochs„Å´Ëá≥„Çã„Åæ„Åß„Ç≥„Çµ„Ç§„É≥Êõ≤Á∑ö„Å´Ê≤ø„Å£„Å¶„Çπ„Ç±„Ç∏„É•„Éº„É´„Åô„Çã
        epoch 0„Åã„Çâwarmup_epochs„Åæ„Åß„ÅÆÂ≠¶ÁøíÊõ≤Á∑ö„ÅØÁ∑öÂΩ¢warmup„Åå„Åã„Åã„Çã
        https://pytorch-lightning-bolts.readthedocs.io/en/stable/schedulers/warmup_cosine_annealing.html
        """
        self.warmup_epochs = warmup_epochs
        self.max_epochs = max_epochs
        self.warmup_start_lr = warmup_start_lr
        self.eta_min = eta_min
        super().__init__(optimizer, last_epoch)
        return None

    def get_lr(self):
        if self.last_epoch == 0:
            return [self.warmup_start_lr] * len(self.base_lrs)
        if self.last_epoch < self.warmup_epochs:
            return [
                group["lr"] + (base_lr - self.warmup_start_lr) / (self.warmup_epochs - 1)
                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)
            ]
        if self.last_epoch == self.warmup_epochs:
            return self.base_lrs
        if (self.last_epoch - 1 - self.max_epochs) % (2 * (self.max_epochs - self.warmup_epochs)) == 0:
            return [
                group["lr"] + (base_lr - self.eta_min) * (1 - math.cos(math.pi / (self.max_epochs - self.warmup_epochs))) / 2
                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)
            ]

        return [
            (1 + math.cos(math.pi * (self.last_epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)))
            / (1 + math.cos(math.pi * (self.last_epoch - self.warmup_epochs - 1) / (self.max_epochs - self.warmup_epochs)))
            * (group["lr"] - self.eta_min)
            + self.eta_min
            for group in self.optimizer.param_groups
        ]
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'import math\n\n\nclass CosineAnnealingLR(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(\n        self,\n        optimizer: torch.optim.Optimizer,\n        warmup_epochs: int,\n        max_epochs: int,\n        warmup_start_lr: float = 0.00001,\n        eta_min: float = 0.00001,\n        last_epoch: int = -1,\n    ):\n        """\n        Args:\n            optimizer (torch.optim.Optimizer):\n                ÊúÄÈÅ©ÂåñÊâãÊ≥ï„Ç§„É≥„Çπ„Çø„É≥„Çπ\n            warmup_epochs (int):\n                linear warmup„ÇíË°å„ÅÜepochÊï∞\n            max_epochs (int):\n                cosineÊõ≤Á∑ö„ÅÆÁµÇ‰∫Ü„Å´Áî®„ÅÑ„Çã Â≠¶Áøí„ÅÆepochÊï∞\n            warmup_start_lr (float):\n                linear warmup 0 epochÁõÆ„ÅÆÂ≠¶ÁøíÁéá\n            eta_min (float):\n                cosineÊõ≤Á∑ö„ÅÆ‰∏ãÈôê\n            last_epoch (int):\n                cosineÊõ≤Á∑ö„ÅÆ‰ΩçÁõ∏„Ç™„Éï„Çª„ÉÉ„Éà\n        Â≠¶ÁøíÁéá„Çímax_epochs„Å´Ëá≥„Çã„Åæ„Åß„Ç≥„Çµ„Ç§„É≥Êõ≤Á∑ö„Å´Ê≤ø„Å£„Å¶„Çπ„Ç±„Ç∏„É•„Éº„É´„Åô„Çã\n        epoch 0„Åã„Çâwarmup_epochs„Åæ„Åß„ÅÆÂ≠¶ÁøíÊõ≤Á∑ö„ÅØÁ∑öÂΩ¢warmup„Åå„Åã„Åã„Çã\n        https://pytorch-lightning-bolts.readthedocs.io/en/stable/schedulers/warmup_cosine_annealing.html\n        """\n        self.warmup_epochs = warmup_epochs\n        self.max_epochs = max_epochs\n        self.warmup_start_lr = warmup_start_lr\n        self.eta_min = eta_min\n        super().__init__(optimizer, last_epoch)\n        return None\n\n    def get_lr(self):\n        if self.last_epoch == 0:\n            return [self.warmup_start_lr] * len(self.base_lrs)\n        if self.last_epoch < self.warmup_epochs:\n            return [\n                group["lr"] + (base_lr - self.warmup_start_lr) / (self.warmup_epochs - 1)\n                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)\n            ]\n        if self.last_epoch == self.warmup_epochs:\n            return self.base_lrs\n        if (self.last_epoch - 1 - self.max_epochs) % (2 * (self.max_epochs - self.warmup_epochs)) == 0:\n            return [\n                group["lr"] + (base_lr - self.eta_min) * (1 - math.cos(math.pi / (self.max_epochs - self.warmup_epochs))) / 2\n                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)\n            ]\n\n        return [\n            (1 + math.cos(math.pi * (self.last_epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)))\n            / (1 + math.cos(math.pi * (self.last_epoch - self.warmup_epochs - 1) / (self.max_epochs - self.warmup_epochs)))\n            * (group["lr"] - self.eta_min)\n            + self.eta_min\n            for group in self.optimizer.param_groups\n        ]', 'execution_count': 9}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
import matplotlib.pyplot as plt
sche=CosineAnnealingLR(torch.optim.Adam([torch.nn.Parameter(torch.tensor(1.0))]),warmup_epochs=10,max_epochs=100)
sche.last_epoch=999
lrs=[]
for i in range(300):
    sche.step()
    lrs.append(sche.get_lr()[0])
plt.plot(lrs)
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'import matplotlib.pyplot as plt\nsche=CosineAnnealingLR(torch.optim.Adam([torch.nn.Parameter(torch.tensor(1.0))]),warmup_epochs=10,max_epochs=100)\nsche.last_epoch=999\nlrs=[]\nfor i in range(300):\n    sche.step()\n    lrs.append(sche.get_lr()[0])\nplt.plot(lrs)', 'execution_count': 10}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': '/home/rain/exp_env/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "\n'}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '[<matplotlib.lines.Line2D at 0x7efcdda98d10>]'}, 'metadata': {}, 'execution_count': 10}
[NbConvertApp] msg_type: display_data
[NbConvertApp] content: {'data': {'text/plain': '<Figure size 640x480 with 1 Axes>', 'image/png': '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'}, 'metadata': {}, 'transient': {}}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
import torch
class CossineLRDecay(torch.optim.lr_scheduler._LRScheduler):
    def __init__(self,optimizer,lr_max,lr_min,max_steps,warmup_steps):
        self.lr_max=lr_max
        self.lr_min=lr_min
        self.max_steps=max_steps
        self.warmup_steps=warmup_steps
        super().__init__(optimizer)
    def get_lr(self):
        if self.last_epoch<self.warmup_steps:
            return [self.lr_max/self.warmup_steps*(self.last_epoch+1) for _ in self.optimizer.param_groups]
        elif self.last_epoch<self.max_steps:
            return [self.lr_min+(self.lr_max-self.lr_min)*(1+math.cos(math.pi*(self.last_epoch-self.warmup_steps)/(self.max_steps-self.warmup_steps)))/2 for _ in self.optimizer.param_groups]
        else:
            return [self.lr_min for _ in self.optimizer.param_groups]

sche=CossineLRDecay(torch.optim.Adam([torch.nn.Parameter(torch.tensor(1.0))]),lr_max=0.1,lr_min=0.0001,max_steps=1000,warmup_steps=0)
sche.last_epoch=999
lrs=[]
for i in range(1000):
    sche.step()
    lrs.append(sche.get_lr()[0])
plt.plot(lrs)
    
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'import torch\nclass CossineLRDecay(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(self,optimizer,lr_max,lr_min,max_steps,warmup_steps):\n        self.lr_max=lr_max\n        self.lr_min=lr_min\n        self.max_steps=max_steps\n        self.warmup_steps=warmup_steps\n        super().__init__(optimizer)\n    def get_lr(self):\n        if self.last_epoch<self.warmup_steps:\n            return [self.lr_max/self.warmup_steps*(self.last_epoch+1) for _ in self.optimizer.param_groups]\n        elif self.last_epoch<self.max_steps:\n            return [self.lr_min+(self.lr_max-self.lr_min)*(1+math.cos(math.pi*(self.last_epoch-self.warmup_steps)/(self.max_steps-self.warmup_steps)))/2 for _ in self.optimizer.param_groups]\n        else:\n            return [self.lr_min for _ in self.optimizer.param_groups]\n\nsche=CossineLRDecay(torch.optim.Adam([torch.nn.Parameter(torch.tensor(1.0))]),lr_max=0.1,lr_min=0.0001,max_steps=1000,warmup_steps=0)\nsche.last_epoch=999\nlrs=[]\nfor i in range(1000):\n    sche.step()\n    lrs.append(sche.get_lr()[0])\nplt.plot(lrs)\n    ', 'execution_count': 11}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '[<matplotlib.lines.Line2D at 0x7efcdd9ff150>]'}, 'metadata': {}, 'execution_count': 11}
[NbConvertApp] msg_type: display_data
[NbConvertApp] content: {'data': {'text/plain': '<Figure size 640x480 with 1 Axes>', 'image/png': 'iVBORw0KGgoAAAANSUhEUgAAAk8AAAGdCAYAAAAL2ZfXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAtkUlEQVR4nO3df3BU9b3/8Xd+sLtB3Cw/zG6CCQSNRDGFQGQbf6Flx2Azluidqpk0Eyka9cIVLr0oXAvcaccmA1xnFFG0MxVnaokwKrT8cjJBTIPJgiH8CKGRK7nKRTcMhOwGhQDZ9/ePfjn1lAD5UCANPB8zO3HP53XOfs7HgfOak90lRlVVAAAA0COxvT0BAACAvoTyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYCC+tydwNYpGo/L111/L9ddfLzExMb09HQAA0AOqKh0dHZKSkiKxsee+v0R5ugy+/vprSU1N7e1pAACAi3DgwAG58cYbzzlOeboMrr/+ehH56+K73e5eng0AAOiJSCQiqamp1nX8XChPl8GZX9W53W7KEwAAfcyF3nLDG8YBAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMXFR5Wrp0qQwfPlxcLpf4/X7ZunXrefOrVq2SzMxMcblckpWVJevXr7eNq6rMnz9fkpOTJSEhQQKBgOzbt8+WaWtrk6KiInG73eLxeGTq1Kly7Ngxa/zEiRPyxBNPSFZWlsTHx0tBQUG3c9m8ebOMHTtWnE6n3HzzzbJ8+fJzzru8vFxiYmJk5syZ5z0/AABw7TAuT++9957MmjVLFixYINu3b5fRo0dLXl6eHDp0qNv8p59+KoWFhTJ16lRpaGiQgoICKSgokMbGRiuzcOFCefXVV2XZsmUSDAbluuuuk7y8PDlx4oSVKSoqkj179khlZaWsXbtWqqurpbS01Brv6uqShIQEee655yQQCHQ7l5aWFsnPz5f7779fduzYITNnzpQnn3xSPvroo7Oy27ZtkzfffFN+8IMfmC4RAAC4mqmh8ePH67Rp06znXV1dmpKSomVlZd3mH330Uc3Pz7dt8/v9+vTTT6uqajQaVZ/Pp4sWLbLG29vb1el06ooVK1RVtampSUVEt23bZmU2bNigMTExevDgwbNes6SkRCdPnnzW9ueff15HjRpl2/bYY49pXl6ebVtHR4dmZGRoZWWlTpgwQWfMmNHtuZ1LOBxWEdFwOGy0HwAA6D09vX4b3Xk6efKk1NfX2+7sxMbGSiAQkNra2m73qa2tPetOUF5enpVvaWmRUChkyyQmJorf77cytbW14vF4JCcnx8oEAgGJjY2VYDDY4/lfaC5nTJs2TfLz8895B+vvdXZ2SiQSsT0AAMDVyag8HT58WLq6usTr9dq2e71eCYVC3e4TCoXOmz/z80KZpKQk23h8fLwMGjTonK9rMpdIJCLHjx8XEZGKigrZvn27lJWV9fi4ZWVlkpiYaD1SU1N7vC8AAOhb+LTd9xw4cEBmzJgh7777rrhcrh7vN3fuXAmHw9bjwIEDl3GWAACgN8WbhIcMGSJxcXHS2tpq297a2io+n6/bfXw+33nzZ362trZKcnKyLTNmzBgr8/dvSD99+rS0tbWd83VN5uJ2uyUhIUHq6+vl0KFDMnbsWGu8q6tLqqur5bXXXpPOzk6Ji4s767hOp1OcTmeP5wEAAPouoztPDodDxo0bJ1VVVda2aDQqVVVVkpub2+0+ubm5tryISGVlpZVPT08Xn89ny0QiEQkGg1YmNzdX2tvbpb6+3sps2rRJotGo+P3+Hs//QnOZOHGi7N69W3bs2GE9cnJypKioSHbs2NFtcQIAANcY03eiV1RUqNPp1OXLl2tTU5OWlpaqx+PRUCikqqrFxcU6Z84cK79lyxaNj4/XxYsX6969e3XBggXar18/3b17t5UpLy9Xj8eja9as0V27dunkyZM1PT1djx8/bmUmTZqk2dnZGgwGtaamRjMyMrSwsNA2tz179mhDQ4M+9NBDet9992lDQ4M2NDRY4/v379f+/fvr7Nmzde/evbp06VKNi4vTjRs3nvN8+bQdAADXhp5ev43Lk6rqkiVLNC0tTR0Oh44fP17r6uqssQkTJmhJSYktv3LlSr3lllvU4XDoqFGjdN26dbbxaDSq8+bNU6/Xq06nUydOnKjNzc22zJEjR7SwsFAHDBigbrdbp0yZoh0dHbbMsGHDVETOenzfxx9/rGPGjFGHw6EjRozQt99++7znSnkCAODa0NPrd4yqaq/d9rpKRSIRSUxMlHA4LG63u7enAwAAeqCn128+bQcAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGDgosrT0qVLZfjw4eJyucTv98vWrVvPm1+1apVkZmaKy+WSrKwsWb9+vW1cVWX+/PmSnJwsCQkJEggEZN++fbZMW1ubFBUVidvtFo/HI1OnTpVjx45Z4ydOnJAnnnhCsrKyJD4+XgoKCrqdy+bNm2Xs2LHidDrl5ptvluXLl9vGy8rK5I477pDrr79ekpKSpKCgQJqbm3u+OAAA4KpmXJ7ee+89mTVrlixYsEC2b98uo0ePlry8PDl06FC3+U8//VQKCwtl6tSp0tDQIAUFBVJQUCCNjY1WZuHChfLqq6/KsmXLJBgMynXXXSd5eXly4sQJK1NUVCR79uyRyspKWbt2rVRXV0tpaak13tXVJQkJCfLcc89JIBDodi4tLS2Sn58v999/v+zYsUNmzpwpTz75pHz00UdW5pNPPpFp06ZJXV2dVFZWyqlTp+SBBx6Qb7/91nSpAADA1UgNjR8/XqdNm2Y97+rq0pSUFC0rK+s2/+ijj2p+fr5tm9/v16efflpVVaPRqPp8Pl20aJE13t7erk6nU1esWKGqqk1NTSoium3bNiuzYcMGjYmJ0YMHD571miUlJTp58uSztj///PM6atQo27bHHntM8/Lyznm+hw4dUhHRTz755JyZvxcOh1VENBwO93gfAADQu3p6/Ta683Ty5Empr6+33dmJjY2VQCAgtbW13e5TW1t71p2gvLw8K9/S0iKhUMiWSUxMFL/fb2Vqa2vF4/FITk6OlQkEAhIbGyvBYLDH87/QXLoTDodFRGTQoEHnzHR2dkokErE9AADA1cmoPB0+fFi6urrE6/Xatnu9XgmFQt3uEwqFzps/8/NCmaSkJNt4fHy8DBo06JyvazKXSCQix48fPysfjUZl5syZctddd8ntt99+zuOWlZVJYmKi9UhNTe3xnAAAQN/Cp+3OY9q0adLY2CgVFRXnzc2dO1fC4bD1OHDgwBWaIQAAuNLiTcJDhgyRuLg4aW1ttW1vbW0Vn8/X7T4+n++8+TM/W1tbJTk52ZYZM2aMlfn7N6SfPn1a2trazvm6JnNxu92SkJBg2z59+nTrjek33njjeY/rdDrF6XT2eB4AAKDvMrrz5HA4ZNy4cVJVVWVti0ajUlVVJbm5ud3uk5uba8uLiFRWVlr59PR08fl8tkwkEpFgMGhlcnNzpb29Xerr663Mpk2bJBqNit/v7/H8LzQXkb9+bcL06dPlww8/lE2bNkl6enqPjw8AAK4Bpu9Er6ioUKfTqcuXL9empiYtLS1Vj8ejoVBIVVWLi4t1zpw5Vn7Lli0aHx+vixcv1r179+qCBQu0X79+unv3bitTXl6uHo9H16xZo7t27dLJkydrenq6Hj9+3MpMmjRJs7OzNRgMak1NjWZkZGhhYaFtbnv27NGGhgZ96KGH9L777tOGhgZtaGiwxvfv36/9+/fX2bNn6969e3Xp0qUaFxenGzdutDLPPvusJiYm6ubNm/Wbb76xHt99912P14hP2wEA0Pf09PptXJ5UVZcsWaJpaWnqcDh0/PjxWldXZ41NmDBBS0pKbPmVK1fqLbfcog6HQ0eNGqXr1q2zjUejUZ03b556vV51Op06ceJEbW5utmWOHDmihYWFOmDAAHW73TplyhTt6OiwZYYNG6Yictbj+z7++GMdM2aMOhwOHTFihL799tv2BelmfxE5K3c+lCcAAPqenl6/Y1RVe+GG11UtEolIYmKihMNhcbvdvT0dAADQAz29fvNpOwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAMXVZ6WLl0qw4cPF5fLJX6/X7Zu3Xre/KpVqyQzM1NcLpdkZWXJ+vXrbeOqKvPnz5fk5GRJSEiQQCAg+/bts2Xa2tqkqKhI3G63eDwemTp1qhw7dswaP3HihDzxxBOSlZUl8fHxUlBQ0O1cNm/eLGPHjhWn0yk333yzLF++/B8+PwAAcO0wLk/vvfeezJo1SxYsWCDbt2+X0aNHS15enhw6dKjb/KeffiqFhYUydepUaWhokIKCAikoKJDGxkYrs3DhQnn11Vdl2bJlEgwG5brrrpO8vDw5ceKElSkqKpI9e/ZIZWWlrF27Vqqrq6W0tNQa7+rqkoSEBHnuueckEAh0O5eWlhbJz8+X+++/X3bs2CEzZ86UJ598Uj766KOLPj8AAHBtiVFVNdnB7/fLHXfcIa+99pqIiESjUUlNTZV/+7d/kzlz5pyVf+yxx+Tbb7+VtWvXWtt++MMfypgxY2TZsmWiqpKSkiK/+MUv5D/+4z9ERCQcDovX65Xly5fL448/Lnv37pXbbrtNtm3bJjk5OSIisnHjRvnxj38s//d//ycpKSm213ziiSekvb1dVq9ebdv+wgsvyLp162zF7fHHH5f29nbZuHHjRZ1fdyKRiCQmJko4HBa3292jfS5EVeX4qa5LciwAAPq6hH5xEhMTc0mP2dPrd7zJQU+ePCn19fUyd+5ca1tsbKwEAgGpra3tdp/a2lqZNWuWbVteXp5VbFpaWiQUCtnuFiUmJorf75fa2lp5/PHHpba2Vjwej1WcREQCgYDExsZKMBiUhx9+uEfzr62tPeuuVF5ensycOfOiz09EpLOzUzo7O63nkUikR/MxcfxUl9w2/6MLBwEAuAY0/SpP+juMaswlY/Rru8OHD0tXV5d4vV7bdq/XK6FQqNt9QqHQefNnfl4ok5SUZBuPj4+XQYMGnfN1TeYSiUTk+PHjF3V+IiJlZWWSmJhoPVJTU3s8JwAA0Lf0TmW7ysydO9d2dy0SiVzyApXQL06afpV3SY8JAEBfldAvrtde26g8DRkyROLi4qS1tdW2vbW1VXw+X7f7+Hy+8+bP/GxtbZXk5GRbZsyYMVbm79+wffr0aWlrazvn65rMxe12S0JCgsTFxRmfn4iI0+kUp9PZ43lcjJiYmF67PQkAAP7G6Nd2DodDxo0bJ1VVVda2aDQqVVVVkpub2+0+ubm5tryISGVlpZVPT08Xn89ny0QiEQkGg1YmNzdX2tvbpb6+3sps2rRJotGo+P3+Hs//QnO5mPMDAADXGDVUUVGhTqdTly9frk1NTVpaWqoej0dDoZCqqhYXF+ucOXOs/JYtWzQ+Pl4XL16se/fu1QULFmi/fv109+7dVqa8vFw9Ho+uWbNGd+3apZMnT9b09HQ9fvy4lZk0aZJmZ2drMBjUmpoazcjI0MLCQtvc9uzZow0NDfrQQw/pfffdpw0NDdrQ0GCN79+/X/v376+zZ8/WvXv36tKlSzUuLk43btzY4/PriXA4rCKi4XC4x/sAAIDe1dPrt3F5UlVdsmSJpqWlqcPh0PHjx2tdXZ01NmHCBC0pKbHlV65cqbfccos6HA4dNWqUrlu3zjYejUZ13rx56vV61el06sSJE7W5udmWOXLkiBYWFuqAAQPU7XbrlClTtKOjw5YZNmyYishZj+/7+OOPdcyYMepwOHTEiBH69ttvG51fT1CeAADoe3p6/Tb+nidc2OX4nicAAHB59fT6zb9tBwAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYOCiytPSpUtl+PDh4nK5xO/3y9atW8+bX7VqlWRmZorL5ZKsrCxZv369bVxVZf78+ZKcnCwJCQkSCARk3759tkxbW5sUFRWJ2+0Wj8cjU6dOlWPHjtkyu3btknvuuUdcLpekpqbKwoULbeOnTp2SX/3qV3LTTTeJy+WS0aNHy8aNG22Zrq4umTdvnqSnp0tCQoLcdNNN8utf/1pU1XSZAADA1UgNVVRUqMPh0N/97ne6Z88efeqpp9Tj8Whra2u3+S1btmhcXJwuXLhQm5qa9Je//KX269dPd+/ebWXKy8s1MTFRV69erTt37tSf/OQnmp6ersePH7cykyZN0tGjR2tdXZ3++c9/1ptvvlkLCwut8XA4rF6vV4uKirSxsVFXrFihCQkJ+uabb1qZ559/XlNSUnTdunX6xRdf6Ouvv64ul0u3b99uZV566SUdPHiwrl27VltaWnTVqlU6YMAAfeWVV3q8RuFwWEVEw+Fwj/cBAAC9q6fXb+PyNH78eJ02bZr1vKurS1NSUrSsrKzb/KOPPqr5+fm2bX6/X59++mlVVY1Go+rz+XTRokXWeHt7uzqdTl2xYoWqqjY1NamI6LZt26zMhg0bNCYmRg8ePKiqqq+//roOHDhQOzs7rcwLL7ygI0eOtJ4nJyfra6+9ZpvLI488okVFRdbz/Px8/fnPf37ezIVQngAA6Ht6ev02+rXdyZMnpb6+XgKBgLUtNjZWAoGA1NbWdrtPbW2tLS8ikpeXZ+VbWlokFArZMomJieL3+61MbW2teDweycnJsTKBQEBiY2MlGAxamXvvvVccDoftdZqbm+Xo0aMiItLZ2Skul8s2l4SEBKmpqbGe33nnnVJVVSWff/65iIjs3LlTampq5MEHHzznunR2dkokErE9AADA1cmoPB0+fFi6urrE6/Xatnu9XgmFQt3uEwqFzps/8/NCmaSkJNt4fHy8DBo0yJbp7hjff428vDx5+eWXZd++fRKNRqWyslI++OAD+eabb6x95syZI48//rhkZmZKv379JDs7W2bOnClFRUXnXJeysjJJTEy0HqmpqefMAgCAvu2a+rTdK6+8IhkZGZKZmSkOh0OmT58uU6ZMkdjYvy3DypUr5d1335U//OEPsn37dnnnnXdk8eLF8s4775zzuHPnzpVwOGw9Dhw4cCVOBwAA9IJ4k/CQIUMkLi5OWltbbdtbW1vF5/N1u4/P5ztv/szP1tZWSU5OtmXGjBljZQ4dOmQ7xunTp6Wtrc12nO5e5/uvccMNN8jq1avlxIkTcuTIEUlJSZE5c+bIiBEjrH1mz55t3X0SEcnKypIvv/xSysrKpKSkpNtzdDqd4nQ6ux0DAABXF6M7Tw6HQ8aNGydVVVXWtmg0KlVVVZKbm9vtPrm5uba8iEhlZaWVT09PF5/PZ8tEIhEJBoNWJjc3V9rb26W+vt7KbNq0SaLRqPj9fitTXV0tp06dsr3OyJEjZeDAgbbXd7lcMnToUDl9+rS8//77MnnyZGvsu+++s92JEhGJi4uTaDR64QUCAABXP9N3oldUVKjT6dTly5drU1OTlpaWqsfj0VAopKqqxcXFOmfOHCu/ZcsWjY+P18WLF+vevXt1wYIF3X5Vgcfj0TVr1uiuXbt08uTJ3X5VQXZ2tgaDQa2pqdGMjAzbVxW0t7er1+vV4uJibWxs1IqKCu3fv7/tqwrq6ur0/fff1y+++EKrq6v1Rz/6kaanp+vRo0etTElJiQ4dOtT6qoIPPvhAhwwZos8//3yP14hP2wEA0Pdctq8qUFVdsmSJpqWlqcPh0PHjx2tdXZ01NmHCBC0pKbHlV65cqbfccos6HA4dNWqUrlu3zjYejUZ13rx56vV61el06sSJE7W5udmWOXLkiBYWFuqAAQPU7XbrlClTtKOjw5bZuXOn3n333ep0OnXo0KFaXl5uG9+8ebPeeuut6nQ6dfDgwVpcXGx91cEZkUhEZ8yYoWlpaepyuXTEiBH64osv2r4C4UIoTwAA9D09vX7HqPLV2ZdaJBKRxMRECYfD4na7e3s6AACgB3p6/b6mPm0HAADwj6I8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGLio8rR06VIZPny4uFwu8fv9snXr1vPmV61aJZmZmeJyuSQrK0vWr19vG1dVmT9/viQnJ0tCQoIEAgHZt2+fLdPW1iZFRUXidrvF4/HI1KlT5dixY7bMrl275J577hGXyyWpqamycOFC2/ipU6fkV7/6ldx0003icrlk9OjRsnHjxrPme/DgQfnZz34mgwcPloSEBMnKypLPPvvMZIkAAMBVyrg8vffeezJr1ixZsGCBbN++XUaPHi15eXly6NChbvOffvqpFBYWytSpU6WhoUEKCgqkoKBAGhsbrczChQvl1VdflWXLlkkwGJTrrrtO8vLy5MSJE1amqKhI9uzZI5WVlbJ27Vqprq6W0tJSazwSicgDDzwgw4YNk/r6elm0aJH813/9l7z11ltW5pe//KW8+eabsmTJEmlqapJnnnlGHn74YWloaLAyR48elbvuukv69esnGzZskKamJvnv//5vGThwoOlSAQCAq5EaGj9+vE6bNs163tXVpSkpKVpWVtZt/tFHH9X8/HzbNr/fr08//bSqqkajUfX5fLpo0SJrvL29XZ1Op65YsUJVVZuamlREdNu2bVZmw4YNGhMTowcPHlRV1ddff10HDhyonZ2dVuaFF17QkSNHWs+Tk5P1tddes83lkUce0aKiIts+d999d88W4xzC4bCKiIbD4X/oOAAA4Mrp6fXb6M7TyZMnpb6+XgKBgLUtNjZWAoGA1NbWdrtPbW2tLS8ikpeXZ+VbWlokFArZMomJieL3+61MbW2teDweycnJsTKBQEBiY2MlGAxamXvvvVccDoftdZqbm+Xo0aMiItLZ2Skul8s2l4SEBKmpqbGe//GPf5ScnBz56U9/KklJSZKdnS2//e1vz7sunZ2dEolEbA8AAHB1MipPhw8flq6uLvF6vbbtXq9XQqFQt/uEQqHz5s/8vFAmKSnJNh4fHy+DBg2yZbo7xvdfIy8vT15++WXZt2+fRKNRqayslA8++EC++eYba5/9+/fLG2+8IRkZGfLRRx/Js88+K88995y8884751yXsrIySUxMtB6pqannzAIAgL7tmvq03SuvvCIZGRmSmZkpDodDpk+fLlOmTJHY2L8tQzQalbFjx8pvfvMbyc7OltLSUnnqqadk2bJl5zzu3LlzJRwOW48DBw5cidMBAAC9wKg8DRkyROLi4qS1tdW2vbW1VXw+X7f7+Hy+8+bP/LxQ5u/fkH769Glpa2uzZbo7xvdf44YbbpDVq1fLt99+K19++aX85S9/kQEDBsiIESOsfZKTk+W2226zHefWW2+Vr776qtvzExFxOp3idrttDwAAcHUyKk8Oh0PGjRsnVVVV1rZoNCpVVVWSm5vb7T65ubm2vIhIZWWllU9PTxefz2fLRCIRCQaDViY3N1fa29ulvr7eymzatEmi0aj4/X4rU11dLadOnbK9zsiRI8/6pJzL5ZKhQ4fK6dOn5f3335fJkydbY3fddZc0Nzfb8p9//rkMGzbswgsEAACufqbvRK+oqFCn06nLly/XpqYmLS0tVY/Ho6FQSFVVi4uLdc6cOVZ+y5YtGh8fr4sXL9a9e/fqggULtF+/frp7924rU15erh6PR9esWaO7du3SyZMna3p6uh4/ftzKTJo0SbOzszUYDGpNTY1mZGRoYWGhNd7e3q5er1eLi4u1sbFRKyoqtH///vrmm29ambq6On3//ff1iy++0Orqav3Rj36k6enpevToUSuzdetWjY+P15deekn37dun7777rvbv319///vf93iN+LQdAAB9T0+v38blSVV1yZIlmpaWpg6HQ8ePH691dXXW2IQJE7SkpMSWX7lypd5yyy3qcDh01KhRum7dOtt4NBrVefPmqdfrVafTqRMnTtTm5mZb5siRI1pYWKgDBgxQt9utU6ZM0Y6ODltm586devfdd6vT6dShQ4dqeXm5bXzz5s166623qtPp1MGDB2txcbH1VQff96c//Ulvv/12dTqdmpmZqW+99ZbR+lCeAADoe3p6/Y5RVe3de19Xn0gkIomJiRIOh3n/EwAAfURPr9/X1KftAAAA/lGUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAPxvT2Bq5GqiohIJBLp5ZkAAICeOnPdPnMdPxfK02XQ0dEhIiKpqam9PBMAAGCqo6NDEhMTzzkeoxeqVzAWjUbl66+/luuvv15iYmIu2XEjkYikpqbKgQMHxO12X7Ljwo51vnJY6yuDdb4yWOcr53KttapKR0eHpKSkSGzsud/ZxJ2nyyA2NlZuvPHGy3Z8t9vNH8wrgHW+cljrK4N1vjJY5yvncqz1+e44ncEbxgEAAAxQngAAAAxQnvoQp9MpCxYsEKfT2dtTuaqxzlcOa31lsM5XBut85fT2WvOGcQAAAAPceQIAADBAeQIAADBAeQIAADBAeQIAADBAeepDli5dKsOHDxeXyyV+v1+2bt3a21PqM8rKyuSOO+6Q66+/XpKSkqSgoECam5ttmRMnTsi0adNk8ODBMmDAAPmXf/kXaW1ttWW++uoryc/Pl/79+0tSUpLMnj1bTp8+fSVPpU8pLy+XmJgYmTlzprWNdb50Dh48KD/72c9k8ODBkpCQIFlZWfLZZ59Z46oq8+fPl+TkZElISJBAICD79u2zHaOtrU2KiorE7XaLx+ORqVOnyrFjx670qfzT6urqknnz5kl6erokJCTITTfdJL/+9a9t//YZ63xxqqur5aGHHpKUlBSJiYmR1atX28Yv1bru2rVL7rnnHnG5XJKamioLFy78xyev6BMqKirU4XDo7373O92zZ48+9dRT6vF4tLW1tben1ifk5eXp22+/rY2Njbpjxw798Y9/rGlpaXrs2DEr88wzz2hqaqpWVVXpZ599pj/84Q/1zjvvtMZPnz6tt99+uwYCAW1oaND169frkCFDdO7cub1xSv/0tm7dqsOHD9cf/OAHOmPGDGs763xptLW16bBhw/SJJ57QYDCo+/fv148++kj/53/+x8qUl5drYmKirl69Wnfu3Kk/+clPND09XY8fP25lJk2apKNHj9a6ujr985//rDfffLMWFhb2xin9U3rppZd08ODBunbtWm1padFVq1bpgAED9JVXXrEyrPPFWb9+vb744ov6wQcfqIjohx9+aBu/FOsaDofV6/VqUVGRNjY26ooVKzQhIUHffPPNf2julKc+Yvz48Tpt2jTreVdXl6akpGhZWVkvzqrvOnTokIqIfvLJJ6qq2t7erv369dNVq1ZZmb1796qIaG1trar+9Q96bGyshkIhK/PGG2+o2+3Wzs7OK3sC/+Q6Ojo0IyNDKysrdcKECVZ5Yp0vnRdeeEHvvvvuc45Ho1H1+Xy6aNEia1t7e7s6nU5dsWKFqqo2NTWpiOi2bduszIYNGzQmJkYPHjx4+Sbfh+Tn5+vPf/5z27ZHHnlEi4qKVJV1vlT+vjxdqnV9/fXXdeDAgba/O1544QUdOXLkPzRffm3XB5w8eVLq6+slEAhY22JjYyUQCEhtbW0vzqzvCofDIiIyaNAgERGpr6+XU6dO2dY4MzNT0tLSrDWura2VrKws8Xq9ViYvL08ikYjs2bPnCs7+n9+0adMkPz/ftp4irPOl9Mc//lFycnLkpz/9qSQlJUl2drb89re/tcZbWlokFArZ1joxMVH8fr9trT0ej+Tk5FiZQCAgsbGxEgwGr9zJ/BO78847paqqSj7//HMREdm5c6fU1NTIgw8+KCKs8+Vyqda1trZW7r33XnE4HFYmLy9Pmpub5ejRoxc9P/5h4D7g8OHD0tXVZbuYiIh4vV75y1/+0kuz6rui0ajMnDlT7rrrLrn99ttFRCQUConD4RCPx2PLer1eCYVCVqa7/wdnxvBXFRUVsn37dtm2bdtZY6zzpbN//3554403ZNasWfKf//mfsm3bNnnuuefE4XBISUmJtVbdreX31zopKck2Hh8fL4MGDWKt/785c+ZIJBKRzMxMiYuLk66uLnnppZekqKhIRIR1vkwu1bqGQiFJT08/6xhnxgYOHHhR86M84Zozbdo0aWxslJqamt6eylXnwIEDMmPGDKmsrBSXy9Xb07mqRaNRycnJkd/85jciIpKdnS2NjY2ybNkyKSkp6eXZXT1Wrlwp7777rvzhD3+QUaNGyY4dO2TmzJmSkpLCOl/D+LVdHzBkyBCJi4s76xNJra2t4vP5emlWfdP06dNl7dq18vHHH8uNN95obff5fHLy5Elpb2+35b+/xj6fr9v/B2fG8Ndfyx06dEjGjh0r8fHxEh8fL5988om8+uqrEh8fL16vl3W+RJKTk+W2226zbbv11lvlq6++EpG/rdX5/t7w+Xxy6NAh2/jp06elra2Ntf7/Zs+eLXPmzJHHH39csrKypLi4WP793/9dysrKRIR1vlwu1bperr9PKE99gMPhkHHjxklVVZW1LRqNSlVVleTm5vbizPoOVZXp06fLhx9+KJs2bTrrNu64ceOkX79+tjVubm6Wr776ylrj3Nxc2b17t+0Pa2Vlpbjd7rMuYteqiRMnyu7du2XHjh3WIycnR4qKiqz/Zp0vjbvuuuusr9v4/PPPZdiwYSIikp6eLj6fz7bWkUhEgsGgba3b29ulvr7eymzatEmi0aj4/f4rcBb//L777juJjbVfKuPi4iQajYoI63y5XKp1zc3Nlerqajl16pSVqayslJEjR170r+xEhK8q6CsqKirU6XTq8uXLtampSUtLS9Xj8dg+kYRze/bZZzUxMVE3b96s33zzjfX47rvvrMwzzzyjaWlpumnTJv3ss880NzdXc3NzrfEzH6F/4IEHdMeOHbpx40a94YYb+Aj9BXz/03aqrPOlsnXrVo2Pj9eXXnpJ9+3bp++++672799ff//731uZ8vJy9Xg8umbNGt21a5dOnjy52496Z2dnazAY1JqaGs3IyLjmP0L/fSUlJTp06FDrqwo++OADHTJkiD7//PNWhnW+OB0dHdrQ0KANDQ0qIvryyy9rQ0ODfvnll6p6ada1vb1dvV6vFhcXa2Njo1ZUVGj//v35qoJryZIlSzQtLU0dDoeOHz9e6+rqentKfYaIdPt4++23rczx48f1X//1X3XgwIHav39/ffjhh/Wbb76xHed///d/9cEHH9SEhAQdMmSI/uIXv9BTp05d4bPpW/6+PLHOl86f/vQnvf3229XpdGpmZqa+9dZbtvFoNKrz5s1Tr9erTqdTJ06cqM3NzbbMkSNHtLCwUAcMGKBut1unTJmiHR0dV/I0/qlFIhGdMWOGpqWlqcvl0hEjRuiLL75o++g763xxPv74427/Xi4pKVHVS7euO3fu1LvvvludTqcOHTpUy8vL/+G5x6h+72tSAQAAcF685wkAAMAA5QkAAMAA5QkAAMAA5QkAAMAA5QkAAMAA5QkAAMAA5QkAAMAA5QkAAMAA5QkAAMAA5QkAAMAA5QkAAMAA5QkAAMDA/wOKVjPI1beDsgAAAABJRU5ErkJggg=='}, 'metadata': {}, 'transient': {}}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
import matplotlib.pyplot as plt
import math
max_lr = 6e-4
min_lr = max_lr * 0.1
warmup_steps = 0
max_steps = 840 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens
def get_lr(it):
    # 1) linear warmup for warmup_iters steps
    if it < warmup_steps:
        return max_lr * (it+1) / warmup_steps
    # 2) if it > lr_decay_iters, return min learning rate
    if it > max_steps:
        return min_lr
    # 3) in between, use cosine decay down to min learning rate
    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)
    assert 0 <= decay_ratio <= 1
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0
    return min_lr + coeff * (max_lr - min_lr)

plt.plot([get_lr(i) for i in range(840)])
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'import matplotlib.pyplot as plt\nimport math\nmax_lr = 6e-4\nmin_lr = max_lr * 0.1\nwarmup_steps = 0\nmax_steps = 840 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens\ndef get_lr(it):\n    # 1) linear warmup for warmup_iters steps\n    if it < warmup_steps:\n        return max_lr * (it+1) / warmup_steps\n    # 2) if it > lr_decay_iters, return min learning rate\n    if it > max_steps:\n        return min_lr\n    # 3) in between, use cosine decay down to min learning rate\n    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n    assert 0 <= decay_ratio <= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n    return min_lr + coeff * (max_lr - min_lr)\n\nplt.plot([get_lr(i) for i in range(840)])', 'execution_count': 12}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '[<matplotlib.lines.Line2D at 0x7efcdd9092d0>]'}, 'metadata': {}, 'execution_count': 12}
[NbConvertApp] msg_type: display_data
[NbConvertApp] content: {'data': {'text/plain': '<Figure size 640x480 with 1 Axes>', 'image/png': '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'}, 'metadata': {}, 'transient': {}}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:


@dataclass
class TransformerTrainGenKeys(BaseTrainGenKeys):
    src_ids='src_ids'
    tgt_ids='tgt_ids'
    encoder_mask='encoder_mask'
    decoder_mask='decoder_mask'
    sampled_src='sampled_src'
    sampled_tgt='sampled_tgt'
    sampled_pred='sampled_pred'
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': "\n\n@dataclass\nclass TransformerTrainGenKeys(BaseTrainGenKeys):\n    src_ids='src_ids'\n    tgt_ids='tgt_ids'\n    encoder_mask='encoder_mask'\n    decoder_mask='decoder_mask'\n    sampled_src='sampled_src'\n    sampled_tgt='sampled_tgt'\n    sampled_pred='sampled_pred'", 'execution_count': 13}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
import sys
import wandb

class TrainLogger:
    def __init__(self,cfg):
        self.test_scores=[]
        wandb.init(project='transformer-jesc',config=cfg)
    
    def write(self,g):
        wandb.log({BaseTrainGenKeys.epoch:g[BaseTrainGenKeys.epoch],BaseTrainGenKeys.train_loss:g[BaseTrainGenKeys.train_loss],BaseTrainGenKeys.test_loss:g[BaseTrainGenKeys.test_loss],TransformerTrainGenKeys.sampled_src:g[TransformerTrainGenKeys.sampled_src],TransformerTrainGenKeys.sampled_tgt:g[TransformerTrainGenKeys.sampled_tgt],TransformerTrainGenKeys.sampled_pred:g[TransformerTrainGenKeys.sampled_pred]})


    def log(self, epoch, batch_step, max_batch_step, train_loss, test_loss: float|None):
        if batch_step!=0:
            print("\033[F\033[K",end="")
            print("\033[F\033[K",end="")
        if max_batch_step==0:
            max_batch_step=1
        progress=['=' for _ in range(int(batch_step/max_batch_step*50))]
        progress.append('>')
        progress.extend([' ' for _ in range(50-len(progress))])
        train_loss = f'{train_loss:.4f}'
        if test_loss is None:
            print(f'epoch:{epoch} batch_step:{batch_step}/{max_batch_step} [{"".join(progress)}] train_loss:{train_loss}')
        else:
            self.test_scores.append(test_loss)
            test_loss = f'{test_loss:.4f}'
            print(f'epoch:{epoch} batch_step:{batch_step}/{max_batch_step} [{"".join(progress)}] train_loss:{train_loss} test_loss:{test_loss}')

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'import sys\nimport wandb\n\nclass TrainLogger:\n    def __init__(self,cfg):\n        self.test_scores=[]\n        wandb.init(project=\'transformer-jesc\',config=cfg)\n    \n    def write(self,g):\n        wandb.log({BaseTrainGenKeys.epoch:g[BaseTrainGenKeys.epoch],BaseTrainGenKeys.train_loss:g[BaseTrainGenKeys.train_loss],BaseTrainGenKeys.test_loss:g[BaseTrainGenKeys.test_loss],TransformerTrainGenKeys.sampled_src:g[TransformerTrainGenKeys.sampled_src],TransformerTrainGenKeys.sampled_tgt:g[TransformerTrainGenKeys.sampled_tgt],TransformerTrainGenKeys.sampled_pred:g[TransformerTrainGenKeys.sampled_pred]})\n\n\n    def log(self, epoch, batch_step, max_batch_step, train_loss, test_loss: float|None):\n        if batch_step!=0:\n            print("\\033[F\\033[K",end="")\n            print("\\033[F\\033[K",end="")\n        if max_batch_step==0:\n            max_batch_step=1\n        progress=[\'=\' for _ in range(int(batch_step/max_batch_step*50))]\n        progress.append(\'>\')\n        progress.extend([\' \' for _ in range(50-len(progress))])\n        train_loss = f\'{train_loss:.4f}\'\n        if test_loss is None:\n            print(f\'epoch:{epoch} batch_step:{batch_step}/{max_batch_step} [{"".join(progress)}] train_loss:{train_loss}\')\n        else:\n            self.test_scores.append(test_loss)\n            test_loss = f\'{test_loss:.4f}\'\n            print(f\'epoch:{epoch} batch_step:{batch_step}/{max_batch_step} [{"".join(progress)}] train_loss:{train_loss} test_loss:{test_loss}\')\n', 'execution_count': 14}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:

from adabelief_pytorch import AdaBelief
class SchedulerType:
    CosineAnnealingLR="CosineAnnealingLR"
    CosineAnnealingWarmRestarts="CosineAnnealingWarmRestarts"
    CosineAnnealingWarmUp="CosineAnnealingWarmUp"
    CossineLRDecay="CossineLRDecay"
    StepLR="StepLR"
    NoneType="NoneType"
@dataclass
class CFG(BaseTrainPipelineConfig):
    batch_size:int=128
    skip=13920
    max_epochs:int|None=1
    is_running:bool=True
    debug:bool=False
    max_len: int = maker.src_tokenizer.max_length
    lr: float =0.0007 #0.0010466176# 2.79936e-05 #2.79936e-05*6#
    min_lr: float = 2.79936e-05
    accum_iter: int = 512//128//2
    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'
    embedding_dim: int = 512
    hidden_dim: int = 512
    num_heads: int = 12
    num_blocks: int = 3
    encode_vocab_size: int = maker.src_tokenizer.tokenizer.get_vocab().keys().__len__()
    decode_vocab_size: int = maker.tgt_tokenizer.tokenizer.get_vocab().keys().__len__()
    pad_idx:int=maker.src_tokenizer.tokenizer.pad_token_id # type: ignore
    tokenizer=maker
    optimizer=torch.optim.AdamW#AdaBelief#torch.optim.RAdam#
    default_dtype=torch.bfloat16
    eval_steps:int=60
    scheduler=SchedulerType.CossineLRDecay
    warmup_percent=0.0015

def cfg_to_dict(cfg):
    return {k:v for k,v in cfg.__dict__.items() if not k.startswith('_')}
logger=TrainLogger(cfg_to_dict(CFG()))
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '\nfrom adabelief_pytorch import AdaBelief\nclass SchedulerType:\n    CosineAnnealingLR="CosineAnnealingLR"\n    CosineAnnealingWarmRestarts="CosineAnnealingWarmRestarts"\n    CosineAnnealingWarmUp="CosineAnnealingWarmUp"\n    CossineLRDecay="CossineLRDecay"\n    StepLR="StepLR"\n    NoneType="NoneType"\n@dataclass\nclass CFG(BaseTrainPipelineConfig):\n    batch_size:int=128\n    skip=13920\n    max_epochs:int|None=1\n    is_running:bool=True\n    debug:bool=False\n    max_len: int = maker.src_tokenizer.max_length\n    lr: float =0.0007 #0.0010466176# 2.79936e-05 #2.79936e-05*6#\n    min_lr: float = 2.79936e-05\n    accum_iter: int = 512//128//2\n    device: str = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n    embedding_dim: int = 512\n    hidden_dim: int = 512\n    num_heads: int = 12\n    num_blocks: int = 3\n    encode_vocab_size: int = maker.src_tokenizer.tokenizer.get_vocab().keys().__len__()\n    decode_vocab_size: int = maker.tgt_tokenizer.tokenizer.get_vocab().keys().__len__()\n    pad_idx:int=maker.src_tokenizer.tokenizer.pad_token_id # type: ignore\n    tokenizer=maker\n    optimizer=torch.optim.AdamW#AdaBelief#torch.optim.RAdam#\n    default_dtype=torch.bfloat16\n    eval_steps:int=60\n    scheduler=SchedulerType.CossineLRDecay\n    warmup_percent=0.0015\n\ndef cfg_to_dict(cfg):\n    return {k:v for k,v in cfg.__dict__.items() if not k.startswith(\'_\')}\nlogger=TrainLogger(cfg_to_dict(CFG()))', 'execution_count': 15}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': '\x1b[34m\x1b[1mwandb\x1b[0m: Currently logged in as: \x1b[33myama-yeah\x1b[0m (\x1b[33mgrad-exp\x1b[0m). Use \x1b[1m`wandb login --relogin`\x1b[0m to force relogin\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': '\x1b[34m\x1b[1mwandb\x1b[0m: wandb version 0.17.4 is available!  To upgrade, please run:\n\x1b[34m\x1b[1mwandb\x1b[0m:  $ pip install wandb --upgrade\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': '\x1b[34m\x1b[1mwandb\x1b[0m: Tracking run with wandb version 0.17.3\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': '\x1b[34m\x1b[1mwandb\x1b[0m: Run data is saved locally in \x1b[35m\x1b[1m/home/rain/exp_env/src/work/translate/wandb/run-20240704_131003-m44eoa2g\x1b[0m\n\x1b[34m\x1b[1mwandb\x1b[0m: Run \x1b[1m`wandb offline`\x1b[0m to turn off syncing.\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': '\x1b[34m\x1b[1mwandb\x1b[0m: Syncing run \x1b[33mfearless-shape-113\x1b[0m\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': '\x1b[34m\x1b[1mwandb\x1b[0m: ‚≠êÔ∏è View project at \x1b[34m\x1b[4mhttps://wandb.ai/grad-exp/transformer-jesc\x1b[0m\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': '\x1b[34m\x1b[1mwandb\x1b[0m: üöÄ View run at \x1b[34m\x1b[4mhttps://wandb.ai/grad-exp/transformer-jesc/runs/m44eoa2g\x1b[0m\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:


class TransformerTrainPipeline(BaseTrainPipeline):
    def __init__(self,train_src,train_tgt,test_src,test_tgt,config:CFG=CFG()):
        super().__init__(config)
        self.encoder=transformer.Encoder(
            how_many_block=config.num_blocks,
            how_many_heads=config.num_heads,
            vocab_size=config.encode_vocab_size,
            embedding_dim=config.embedding_dim,
            hidden_dim=config.hidden_dim,
            pad_idx= config.pad_idx,
            max_seq_len=config.max_len,
            layer_norm_eps=1e-6,
        ).to(config.device)
        self.decoder=transformer.Decoder(
            how_many_block=config.num_blocks,
            how_many_heads=config.num_heads,
            vocab_size=config.decode_vocab_size,
            embedding_dim=config.embedding_dim,
            hidden_dim=config.hidden_dim,
            pad_idx= config.pad_idx,
            max_seq_len=config.max_len,
            layer_norm_eps=1e-6,
        ).to(config.device)
        self.optimizer=config.optimizer(list(self.encoder.parameters())[1:]+list(self.decoder.parameters())[1:],lr=config.lr,betas=(0.9,0.95),eps=1e-8,fused=True)#,eps=1e-16, betas=(0.9,0.999), weight_decouple = True, rectify = True)
        self.optimizer2=torch.optim.RAdam(list(self.encoder.parameters())[0:1]+list(self.decoder.parameters())[0:1],lr=config.min_lr,betas=(0.9,0.95),eps=1e-8)
        if config.scheduler==SchedulerType.CosineAnnealingLR:
            self.scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer,int(2000/config.batch_size*20*5/config.accum_iter),config.min_lr)
        elif config.scheduler==SchedulerType.CosineAnnealingWarmUp:
            self.scheduler=CosineAnnealingLR(self.optimizer,int(2000/config.batch_size*10*5/config.accum_iter),int(2000/config.batch_size*20*5/config.accum_iter),config.min_lr,config.min_lr)
        elif config.scheduler==SchedulerType.CossineLRDecay:
            self.scheduler=CossineLRDecay(self.optimizer,lr_max=config.lr,lr_min=config.min_lr,max_steps=len(train_src)//config.batch_size*config.max_epochs//config.accum_iter,warmup_steps=int(len(train_src)//config.batch_size*config.max_epochs//config.accum_iter*config.warmup_percent))
        elif config.scheduler==SchedulerType.StepLR:
            self.scheduler=torch.optim.lr_scheduler.StepLR(self.optimizer,step_size=1,gamma=6)
        if config.scheduler!=SchedulerType.NoneType:
            self.scheduler.last_step=self.config.skip-1
        #self.scheduler=CossineLRDecay(self.optimizer,lr_max=config.lr,lr_min=2.79936e-05,max_steps=840//2,warmup_steps=0)
        #self.scheduler=torch.optim.lr_scheduler.StepLR(self.optimizer,step_size=1,gamma=6)
        ## type: ignore
        self.config:CFG
            
    def id_mask_split(self,G):
        for g in G:
            g[TransformerTrainGenKeys.src_ids]=g[BaseTrainGenKeys.batch_x][0].squeeze(1)
            #print(maker.src_tokenizer.tokenizer.decode(g[BaseTrainGenKeys.batch_x][0].squeeze(1)[0].tolist()))
            g[TransformerTrainGenKeys.encoder_mask]=g[BaseTrainGenKeys.batch_x][1].squeeze(1)
            g[TransformerTrainGenKeys.tgt_ids]=g[BaseTrainGenKeys.batch_y][0].squeeze(1)
            #print(maker.tgt_tokenizer.tokenizer.decode(g[BaseTrainGenKeys.batch_y][0].squeeze(1)[0].tolist()))
            g[TransformerTrainGenKeys.decoder_mask]=g[BaseTrainGenKeys.batch_y][1].squeeze(1)
            yield g
    
    def forward(self,G):
        self.encoder.train()
        self.decoder.train()
        for g in G:
            with torch.set_grad_enabled(True):
                loss=None
                self.optimizer.zero_grad()
                y=self.encoder(g[TransformerTrainGenKeys.src_ids].to(self.config.device),g[TransformerTrainGenKeys.encoder_mask].to(self.config.device))
                y=self.decoder(g[TransformerTrainGenKeys.tgt_ids].to(self.config.device),y,g[TransformerTrainGenKeys.decoder_mask].to(self.config.device))
                tgt_ids=g[TransformerTrainGenKeys.tgt_ids][:,1:].to(self.config.device)
                # one hot encode
                y=y[:,:-1]
                tgt_ids=tgt_ids.contiguous().view(-1)
                y=y.contiguous().view(-1,self.config.decode_vocab_size)
                loss = torch.nn.CrossEntropyLoss(
                    ignore_index=self.config.pad_idx
                )(y, tgt_ids)/self.config.accum_iter
                del tgt_ids,y
                gc.collect()
                torch.cuda.empty_cache()
                loss.backward()
                if g[BaseTrainGenKeys.batch_step] % self.config.accum_iter == 0 or g[BaseTrainGenKeys.batch_step] == g[BaseTrainGenKeys.max_batch_step]:
                    # print(loss.item())
                    torch.nn.utils.clip_grad_norm_(list(self.encoder.parameters())+list(self.decoder.parameters()), 1.0)
                    self.optimizer.step()
                    self.optimizer2.step()
                    if self.scheduler is not None and self.config.scheduler!=SchedulerType.StepLR:
                        self.scheduler.step()
                
                g[BaseTrainGenKeys.train_loss]=loss.item()*self.config.accum_iter
                
            yield g
    def test(self,G):
        for g in G:
            
            with torch.no_grad():
                self.encoder.eval()
                self.decoder.eval()
                src_ids=g[TransformerTrainGenKeys.src_ids].to(self.config.device)
                encoder_mask=g[TransformerTrainGenKeys.encoder_mask].to(self.config.device)
                tgt_ids=g[TransformerTrainGenKeys.tgt_ids].to(self.config.device)
                decoder_mask=g[TransformerTrainGenKeys.decoder_mask].to(self.config.device)
                y=self.encoder(src_ids,encoder_mask)
                y=self.decoder(tgt_ids,y,decoder_mask)
                tgt_ids=tgt_ids[:,1:]
                y=y[:,:-1]
                if g[BaseTrainGenKeys.batch_step]==g[BaseTrainGenKeys.max_batch_step]:
                    how_many=3
                    src_text=maker.src_tokenizer.tokenizer.batch_decode(src_ids[:how_many].tolist())
                    src_text=[text.replace('[PAD]','') for text in src_text]
                    tgt_text=maker.tgt_tokenizer.tokenizer.batch_decode(tgt_ids[:how_many].tolist())
                    tgt_text=[text.replace('[PAD]','') for text in tgt_text]
                    pred_text=maker.tgt_tokenizer.tokenizer.batch_decode(torch.argmax(y,dim=-1)[:how_many].tolist())
                    pred_text=[text.replace('[PAD]','') for text in pred_text]
                    #EOS„Åæ„Åß„ÅÆ„ÅøË°®Á§∫
                    pred_text=[text[:text.find('[EOS]')+5] for text in pred_text]
                    print(src_text)
                    print(tgt_text)
                    print(pred_text)
                    g[TransformerTrainGenKeys.sampled_src]=src_text[0]
                    g[TransformerTrainGenKeys.sampled_tgt]=tgt_text[0]
                    g[TransformerTrainGenKeys.sampled_pred]=pred_text[0]
                tgt_ids=tgt_ids.contiguous().view(-1)
                y=y.contiguous().view(-1,self.config.decode_vocab_size)
                loss = torch.nn.CrossEntropyLoss(ignore_index=self.config.pad_idx)(y, tgt_ids).cpu()
                g[BaseTrainGenKeys.test_loss]=loss.detach().item()
            yield g
    def run_after_epoch(self,G):
        for g in G:
            logger.log(g[BaseTrainGenKeys.epoch],g[BaseTrainGenKeys.batch_step],g[BaseTrainGenKeys.max_batch_step],g[BaseTrainGenKeys.train_loss],g.get(BaseTrainGenKeys.test_loss,None))
            if g[BaseTrainGenKeys.batch_step] == g[BaseTrainGenKeys.max_batch_step]:
                if self.config.scheduler==SchedulerType.StepLR:
                    self.scheduler.step()
                    print(self.optimizer.param_groups[0]['lr'])
                yield g
    def run_eval_each_step(self,G):
        train_loss=0
        cnt=0
        for g in G:
            train_loss+=g[BaseTrainGenKeys.train_loss]
            cnt+=1
            logger.log(g[BaseTrainGenKeys.epoch],g[BaseTrainGenKeys.batch_step],g[BaseTrainGenKeys.max_batch_step],g[BaseTrainGenKeys.train_loss],g.get(BaseTrainGenKeys.test_loss,None))
            if g[BaseTrainGenKeys.batch_step]% (self.config.accum_iter*self.config.eval_steps)==0 or g[BaseTrainGenKeys.batch_step] == g[BaseTrainGenKeys.max_batch_step]:
                g[BaseTrainGenKeys.train_loss]=train_loss/cnt
                train_loss=0
                cnt=0
                yield g
    def run_after_test(self,G):
        loss=0
        for g in G:
            if g[BaseTrainGenKeys.batch_step] == g[BaseTrainGenKeys.max_batch_step]:
                g[BaseTrainGenKeys.test_loss]+=loss
                g[BaseTrainGenKeys.test_loss]/=g[BaseTrainGenKeys.max_batch_step]
                loss=0
                yield g
            else:
                loss+=g[BaseTrainGenKeys.test_loss]
    def reduce_mem_gen(self,G):
        for g in G:
            torch.cuda.empty_cache()
            gc.collect()
            yield g
config=CFG()
#test
pipeline=TransformerTrainPipeline(train_x,train_y,test_x,test_y,config)
# pipeline.encoder.embedding.load_state_dict(torch.load('/home/rain/exp_env/src/work/translate/model/fasttext_jp.pth'))
# pipeline.decoder.embedding.load_state_dict(torch.load('/home/rain/exp_env/src/work/translate/model/fasttext_en.pth'))
pipeline.encoder.load_state_dict(torch.load('encoder.pth'))
pipeline.decoder.load_state_dict(torch.load('decoder.pth'))
# compile
pipeline.encoder=torch.compile(pipeline.encoder,mode="reduce-overhead")
pipeline.decoder=torch.compile(pipeline.decoder,mode="reduce-overhead")
# pipeline.encoder.embedding.requires_grad=False
# pipeline.decoder.embedding.requires_grad=False

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '\n\nclass TransformerTrainPipeline(BaseTrainPipeline):\n    def __init__(self,train_src,train_tgt,test_src,test_tgt,config:CFG=CFG()):\n        super().__init__(config)\n        self.encoder=transformer.Encoder(\n            how_many_block=config.num_blocks,\n            how_many_heads=config.num_heads,\n            vocab_size=config.encode_vocab_size,\n            embedding_dim=config.embedding_dim,\n            hidden_dim=config.hidden_dim,\n            pad_idx= config.pad_idx,\n            max_seq_len=config.max_len,\n            layer_norm_eps=1e-6,\n        ).to(config.device)\n        self.decoder=transformer.Decoder(\n            how_many_block=config.num_blocks,\n            how_many_heads=config.num_heads,\n            vocab_size=config.decode_vocab_size,\n            embedding_dim=config.embedding_dim,\n            hidden_dim=config.hidden_dim,\n            pad_idx= config.pad_idx,\n            max_seq_len=config.max_len,\n            layer_norm_eps=1e-6,\n        ).to(config.device)\n        self.optimizer=config.optimizer(list(self.encoder.parameters())[1:]+list(self.decoder.parameters())[1:],lr=config.lr,betas=(0.9,0.95),eps=1e-8,fused=True)#,eps=1e-16, betas=(0.9,0.999), weight_decouple = True, rectify = True)\n        self.optimizer2=torch.optim.RAdam(list(self.encoder.parameters())[0:1]+list(self.decoder.parameters())[0:1],lr=config.min_lr,betas=(0.9,0.95),eps=1e-8)\n        if config.scheduler==SchedulerType.CosineAnnealingLR:\n            self.scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer,int(2000/config.batch_size*20*5/config.accum_iter),config.min_lr)\n        elif config.scheduler==SchedulerType.CosineAnnealingWarmUp:\n            self.scheduler=CosineAnnealingLR(self.optimizer,int(2000/config.batch_size*10*5/config.accum_iter),int(2000/config.batch_size*20*5/config.accum_iter),config.min_lr,config.min_lr)\n        elif config.scheduler==SchedulerType.CossineLRDecay:\n            self.scheduler=CossineLRDecay(self.optimizer,lr_max=config.lr,lr_min=config.min_lr,max_steps=len(train_src)//config.batch_size*config.max_epochs//config.accum_iter,warmup_steps=int(len(train_src)//config.batch_size*config.max_epochs//config.accum_iter*config.warmup_percent))\n        elif config.scheduler==SchedulerType.StepLR:\n            self.scheduler=torch.optim.lr_scheduler.StepLR(self.optimizer,step_size=1,gamma=6)\n        if config.scheduler!=SchedulerType.NoneType:\n            self.scheduler.last_step=self.config.skip-1\n        #self.scheduler=CossineLRDecay(self.optimizer,lr_max=config.lr,lr_min=2.79936e-05,max_steps=840//2,warmup_steps=0)\n        #self.scheduler=torch.optim.lr_scheduler.StepLR(self.optimizer,step_size=1,gamma=6)\n        ## type: ignore\n        self.config:CFG\n            \n    def id_mask_split(self,G):\n        for g in G:\n            g[TransformerTrainGenKeys.src_ids]=g[BaseTrainGenKeys.batch_x][0].squeeze(1)\n            #print(maker.src_tokenizer.tokenizer.decode(g[BaseTrainGenKeys.batch_x][0].squeeze(1)[0].tolist()))\n            g[TransformerTrainGenKeys.encoder_mask]=g[BaseTrainGenKeys.batch_x][1].squeeze(1)\n            g[TransformerTrainGenKeys.tgt_ids]=g[BaseTrainGenKeys.batch_y][0].squeeze(1)\n            #print(maker.tgt_tokenizer.tokenizer.decode(g[BaseTrainGenKeys.batch_y][0].squeeze(1)[0].tolist()))\n            g[TransformerTrainGenKeys.decoder_mask]=g[BaseTrainGenKeys.batch_y][1].squeeze(1)\n            yield g\n    \n    def forward(self,G):\n        self.encoder.train()\n        self.decoder.train()\n        for g in G:\n            with torch.set_grad_enabled(True):\n                loss=None\n                self.optimizer.zero_grad()\n                y=self.encoder(g[TransformerTrainGenKeys.src_ids].to(self.config.device),g[TransformerTrainGenKeys.encoder_mask].to(self.config.device))\n                y=self.decoder(g[TransformerTrainGenKeys.tgt_ids].to(self.config.device),y,g[TransformerTrainGenKeys.decoder_mask].to(self.config.device))\n                tgt_ids=g[TransformerTrainGenKeys.tgt_ids][:,1:].to(self.config.device)\n                # one hot encode\n                y=y[:,:-1]\n                tgt_ids=tgt_ids.contiguous().view(-1)\n                y=y.contiguous().view(-1,self.config.decode_vocab_size)\n                loss = torch.nn.CrossEntropyLoss(\n                    ignore_index=self.config.pad_idx\n                )(y, tgt_ids)/self.config.accum_iter\n                del tgt_ids,y\n                gc.collect()\n                torch.cuda.empty_cache()\n                loss.backward()\n                if g[BaseTrainGenKeys.batch_step] % self.config.accum_iter == 0 or g[BaseTrainGenKeys.batch_step] == g[BaseTrainGenKeys.max_batch_step]:\n                    # print(loss.item())\n                    torch.nn.utils.clip_grad_norm_(list(self.encoder.parameters())+list(self.decoder.parameters()), 1.0)\n                    self.optimizer.step()\n                    self.optimizer2.step()\n                    if self.scheduler is not None and self.config.scheduler!=SchedulerType.StepLR:\n                        self.scheduler.step()\n                \n                g[BaseTrainGenKeys.train_loss]=loss.item()*self.config.accum_iter\n                \n            yield g\n    def test(self,G):\n        for g in G:\n            \n            with torch.no_grad():\n                self.encoder.eval()\n                self.decoder.eval()\n                src_ids=g[TransformerTrainGenKeys.src_ids].to(self.config.device)\n                encoder_mask=g[TransformerTrainGenKeys.encoder_mask].to(self.config.device)\n                tgt_ids=g[TransformerTrainGenKeys.tgt_ids].to(self.config.device)\n                decoder_mask=g[TransformerTrainGenKeys.decoder_mask].to(self.config.device)\n                y=self.encoder(src_ids,encoder_mask)\n                y=self.decoder(tgt_ids,y,decoder_mask)\n                tgt_ids=tgt_ids[:,1:]\n                y=y[:,:-1]\n                if g[BaseTrainGenKeys.batch_step]==g[BaseTrainGenKeys.max_batch_step]:\n                    how_many=3\n                    src_text=maker.src_tokenizer.tokenizer.batch_decode(src_ids[:how_many].tolist())\n                    src_text=[text.replace(\'[PAD]\',\'\') for text in src_text]\n                    tgt_text=maker.tgt_tokenizer.tokenizer.batch_decode(tgt_ids[:how_many].tolist())\n                    tgt_text=[text.replace(\'[PAD]\',\'\') for text in tgt_text]\n                    pred_text=maker.tgt_tokenizer.tokenizer.batch_decode(torch.argmax(y,dim=-1)[:how_many].tolist())\n                    pred_text=[text.replace(\'[PAD]\',\'\') for text in pred_text]\n                    #EOS„Åæ„Åß„ÅÆ„ÅøË°®Á§∫\n                    pred_text=[text[:text.find(\'[EOS]\')+5] for text in pred_text]\n                    print(src_text)\n                    print(tgt_text)\n                    print(pred_text)\n                    g[TransformerTrainGenKeys.sampled_src]=src_text[0]\n                    g[TransformerTrainGenKeys.sampled_tgt]=tgt_text[0]\n                    g[TransformerTrainGenKeys.sampled_pred]=pred_text[0]\n                tgt_ids=tgt_ids.contiguous().view(-1)\n                y=y.contiguous().view(-1,self.config.decode_vocab_size)\n                loss = torch.nn.CrossEntropyLoss(ignore_index=self.config.pad_idx)(y, tgt_ids).cpu()\n                g[BaseTrainGenKeys.test_loss]=loss.detach().item()\n            yield g\n    def run_after_epoch(self,G):\n        for g in G:\n            logger.log(g[BaseTrainGenKeys.epoch],g[BaseTrainGenKeys.batch_step],g[BaseTrainGenKeys.max_batch_step],g[BaseTrainGenKeys.train_loss],g.get(BaseTrainGenKeys.test_loss,None))\n            if g[BaseTrainGenKeys.batch_step] == g[BaseTrainGenKeys.max_batch_step]:\n                if self.config.scheduler==SchedulerType.StepLR:\n                    self.scheduler.step()\n                    print(self.optimizer.param_groups[0][\'lr\'])\n                yield g\n    def run_eval_each_step(self,G):\n        train_loss=0\n        cnt=0\n        for g in G:\n            train_loss+=g[BaseTrainGenKeys.train_loss]\n            cnt+=1\n            logger.log(g[BaseTrainGenKeys.epoch],g[BaseTrainGenKeys.batch_step],g[BaseTrainGenKeys.max_batch_step],g[BaseTrainGenKeys.train_loss],g.get(BaseTrainGenKeys.test_loss,None))\n            if g[BaseTrainGenKeys.batch_step]% (self.config.accum_iter*self.config.eval_steps)==0 or g[BaseTrainGenKeys.batch_step] == g[BaseTrainGenKeys.max_batch_step]:\n                g[BaseTrainGenKeys.train_loss]=train_loss/cnt\n                train_loss=0\n                cnt=0\n                yield g\n    def run_after_test(self,G):\n        loss=0\n        for g in G:\n            if g[BaseTrainGenKeys.batch_step] == g[BaseTrainGenKeys.max_batch_step]:\n                g[BaseTrainGenKeys.test_loss]+=loss\n                g[BaseTrainGenKeys.test_loss]/=g[BaseTrainGenKeys.max_batch_step]\n                loss=0\n                yield g\n            else:\n                loss+=g[BaseTrainGenKeys.test_loss]\n    def reduce_mem_gen(self,G):\n        for g in G:\n            torch.cuda.empty_cache()\n            gc.collect()\n            yield g\nconfig=CFG()\n#test\npipeline=TransformerTrainPipeline(train_x,train_y,test_x,test_y,config)\n# pipeline.encoder.embedding.load_state_dict(torch.load(\'/home/rain/exp_env/src/work/translate/model/fasttext_jp.pth\'))\n# pipeline.decoder.embedding.load_state_dict(torch.load(\'/home/rain/exp_env/src/work/translate/model/fasttext_en.pth\'))\npipeline.encoder.load_state_dict(torch.load(\'encoder.pth\'))\npipeline.decoder.load_state_dict(torch.load(\'decoder.pth\'))\n# compile\npipeline.encoder=torch.compile(pipeline.encoder,mode="reduce-overhead")\npipeline.decoder=torch.compile(pipeline.decoder,mode="reduce-overhead")\n# pipeline.encoder.embedding.requires_grad=False\n# pipeline.decoder.embedding.requires_grad=False\n', 'execution_count': 16}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
cfg_to_dict(pipeline.config)
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'cfg_to_dict(pipeline.config)', 'execution_count': 17}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': "{'batch_size': 128,\n 'max_epochs': 1,\n 'is_running': True,\n 'debug': False,\n 'max_len': 256,\n 'lr': 0.0007,\n 'min_lr': 2.79936e-05,\n 'accum_iter': 2,\n 'device': 'cuda',\n 'embedding_dim': 512,\n 'hidden_dim': 512,\n 'num_heads': 12,\n 'num_blocks': 3,\n 'encode_vocab_size': 30004,\n 'decode_vocab_size': 30004,\n 'pad_idx': 30003,\n 'eval_steps': 60}"}, 'metadata': {}, 'execution_count': 17}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
g=pipeline.train_loop_gen(train_dataset)
g=pipeline.id_mask_split(g)
g=pipeline.forward(g)
g=pipeline.reduce_mem_gen(g)
g=pipeline.run_eval_each_step(g)
# g=[{}]
g=pipeline.batch_gen(g,test_dataset,scale=1)
g=pipeline.id_mask_split(g)
g=pipeline.test(g)
g=pipeline.reduce_mem_gen(g)
g=pipeline.run_after_test(g)
best=100
for i in g:
    if i[TransformerTrainGenKeys.test_loss]<best:
        best=i[TransformerTrainGenKeys.test_loss]
        torch.save(pipeline.encoder.state_dict(), 'encoder1.pth')
        torch.save(pipeline.decoder.state_dict(), 'decoder1.pth')
    logger.log(i[BaseTrainGenKeys.epoch],i[BaseTrainGenKeys.batch_step],i[BaseTrainGenKeys.max_batch_step],i[BaseTrainGenKeys.train_loss],i.get(BaseTrainGenKeys.test_loss,None))
    logger.write(i)
    pipeline.encoder.train()
    pipeline.decoder.train()
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': "g=pipeline.train_loop_gen(train_dataset)\ng=pipeline.id_mask_split(g)\ng=pipeline.forward(g)\ng=pipeline.reduce_mem_gen(g)\ng=pipeline.run_eval_each_step(g)\n# g=[{}]\ng=pipeline.batch_gen(g,test_dataset,scale=1)\ng=pipeline.id_mask_split(g)\ng=pipeline.test(g)\ng=pipeline.reduce_mem_gen(g)\ng=pipeline.run_after_test(g)\nbest=100\nfor i in g:\n    if i[TransformerTrainGenKeys.test_loss]<best:\n        best=i[TransformerTrainGenKeys.test_loss]\n        torch.save(pipeline.encoder.state_dict(), 'encoder1.pth')\n        torch.save(pipeline.decoder.state_dict(), 'decoder1.pth')\n    logger.log(i[BaseTrainGenKeys.epoch],i[BaseTrainGenKeys.batch_step],i[BaseTrainGenKeys.max_batch_step],i[BaseTrainGenKeys.train_loss],i.get(BaseTrainGenKeys.test_loss,None))\n    logger.write(i)\n    pipeline.encoder.train()\n    pipeline.decoder.train()", 'execution_count': 18}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': '/home/rain/exp_env/.venv/lib/python3.11/site-packages/torch/_inductor/lowering.py:1611: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.\n  warnings.warn(\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': "skipping cudagraphs due to ['non-cuda device in graph']\n"}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': "skipping cudagraphs due to ['non-cuda device in graph']\n"}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13921/79140 [========>                                         ] train_loss:3.8281\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13922/79140 [========>                                         ] train_loss:3.8594\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13923/79140 [========>                                         ] train_loss:4.0938\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13924/79140 [========>                                         ] train_loss:3.8438\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13925/79140 [========>                                         ] train_loss:3.7969\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13926/79140 [========>                                         ] train_loss:3.8594\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13927/79140 [========>                                         ] train_loss:4.1875\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13928/79140 [========>                                         ] train_loss:3.7500\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13929/79140 [========>                                         ] train_loss:3.9062\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13930/79140 [========>                                         ] train_loss:3.9688\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13931/79140 [========>                                         ] train_loss:3.7188\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13932/79140 [========>                                         ] train_loss:3.7500\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13933/79140 [========>                                         ] train_loss:3.7344\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13934/79140 [========>                                         ] train_loss:3.9844\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13935/79140 [========>                                         ] train_loss:3.8438\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13936/79140 [========>                                         ] train_loss:4.0000\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13937/79140 [========>                                         ] train_loss:3.7969\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13938/79140 [========>                                         ] train_loss:3.8750\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13939/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13940/79140 [========>                                         ] train_loss:3.5938\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13941/79140 [========>                                         ] train_loss:3.8906\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13942/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13943/79140 [========>                                         ] train_loss:3.8438\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13944/79140 [========>                                         ] train_loss:3.8906\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13945/79140 [========>                                         ] train_loss:3.8125\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13946/79140 [========>                                         ] train_loss:4.0938\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13947/79140 [========>                                         ] train_loss:3.7031\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13948/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13949/79140 [========>                                         ] train_loss:3.6875\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13950/79140 [========>                                         ] train_loss:3.9375\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13951/79140 [========>                                         ] train_loss:3.5625\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13952/79140 [========>                                         ] train_loss:3.6562\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13953/79140 [========>                                         ] train_loss:3.8125\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13954/79140 [========>                                         ] train_loss:3.6094\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13955/79140 [========>                                         ] train_loss:3.9531\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13956/79140 [========>                                         ] train_loss:3.6875\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13957/79140 [========>                                         ] train_loss:3.7969\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13958/79140 [========>                                         ] train_loss:3.7500\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13959/79140 [========>                                         ] train_loss:3.8438\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13960/79140 [========>                                         ] train_loss:3.7969\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13961/79140 [========>                                         ] train_loss:3.9531\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13962/79140 [========>                                         ] train_loss:3.9062\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13963/79140 [========>                                         ] train_loss:3.7500\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13964/79140 [========>                                         ] train_loss:3.8281\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13965/79140 [========>                                         ] train_loss:3.6250\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13966/79140 [========>                                         ] train_loss:3.6406\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13967/79140 [========>                                         ] train_loss:3.5781\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13968/79140 [========>                                         ] train_loss:3.9844\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13969/79140 [========>                                         ] train_loss:3.7031\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13970/79140 [========>                                         ] train_loss:3.6719\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13971/79140 [========>                                         ] train_loss:3.7500\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13972/79140 [========>                                         ] train_loss:3.6875\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13973/79140 [========>                                         ] train_loss:4.0312\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13974/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13975/79140 [========>                                         ] train_loss:3.6875\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13976/79140 [========>                                         ] train_loss:3.9844\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13977/79140 [========>                                         ] train_loss:3.9219\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13978/79140 [========>                                         ] train_loss:3.9688\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13979/79140 [========>                                         ] train_loss:3.8750\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13980/79140 [========>                                         ] train_loss:3.7188\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13981/79140 [========>                                         ] train_loss:4.0000\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13982/79140 [========>                                         ] train_loss:3.9219\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13983/79140 [========>                                         ] train_loss:3.7188\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13984/79140 [========>                                         ] train_loss:3.7344\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13985/79140 [========>                                         ] train_loss:3.7500\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13986/79140 [========>                                         ] train_loss:3.7344\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13987/79140 [========>                                         ] train_loss:3.8750\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13988/79140 [========>                                         ] train_loss:3.6875\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13989/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13990/79140 [========>                                         ] train_loss:4.0000\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13991/79140 [========>                                         ] train_loss:3.5938\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13992/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13993/79140 [========>                                         ] train_loss:3.8594\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13994/79140 [========>                                         ] train_loss:3.7344\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13995/79140 [========>                                         ] train_loss:3.5938\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13996/79140 [========>                                         ] train_loss:3.8438\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13997/79140 [========>                                         ] train_loss:3.8125\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13998/79140 [========>                                         ] train_loss:3.8594\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13999/79140 [========>                                         ] train_loss:3.9375\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14000/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14001/79140 [========>                                         ] train_loss:4.0312\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14002/79140 [========>                                         ] train_loss:3.8906\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14003/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14004/79140 [========>                                         ] train_loss:3.7031\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14005/79140 [========>                                         ] train_loss:3.7344\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14006/79140 [========>                                         ] train_loss:3.5781\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14007/79140 [========>                                         ] train_loss:3.9688\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14008/79140 [========>                                         ] train_loss:3.8594\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14009/79140 [========>                                         ] train_loss:3.8906\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14010/79140 [========>                                         ] train_loss:3.7656\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14011/79140 [========>                                         ] train_loss:4.0312\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14012/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14013/79140 [========>                                         ] train_loss:3.9375\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14014/79140 [========>                                         ] train_loss:3.9062\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14015/79140 [========>                                         ] train_loss:3.8125\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14016/79140 [========>                                         ] train_loss:3.8281\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14017/79140 [========>                                         ] train_loss:3.5938\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14018/79140 [========>                                         ] train_loss:3.7969\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14019/79140 [========>                                         ] train_loss:3.8281\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14020/79140 [========>                                         ] train_loss:4.0625\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14021/79140 [========>                                         ] train_loss:4.0625\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14022/79140 [========>                                         ] train_loss:3.8750\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14023/79140 [========>                                         ] train_loss:3.8750\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14024/79140 [========>                                         ] train_loss:3.9688\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14025/79140 [========>                                         ] train_loss:3.8906\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14026/79140 [========>                                         ] train_loss:3.5312\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14027/79140 [========>                                         ] train_loss:3.8281\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14028/79140 [========>                                         ] train_loss:3.8906\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14029/79140 [========>                                         ] train_loss:3.8438\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14030/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14031/79140 [========>                                         ] train_loss:3.7500\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14032/79140 [========>                                         ] train_loss:3.8594\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14033/79140 [========>                                         ] train_loss:3.7188\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14034/79140 [========>                                         ] train_loss:3.6719\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14035/79140 [========>                                         ] train_loss:3.9219\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14036/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14037/79140 [========>                                         ] train_loss:4.0312\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14038/79140 [========>                                         ] train_loss:3.7188\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14039/79140 [========>                                         ] train_loss:3.7969\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14040/79140 [========>                                         ] train_loss:3.6562\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14041/79140 [========>                                         ] train_loss:3.9688\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14042/79140 [========>                                         ] train_loss:3.7188\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14043/79140 [========>                                         ] train_loss:3.9688\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14044/79140 [========>                                         ] train_loss:4.0000\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14045/79140 [========>                                         ] train_loss:3.6875\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14046/79140 [========>                                         ] train_loss:4.1250\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14047/79140 [========>                                         ] train_loss:3.7031\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14048/79140 [========>                                         ] train_loss:3.8125\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14049/79140 [========>                                         ] train_loss:3.8750\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14050/79140 [========>                                         ] train_loss:3.9219\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14051/79140 [========>                                         ] train_loss:3.8438\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14052/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14053/79140 [========>                                         ] train_loss:3.6406\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14054/79140 [========>                                         ] train_loss:3.7188\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14055/79140 [========>                                         ] train_loss:3.6562\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14056/79140 [========>                                         ] train_loss:3.5156\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14057/79140 [========>                                         ] train_loss:4.0000\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14058/79140 [========>                                         ] train_loss:3.8438\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14059/79140 [========>                                         ] train_loss:3.8281\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14060/79140 [========>                                         ] train_loss:3.8594\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14061/79140 [========>                                         ] train_loss:4.0625\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14062/79140 [========>                                         ] train_loss:3.7188\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14063/79140 [========>                                         ] train_loss:3.7344\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14064/79140 [========>                                         ] train_loss:3.8438\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14065/79140 [========>                                         ] train_loss:3.7031\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14066/79140 [========>                                         ] train_loss:3.8594\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14067/79140 [========>                                         ] train_loss:3.9062\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14068/79140 [========>                                         ] train_loss:3.7500\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14069/79140 [========>                                         ] train_loss:3.8594\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14070/79140 [========>                                         ] train_loss:4.3125\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14071/79140 [========>                                         ] train_loss:3.9375\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14072/79140 [========>                                         ] train_loss:3.8438\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14073/79140 [========>                                         ] train_loss:3.9062\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14074/79140 [========>                                         ] train_loss:3.8906\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14075/79140 [========>                                         ] train_loss:3.9531\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14076/79140 [========>                                         ] train_loss:3.6875\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14077/79140 [========>                                         ] train_loss:4.0000\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14078/79140 [========>                                         ] train_loss:3.9062\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14079/79140 [========>                                         ] train_loss:3.8594\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14080/79140 [========>                                         ] train_loss:3.9688\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14081/79140 [========>                                         ] train_loss:3.9531\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14082/79140 [========>                                         ] train_loss:3.9531\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14083/79140 [========>                                         ] train_loss:3.9375\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14084/79140 [========>                                         ] train_loss:3.8750\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14085/79140 [========>                                         ] train_loss:3.6875\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14086/79140 [========>                                         ] train_loss:4.0000\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14087/79140 [========>                                         ] train_loss:3.9688\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14088/79140 [========>                                         ] train_loss:3.8750\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14089/79140 [========>                                         ] train_loss:3.7656\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14090/79140 [========>                                         ] train_loss:3.8125\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14091/79140 [========>                                         ] train_loss:4.1875\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14092/79140 [========>                                         ] train_loss:3.8281\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14093/79140 [========>                                         ] train_loss:3.9375\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14094/79140 [========>                                         ] train_loss:4.0625\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14095/79140 [========>                                         ] train_loss:4.0312\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14096/79140 [========>                                         ] train_loss:3.7344\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14097/79140 [========>                                         ] train_loss:4.0000\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14098/79140 [========>                                         ] train_loss:3.7969\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14099/79140 [========>                                         ] train_loss:4.0625\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14100/79140 [========>                                         ] train_loss:3.9375\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14101/79140 [========>                                         ] train_loss:4.0312\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14102/79140 [========>                                         ] train_loss:3.9531\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14103/79140 [========>                                         ] train_loss:3.7188\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14104/79140 [========>                                         ] train_loss:4.0938\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14105/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14106/79140 [========>                                         ] train_loss:3.9688\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14107/79140 [========>                                         ] train_loss:4.0312\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14108/79140 [========>                                         ] train_loss:4.0625\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14109/79140 [========>                                         ] train_loss:4.0938\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14110/79140 [========>                                         ] train_loss:3.9375\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14111/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14112/79140 [========>                                         ] train_loss:3.7500\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14113/79140 [========>                                         ] train_loss:3.8906\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14114/79140 [========>                                         ] train_loss:3.5938\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14115/79140 [========>                                         ] train_loss:3.9844\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14116/79140 [========>                                         ] train_loss:3.8594\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14117/79140 [========>                                         ] train_loss:3.9062\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14118/79140 [========>                                         ] train_loss:4.0312\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14119/79140 [========>                                         ] train_loss:3.9062\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14120/79140 [========>                                         ] train_loss:3.7969\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14121/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14122/79140 [========>                                         ] train_loss:4.0625\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14123/79140 [========>                                         ] train_loss:3.7500\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14124/79140 [========>                                         ] train_loss:3.8750\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14125/79140 [========>                                         ] train_loss:4.0625\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14126/79140 [========>                                         ] train_loss:3.8125\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14127/79140 [========>                                         ] train_loss:3.7969\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14128/79140 [========>                                         ] train_loss:3.9688\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14129/79140 [========>                                         ] train_loss:3.8281\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14130/79140 [========>                                         ] train_loss:4.0312\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14131/79140 [========>                                         ] train_loss:3.8125\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14132/79140 [========>                                         ] train_loss:3.7656\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14133/79140 [========>                                         ] train_loss:4.0938\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14134/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14135/79140 [========>                                         ] train_loss:4.0625\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14136/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14137/79140 [========>                                         ] train_loss:3.8906\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14138/79140 [========>                                         ] train_loss:3.7500\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14139/79140 [========>                                         ] train_loss:3.9844\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14140/79140 [========>                                         ] train_loss:4.1250\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14141/79140 [========>                                         ] train_loss:3.7031\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14142/79140 [========>                                         ] train_loss:3.6875\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14143/79140 [========>                                         ] train_loss:3.8281\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14144/79140 [========>                                         ] train_loss:4.0938\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14145/79140 [========>                                         ] train_loss:4.0000\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14146/79140 [========>                                         ] train_loss:3.9844\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14147/79140 [========>                                         ] train_loss:3.8594\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14148/79140 [========>                                         ] train_loss:3.9219\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14149/79140 [========>                                         ] train_loss:3.9219\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14150/79140 [========>                                         ] train_loss:3.8750\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14151/79140 [========>                                         ] train_loss:3.8594\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14152/79140 [========>                                         ] train_loss:4.2500\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14153/79140 [========>                                         ] train_loss:3.6406\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14154/79140 [========>                                         ] train_loss:3.9375\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14155/79140 [========>                                         ] train_loss:3.8906\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14156/79140 [========>                                         ] train_loss:3.6719\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14157/79140 [========>                                         ] train_loss:3.9375\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14158/79140 [========>                                         ] train_loss:3.9531\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14159/79140 [========>                                         ] train_loss:3.9531\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14160/79140 [========>                                         ] train_loss:3.8438\n'}
[NbConvertApp] msg_type: error
[NbConvertApp] content: {'traceback': ['\x1b[0;31m---------------------------------------------------------------------------\x1b[0m', '\x1b[0;31mKeyboardInterrupt\x1b[0m                         Traceback (most recent call last)', 'Cell \x1b[0;32mIn[18], line 13\x1b[0m\n\x1b[1;32m     11\x1b[0m g\x1b[38;5;241m=\x1b[39mpipeline\x1b[38;5;241m.\x1b[39mrun_after_test(g)\n\x1b[1;32m     12\x1b[0m best\x1b[38;5;241m=\x1b[39m\x1b[38;5;241m100\x1b[39m\n\x1b[0;32m---> 13\x1b[0m \x1b[38;5;28;43;01mfor\x1b[39;49;00m\x1b[43m \x1b[49m\x1b[43mi\x1b[49m\x1b[43m \x1b[49m\x1b[38;5;129;43;01min\x1b[39;49;00m\x1b[43m \x1b[49m\x1b[43mg\x1b[49m\x1b[43m:\x1b[49m\n\x1b[1;32m     14\x1b[0m \x1b[43m    \x1b[49m\x1b[38;5;28;43;01mif\x1b[39;49;00m\x1b[43m \x1b[49m\x1b[43mi\x1b[49m\x1b[43m[\x1b[49m\x1b[43mTransformerTrainGenKeys\x1b[49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43mtest_loss\x1b[49m\x1b[43m]\x1b[49m\x1b[38;5;241;43m<\x1b[39;49m\x1b[43mbest\x1b[49m\x1b[43m:\x1b[49m\n\x1b[1;32m     15\x1b[0m \x1b[43m        \x1b[49m\x1b[43mbest\x1b[49m\x1b[38;5;241;43m=\x1b[39;49m\x1b[43mi\x1b[49m\x1b[43m[\x1b[49m\x1b[43mTransformerTrainGenKeys\x1b[49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43mtest_loss\x1b[49m\x1b[43m]\x1b[49m\n', 'Cell \x1b[0;32mIn[16], line 140\x1b[0m, in \x1b[0;36mTransformerTrainPipeline.run_after_test\x1b[0;34m(self, G)\x1b[0m\n\x1b[1;32m    138\x1b[0m \x1b[38;5;28;01mdef\x1b[39;00m \x1b[38;5;21mrun_after_test\x1b[39m(\x1b[38;5;28mself\x1b[39m,G):\n\x1b[1;32m    139\x1b[0m     loss\x1b[38;5;241m=\x1b[39m\x1b[38;5;241m0\x1b[39m\n\x1b[0;32m--> 140\x1b[0m \x1b[43m    \x1b[49m\x1b[38;5;28;43;01mfor\x1b[39;49;00m\x1b[43m \x1b[49m\x1b[43mg\x1b[49m\x1b[43m \x1b[49m\x1b[38;5;129;43;01min\x1b[39;49;00m\x1b[43m \x1b[49m\x1b[43mG\x1b[49m\x1b[43m:\x1b[49m\n\x1b[1;32m    141\x1b[0m \x1b[43m        \x1b[49m\x1b[38;5;28;43;01mif\x1b[39;49;00m\x1b[43m \x1b[49m\x1b[43mg\x1b[49m\x1b[43m[\x1b[49m\x1b[43mBaseTrainGenKeys\x1b[49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43mbatch_step\x1b[49m\x1b[43m]\x1b[49m\x1b[43m \x1b[49m\x1b[38;5;241;43m==\x1b[39;49m\x1b[43m \x1b[49m\x1b[43mg\x1b[49m\x1b[43m[\x1b[49m\x1b[43mBaseTrainGenKeys\x1b[49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43mmax_batch_step\x1b[49m\x1b[43m]\x1b[49m\x1b[43m:\x1b[49m\n\x1b[1;32m    142\x1b[0m \x1b[43m            \x1b[49m\x1b[43mg\x1b[49m\x1b[43m[\x1b[49m\x1b[43mBaseTrainGenKeys\x1b[49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43mtest_loss\x1b[49m\x1b[43m]\x1b[49m\x1b[38;5;241;43m+\x1b[39;49m\x1b[38;5;241;43m=\x1b[39;49m\x1b[43mloss\x1b[49m\n', 'Cell \x1b[0;32mIn[16], line 149\x1b[0m, in \x1b[0;36mTransformerTrainPipeline.reduce_mem_gen\x1b[0;34m(self, G)\x1b[0m\n\x1b[1;32m    148\x1b[0m \x1b[38;5;28;01mdef\x1b[39;00m \x1b[38;5;21mreduce_mem_gen\x1b[39m(\x1b[38;5;28mself\x1b[39m,G):\n\x1b[0;32m--> 149\x1b[0m \x1b[43m    \x1b[49m\x1b[38;5;28;43;01mfor\x1b[39;49;00m\x1b[43m \x1b[49m\x1b[43mg\x1b[49m\x1b[43m \x1b[49m\x1b[38;5;129;43;01min\x1b[39;49;00m\x1b[43m \x1b[49m\x1b[43mG\x1b[49m\x1b[43m:\x1b[49m\n\x1b[1;32m    150\x1b[0m \x1b[43m        \x1b[49m\x1b[43mtorch\x1b[49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43mcuda\x1b[49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43mempty_cache\x1b[49m\x1b[43m(\x1b[49m\x1b[43m)\x1b[49m\n\x1b[1;32m    151\x1b[0m \x1b[43m        \x1b[49m\x1b[43mgc\x1b[49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43mcollect\x1b[49m\x1b[43m(\x1b[49m\x1b[43m)\x1b[49m\n', 'Cell \x1b[0;32mIn[16], line 84\x1b[0m, in \x1b[0;36mTransformerTrainPipeline.test\x1b[0;34m(self, G)\x1b[0m\n\x1b[1;32m     83\x1b[0m \x1b[38;5;28;01mdef\x1b[39;00m \x1b[38;5;21mtest\x1b[39m(\x1b[38;5;28mself\x1b[39m,G):\n\x1b[0;32m---> 84\x1b[0m \x1b[43m    \x1b[49m\x1b[38;5;28;43;01mfor\x1b[39;49;00m\x1b[43m \x1b[49m\x1b[43mg\x1b[49m\x1b[43m \x1b[49m\x1b[38;5;129;43;01min\x1b[39;49;00m\x1b[43m \x1b[49m\x1b[43mG\x1b[49m\x1b[43m:\x1b[49m\n\x1b[1;32m     86\x1b[0m \x1b[43m        \x1b[49m\x1b[38;5;28;43;01mwith\x1b[39;49;00m\x1b[43m \x1b[49m\x1b[43mtorch\x1b[49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43mno_grad\x1b[49m\x1b[43m(\x1b[49m\x1b[43m)\x1b[49m\x1b[43m:\x1b[49m\n\x1b[1;32m     87\x1b[0m \x1b[43m            \x1b[49m\x1b[38;5;28;43mself\x1b[39;49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43mencoder\x1b[49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43meval\x1b[49m\x1b[43m(\x1b[49m\x1b[43m)\x1b[49m\n', 'Cell \x1b[0;32mIn[16], line 42\x1b[0m, in \x1b[0;36mTransformerTrainPipeline.id_mask_split\x1b[0;34m(self, G)\x1b[0m\n\x1b[1;32m     41\x1b[0m \x1b[38;5;28;01mdef\x1b[39;00m \x1b[38;5;21mid_mask_split\x1b[39m(\x1b[38;5;28mself\x1b[39m,G):\n\x1b[0;32m---> 42\x1b[0m \x1b[43m    \x1b[49m\x1b[38;5;28;43;01mfor\x1b[39;49;00m\x1b[43m \x1b[49m\x1b[43mg\x1b[49m\x1b[43m \x1b[49m\x1b[38;5;129;43;01min\x1b[39;49;00m\x1b[43m \x1b[49m\x1b[43mG\x1b[49m\x1b[43m:\x1b[49m\n\x1b[1;32m     43\x1b[0m \x1b[43m        \x1b[49m\x1b[43mg\x1b[49m\x1b[43m[\x1b[49m\x1b[43mTransformerTrainGenKeys\x1b[49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43msrc_ids\x1b[49m\x1b[43m]\x1b[49m\x1b[38;5;241;43m=\x1b[39;49m\x1b[43mg\x1b[49m\x1b[43m[\x1b[49m\x1b[43mBaseTrainGenKeys\x1b[49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43mbatch_x\x1b[49m\x1b[43m]\x1b[49m\x1b[43m[\x1b[49m\x1b[38;5;241;43m0\x1b[39;49m\x1b[43m]\x1b[49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43msqueeze\x1b[49m\x1b[43m(\x1b[49m\x1b[38;5;241;43m1\x1b[39;49m\x1b[43m)\x1b[49m\n\x1b[1;32m     44\x1b[0m \x1b[43m        \x1b[49m\x1b[38;5;66;43;03m#print(maker.src_tokenizer.tokenizer.decode(g[BaseTrainGenKeys.batch_x][0].squeeze(1)[0].tolist()))\x1b[39;49;00m\n', 'Cell \x1b[0;32mIn[6], line 38\x1b[0m, in \x1b[0;36mBaseTrainPipeline.batch_gen\x1b[0;34m(self, G, dataset, scale)\x1b[0m\n\x1b[1;32m     36\x1b[0m all_step\x1b[38;5;241m=\x1b[39m\x1b[38;5;241m0\x1b[39m\n\x1b[1;32m     37\x1b[0m \x1b[38;5;28;01mfor\x1b[39;00m g \x1b[38;5;129;01min\x1b[39;00m G:\n\x1b[0;32m---> 38\x1b[0m \x1b[43m    \x1b[49m\x1b[38;5;28;43;01mfor\x1b[39;49;00m\x1b[43m \x1b[49m\x1b[43mi\x1b[49m\x1b[43m,\x1b[49m\x1b[43m(\x1b[49m\x1b[43mbatch_X\x1b[49m\x1b[43m,\x1b[49m\x1b[43mbatch_y\x1b[49m\x1b[43m)\x1b[49m\x1b[43m \x1b[49m\x1b[38;5;129;43;01min\x1b[39;49;00m\x1b[43m \x1b[49m\x1b[38;5;28;43menumerate\x1b[39;49m\x1b[43m(\x1b[49m\x1b[43mdataloader\x1b[49m\x1b[43m)\x1b[49m\x1b[43m:\x1b[49m\n\x1b[1;32m     39\x1b[0m \x1b[43m        \x1b[49m\x1b[38;5;28;43;01mif\x1b[39;49;00m\x1b[43m \x1b[49m\x1b[43mall_step\x1b[49m\x1b[38;5;241;43m<\x1b[39;49m\x1b[38;5;28;43mself\x1b[39;49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43mconfig\x1b[49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43mskip\x1b[49m\x1b[43m:\x1b[49m\n\x1b[1;32m     40\x1b[0m \x1b[43m            \x1b[49m\x1b[43mall_step\x1b[49m\x1b[38;5;241;43m+\x1b[39;49m\x1b[38;5;241;43m=\x1b[39;49m\x1b[38;5;241;43m1\x1b[39;49m\n', 'File \x1b[0;32m~/exp_env/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\x1b[0m, in \x1b[0;36m_BaseDataLoaderIter.__next__\x1b[0;34m(self)\x1b[0m\n\x1b[1;32m    628\x1b[0m \x1b[38;5;28;01mif\x1b[39;00m \x1b[38;5;28mself\x1b[39m\x1b[38;5;241m.\x1b[39m_sampler_iter \x1b[38;5;129;01mis\x1b[39;00m \x1b[38;5;28;01mNone\x1b[39;00m:\n\x1b[1;32m    629\x1b[0m     \x1b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\x1b[39;00m\n\x1b[1;32m    630\x1b[0m     \x1b[38;5;28mself\x1b[39m\x1b[38;5;241m.\x1b[39m_reset()  \x1b[38;5;66;03m# type: ignore[call-arg]\x1b[39;00m\n\x1b[0;32m--> 631\x1b[0m data \x1b[38;5;241m=\x1b[39m \x1b[38;5;28;43mself\x1b[39;49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43m_next_data\x1b[49m\x1b[43m(\x1b[49m\x1b[43m)\x1b[49m\n\x1b[1;32m    632\x1b[0m \x1b[38;5;28mself\x1b[39m\x1b[38;5;241m.\x1b[39m_num_yielded \x1b[38;5;241m+\x1b[39m\x1b[38;5;241m=\x1b[39m \x1b[38;5;241m1\x1b[39m\n\x1b[1;32m    633\x1b[0m \x1b[38;5;28;01mif\x1b[39;00m \x1b[38;5;28mself\x1b[39m\x1b[38;5;241m.\x1b[39m_dataset_kind \x1b[38;5;241m==\x1b[39m _DatasetKind\x1b[38;5;241m.\x1b[39mIterable \x1b[38;5;129;01mand\x1b[39;00m \\\n\x1b[1;32m    634\x1b[0m         \x1b[38;5;28mself\x1b[39m\x1b[38;5;241m.\x1b[39m_IterableDataset_len_called \x1b[38;5;129;01mis\x1b[39;00m \x1b[38;5;129;01mnot\x1b[39;00m \x1b[38;5;28;01mNone\x1b[39;00m \x1b[38;5;129;01mand\x1b[39;00m \\\n\x1b[1;32m    635\x1b[0m         \x1b[38;5;28mself\x1b[39m\x1b[38;5;241m.\x1b[39m_num_yielded \x1b[38;5;241m>\x1b[39m \x1b[38;5;28mself\x1b[39m\x1b[38;5;241m.\x1b[39m_IterableDataset_len_called:\n', 'File \x1b[0;32m~/exp_env/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1329\x1b[0m, in \x1b[0;36m_MultiProcessingDataLoaderIter._next_data\x1b[0;34m(self)\x1b[0m\n\x1b[1;32m   1326\x1b[0m     \x1b[38;5;28;01mreturn\x1b[39;00m \x1b[38;5;28mself\x1b[39m\x1b[38;5;241m.\x1b[39m_process_data(data)\n\x1b[1;32m   1328\x1b[0m \x1b[38;5;28;01massert\x1b[39;00m \x1b[38;5;129;01mnot\x1b[39;00m \x1b[38;5;28mself\x1b[39m\x1b[38;5;241m.\x1b[39m_shutdown \x1b[38;5;129;01mand\x1b[39;00m \x1b[38;5;28mself\x1b[39m\x1b[38;5;241m.\x1b[39m_tasks_outstanding \x1b[38;5;241m>\x1b[39m \x1b[38;5;241m0\x1b[39m\n\x1b[0;32m-> 1329\x1b[0m idx, data \x1b[38;5;241m=\x1b[39m \x1b[38;5;28;43mself\x1b[39;49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43m_get_data\x1b[49m\x1b[43m(\x1b[49m\x1b[43m)\x1b[49m\n\x1b[1;32m   1330\x1b[0m \x1b[38;5;28mself\x1b[39m\x1b[38;5;241m.\x1b[39m_tasks_outstanding \x1b[38;5;241m-\x1b[39m\x1b[38;5;241m=\x1b[39m \x1b[38;5;241m1\x1b[39m\n\x1b[1;32m   1331\x1b[0m \x1b[38;5;28;01mif\x1b[39;00m \x1b[38;5;28mself\x1b[39m\x1b[38;5;241m.\x1b[39m_dataset_kind \x1b[38;5;241m==\x1b[39m _DatasetKind\x1b[38;5;241m.\x1b[39mIterable:\n\x1b[1;32m   1332\x1b[0m     \x1b[38;5;66;03m# Check for _IterableDatasetStopIteration\x1b[39;00m\n', 'File \x1b[0;32m~/exp_env/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1285\x1b[0m, in \x1b[0;36m_MultiProcessingDataLoaderIter._get_data\x1b[0;34m(self)\x1b[0m\n\x1b[1;32m   1283\x1b[0m \x1b[38;5;28;01melif\x1b[39;00m \x1b[38;5;28mself\x1b[39m\x1b[38;5;241m.\x1b[39m_pin_memory:\n\x1b[1;32m   1284\x1b[0m     \x1b[38;5;28;01mwhile\x1b[39;00m \x1b[38;5;28mself\x1b[39m\x1b[38;5;241m.\x1b[39m_pin_memory_thread\x1b[38;5;241m.\x1b[39mis_alive():\n\x1b[0;32m-> 1285\x1b[0m         success, data \x1b[38;5;241m=\x1b[39m \x1b[38;5;28;43mself\x1b[39;49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43m_try_get_data\x1b[49m\x1b[43m(\x1b[49m\x1b[43m)\x1b[49m\n\x1b[1;32m   1286\x1b[0m         \x1b[38;5;28;01mif\x1b[39;00m success:\n\x1b[1;32m   1287\x1b[0m             \x1b[38;5;28;01mreturn\x1b[39;00m data\n', 'File \x1b[0;32m~/exp_env/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1133\x1b[0m, in \x1b[0;36m_MultiProcessingDataLoaderIter._try_get_data\x1b[0;34m(self, timeout)\x1b[0m\n\x1b[1;32m   1120\x1b[0m \x1b[38;5;28;01mdef\x1b[39;00m \x1b[38;5;21m_try_get_data\x1b[39m(\x1b[38;5;28mself\x1b[39m, timeout\x1b[38;5;241m=\x1b[39m_utils\x1b[38;5;241m.\x1b[39mMP_STATUS_CHECK_INTERVAL):\n\x1b[1;32m   1121\x1b[0m     \x1b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\x1b[39;00m\n\x1b[1;32m   1122\x1b[0m     \x1b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\x1b[39;00m\n\x1b[0;32m   (...)\x1b[0m\n\x1b[1;32m   1130\x1b[0m     \x1b[38;5;66;03m# Returns a 2-tuple:\x1b[39;00m\n\x1b[1;32m   1131\x1b[0m     \x1b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\x1b[39;00m\n\x1b[1;32m   1132\x1b[0m     \x1b[38;5;28;01mtry\x1b[39;00m:\n\x1b[0;32m-> 1133\x1b[0m         data \x1b[38;5;241m=\x1b[39m \x1b[38;5;28;43mself\x1b[39;49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43m_data_queue\x1b[49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43mget\x1b[49m\x1b[43m(\x1b[49m\x1b[43mtimeout\x1b[49m\x1b[38;5;241;43m=\x1b[39;49m\x1b[43mtimeout\x1b[49m\x1b[43m)\x1b[49m\n\x1b[1;32m   1134\x1b[0m         \x1b[38;5;28;01mreturn\x1b[39;00m (\x1b[38;5;28;01mTrue\x1b[39;00m, data)\n\x1b[1;32m   1135\x1b[0m     \x1b[38;5;28;01mexcept\x1b[39;00m \x1b[38;5;167;01mException\x1b[39;00m \x1b[38;5;28;01mas\x1b[39;00m e:\n\x1b[1;32m   1136\x1b[0m         \x1b[38;5;66;03m# At timeout and error, we manually check whether any worker has\x1b[39;00m\n\x1b[1;32m   1137\x1b[0m         \x1b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\x1b[39;00m\n\x1b[1;32m   1138\x1b[0m         \x1b[38;5;66;03m# worker failures.\x1b[39;00m\n', 'File \x1b[0;32m~/.rye/py/cpython@3.11.6/lib/python3.11/queue.py:180\x1b[0m, in \x1b[0;36mQueue.get\x1b[0;34m(self, block, timeout)\x1b[0m\n\x1b[1;32m    178\x1b[0m         \x1b[38;5;28;01mif\x1b[39;00m remaining \x1b[38;5;241m<\x1b[39m\x1b[38;5;241m=\x1b[39m \x1b[38;5;241m0.0\x1b[39m:\n\x1b[1;32m    179\x1b[0m             \x1b[38;5;28;01mraise\x1b[39;00m Empty\n\x1b[0;32m--> 180\x1b[0m         \x1b[38;5;28;43mself\x1b[39;49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43mnot_empty\x1b[49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43mwait\x1b[49m\x1b[43m(\x1b[49m\x1b[43mremaining\x1b[49m\x1b[43m)\x1b[49m\n\x1b[1;32m    181\x1b[0m item \x1b[38;5;241m=\x1b[39m \x1b[38;5;28mself\x1b[39m\x1b[38;5;241m.\x1b[39m_get()\n\x1b[1;32m    182\x1b[0m \x1b[38;5;28mself\x1b[39m\x1b[38;5;241m.\x1b[39mnot_full\x1b[38;5;241m.\x1b[39mnotify()\n', 'File \x1b[0;32m~/.rye/py/cpython@3.11.6/lib/python3.11/threading.py:331\x1b[0m, in \x1b[0;36mCondition.wait\x1b[0;34m(self, timeout)\x1b[0m\n\x1b[1;32m    329\x1b[0m \x1b[38;5;28;01melse\x1b[39;00m:\n\x1b[1;32m    330\x1b[0m     \x1b[38;5;28;01mif\x1b[39;00m timeout \x1b[38;5;241m>\x1b[39m \x1b[38;5;241m0\x1b[39m:\n\x1b[0;32m--> 331\x1b[0m         gotit \x1b[38;5;241m=\x1b[39m \x1b[43mwaiter\x1b[49m\x1b[38;5;241;43m.\x1b[39;49m\x1b[43macquire\x1b[49m\x1b[43m(\x1b[49m\x1b[38;5;28;43;01mTrue\x1b[39;49;00m\x1b[43m,\x1b[49m\x1b[43m \x1b[49m\x1b[43mtimeout\x1b[49m\x1b[43m)\x1b[49m\n\x1b[1;32m    332\x1b[0m     \x1b[38;5;28;01melse\x1b[39;00m:\n\x1b[1;32m    333\x1b[0m         gotit \x1b[38;5;241m=\x1b[39m waiter\x1b[38;5;241m.\x1b[39macquire(\x1b[38;5;28;01mFalse\x1b[39;00m)\n', '\x1b[0;31mKeyboardInterrupt\x1b[0m: '], 'ename': 'KeyboardInterrupt', 'evalue': ''}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
next(g)
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'next(g)', 'execution_count': 19}
[NbConvertApp] msg_type: error
[NbConvertApp] content: {'traceback': ['\x1b[0;31m---------------------------------------------------------------------------\x1b[0m', '\x1b[0;31mStopIteration\x1b[0m                             Traceback (most recent call last)', 'Cell \x1b[0;32mIn[19], line 1\x1b[0m\n\x1b[0;32m----> 1\x1b[0m \x1b[38;5;28;43mnext\x1b[39;49m\x1b[43m(\x1b[49m\x1b[43mg\x1b[49m\x1b[43m)\x1b[49m\n', '\x1b[0;31mStopIteration\x1b[0m: '], 'ename': 'StopIteration', 'evalue': ''}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
pipeline.encoder.state_dict()
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'pipeline.encoder.state_dict()', 'execution_count': 20}
[NbConvertApp] ERROR | Kernel died while waiting for execute reply.
[NbConvertApp] Destroying zmq context for <jupyter_client.asynchronous.client.AsyncKernelClient object at 0x7f6d09e230d0>
Traceback (most recent call last):
  File "/home/rain/exp_env/.venv/bin/jupyter-nbconvert", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/jupyter_core/application.py", line 283, in launch_instance
    super().launch_instance(argv=argv, **kwargs)
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/traitlets/config/application.py", line 1075, in launch_instance
    app.start()
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 420, in start
    self.convert_notebooks()
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 597, in convert_notebooks
    self.convert_single_notebook(notebook_filename)
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 563, in convert_single_notebook
    output, resources = self.export_single_notebook(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 487, in export_single_notebook
    output, resources = self.exporter.from_filename(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 201, in from_filename
    return self.from_file(f, resources=resources, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 220, in from_file
    return self.from_notebook_node(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/exporters/notebook.py", line 36, in from_notebook_node
    nb_copy, resources = super().from_notebook_node(nb, resources, **kw)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 154, in from_notebook_node
    nb_copy, resources = self._preprocess(nb_copy, resources)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 353, in _preprocess
    nbc, resc = preprocessor(nbc, resc)
                ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/preprocessors/base.py", line 48, in __call__
    return self.preprocess(nb, resources)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/preprocessors/execute.py", line 103, in preprocess
    self.preprocess_cell(cell, resources, index)
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/preprocessors/execute.py", line 124, in preprocess_cell
    cell = self.execute_cell(cell, index, store_history=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/jupyter_core/utils/__init__.py", line 165, in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rain/.rye/py/cpython@3.11.6/lib/python3.11/asyncio/base_events.py", line 653, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbclient/client.py", line 1009, in async_execute_cell
    raise DeadKernelError("Kernel died") from None
nbclient.exceptions.DeadKernelError: Kernel died
[NbConvertApp] Searching ['/home/rain/exp_env/.venv/etc/jupyter', '/home/rain/.jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files
[NbConvertApp] Looking for jupyter_config in /etc/jupyter
[NbConvertApp] Looking for jupyter_config in /usr/local/etc/jupyter
[NbConvertApp] Looking for jupyter_config in /home/rain/.jupyter
[NbConvertApp] Looking for jupyter_config in /home/rain/exp_env/.venv/etc/jupyter
[NbConvertApp] Looking for jupyter_nbconvert_config in /etc/jupyter
[NbConvertApp] Looking for jupyter_nbconvert_config in /usr/local/etc/jupyter
[NbConvertApp] Looking for jupyter_nbconvert_config in /home/rain/.jupyter
[NbConvertApp] Looking for jupyter_nbconvert_config in /home/rain/exp_env/.venv/etc/jupyter
[NbConvertApp] Looping through config variables with prefix "JUPYTER_NBCONVERT"
[NbConvertApp] Converting notebook /home/rain/exp_env/src/work/translate/trans.ipynb to notebook
[NbConvertApp] Notebook name is 'trans'
[NbConvertApp] Applying preprocessor: ExecutePreprocessor
[NbConvertApp] Instantiating kernel 'Python 3 (ipykernel)' with kernel provisioner: local-provisioner
[NbConvertApp] Starting kernel: ['/home/rain/exp_env/.venv/bin/python', '-m', 'ipykernel_launcher', '-f', '/tmp/tmpkq5xza5v.json', '--HistoryManager.hist_file=:memory:']
[NbConvertApp] Connecting to: tcp://127.0.0.1:40347
[NbConvertApp] connecting iopub channel to tcp://127.0.0.1:37957
[NbConvertApp] Connecting to: tcp://127.0.0.1:37957
[NbConvertApp] connecting shell channel to tcp://127.0.0.1:50233
[NbConvertApp] Connecting to: tcp://127.0.0.1:50233
[NbConvertApp] connecting stdin channel to tcp://127.0.0.1:33757
[NbConvertApp] Connecting to: tcp://127.0.0.1:33757
[NbConvertApp] connecting heartbeat channel to tcp://127.0.0.1:39143
[NbConvertApp] connecting control channel to tcp://127.0.0.1:40347
[NbConvertApp] Connecting to: tcp://127.0.0.1:40347
[NbConvertApp] Executing cell:
import pandas as pd
from exp_env.transformers.models import transformer
import sentencepiece as spm
import torch
from dataclasses import dataclass
import numpy as np
import gc
#default„Åßbf16„Çí‰Ωø„ÅÜ„Çà„ÅÜ„Å´„Åô„Çã
torch.set_default_dtype(torch.bfloat16)
torch.backends.cudnn.benchmark = True
# torch.backends.cudnn.allow_tf32 = True
dev_mode = False
import random
from exp_env.utils.utils import fix_seed
fix_seed(42)
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'import pandas as pd\nfrom exp_env.transformers.models import transformer\nimport sentencepiece as spm\nimport torch\nfrom dataclasses import dataclass\nimport numpy as np\nimport gc\n#default„Åßbf16„Çí‰Ωø„ÅÜ„Çà„ÅÜ„Å´„Åô„Çã\ntorch.set_default_dtype(torch.bfloat16)\ntorch.backends.cudnn.benchmark = True\n# torch.backends.cudnn.allow_tf32 = True\ndev_mode = False\nimport random\nfrom exp_env.utils.utils import fix_seed\nfix_seed(42)', 'execution_count': 1}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
from tokenizers.implementations import SentencePieceUnigramTokenizer
from transformers import PreTrainedTokenizerFast
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"
def load_tokenizer(model_name):
    tokenizer = SentencePieceUnigramTokenizer.from_spm(model_name)
    return PreTrainedTokenizerFast(tokenizer_object=tokenizer._tokenizer, pad_token='[PAD]', bos_token='[BOS]', eos_token='[EOS]', unk_token='[UNK]')

jp_tokenizer = load_tokenizer('tokenizer/jesc_jp.model')
en_tokenizer = load_tokenizer('tokenizer/jesc_en.model')
tokend=jp_tokenizer(text=['„Åì„Çì„Å´„Å°„ÅØüòÉ'],  truncation=True, padding='max_length' , max_length=10, return_tensors='pt')
print(jp_tokenizer.decode(tokend['input_ids'][0]))
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'from tokenizers.implementations import SentencePieceUnigramTokenizer\nfrom transformers import PreTrainedTokenizerFast\nimport os\nos.environ["TOKENIZERS_PARALLELISM"] = "false"\ndef load_tokenizer(model_name):\n    tokenizer = SentencePieceUnigramTokenizer.from_spm(model_name)\n    return PreTrainedTokenizerFast(tokenizer_object=tokenizer._tokenizer, pad_token=\'[PAD]\', bos_token=\'[BOS]\', eos_token=\'[EOS]\', unk_token=\'[UNK]\')\n\njp_tokenizer = load_tokenizer(\'tokenizer/jesc_jp.model\')\nen_tokenizer = load_tokenizer(\'tokenizer/jesc_en.model\')\ntokend=jp_tokenizer(text=[\'„Åì„Çì„Å´„Å°„ÅØüòÉ\'],  truncation=True, padding=\'max_length\' , max_length=10, return_tensors=\'pt\')\nprint(jp_tokenizer.decode(tokend[\'input_ids\'][0]))', 'execution_count': 2}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': '/home/rain/exp_env/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '„Åì„Çì„Å´„Å°„ÅØ<0xF0><0x9F><0x98><0x83>[PAD][PAD][PAD][PAD][PAD]\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
train_df = pd.read_table("/home/rain/exp_env/src/work/translate/data/all/concat",sep="\t",header=None)#.sample(frac=0.14)
if dev_mode:
    train_df = train_df.sample(2000)
train_x = train_df[1].values.tolist()
train_y = train_df[0].values.tolist()
train_y = ['[BOS] ' + text + ' [EOS]' for text in train_y]
test_df = pd.read_table("/home/rain/exp_env/src/work/translate/data/cleaned_jesc/test",sep="\t",header=None)
test_x = test_df[1].values.tolist()
test_y = test_df[0].values.tolist()
test_y = ['[BOS] ' + text + ' [EOS]' for text in test_y]
dev_df = pd.read_table("/home/rain/exp_env/src/work/translate/data/cleaned_jesc/dev",sep="\t",header=None)
dev_x = dev_df[1].values.tolist()
dev_y = dev_df[0].values.tolist()
dev_y = ['[BOS] ' + text + ' [EOS]' for text in dev_y]

test_x = test_x+dev_x
test_y = test_y+dev_y

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'train_df = pd.read_table("/home/rain/exp_env/src/work/translate/data/all/concat",sep="\\t",header=None)#.sample(frac=0.14)\nif dev_mode:\n    train_df = train_df.sample(2000)\ntrain_x = train_df[1].values.tolist()\ntrain_y = train_df[0].values.tolist()\ntrain_y = [\'[BOS] \' + text + \' [EOS]\' for text in train_y]\ntest_df = pd.read_table("/home/rain/exp_env/src/work/translate/data/cleaned_jesc/test",sep="\\t",header=None)\ntest_x = test_df[1].values.tolist()\ntest_y = test_df[0].values.tolist()\ntest_y = [\'[BOS] \' + text + \' [EOS]\' for text in test_y]\ndev_df = pd.read_table("/home/rain/exp_env/src/work/translate/data/cleaned_jesc/dev",sep="\\t",header=None)\ndev_x = dev_df[1].values.tolist()\ndev_y = dev_df[0].values.tolist()\ndev_y = [\'[BOS] \' + text + \' [EOS]\' for text in dev_y]\n\ntest_x = test_x+dev_x\ntest_y = test_y+dev_y\n', 'execution_count': 3}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
class Masker:
    def __init__(self, device):
        self.device = device
    @staticmethod
    @torch.jit.script
    def _subsequent_mask(x: torch.Tensor) -> torch.Tensor:
        batch_size = x.size(0)
        max_len = x.size(1)
        return torch.tril(torch.ones(batch_size, max_len, max_len)).eq(0).to('cpu')
    @staticmethod
    @torch.jit.script
    def _pad_mask(mask: torch.Tensor) -> torch.Tensor:
        return mask.eq(0).unsqueeze(1).repeat(1, mask.size(1), 1).to('cpu')
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': "class Masker:\n    def __init__(self, device):\n        self.device = device\n    @staticmethod\n    @torch.jit.script\n    def _subsequent_mask(x: torch.Tensor) -> torch.Tensor:\n        batch_size = x.size(0)\n        max_len = x.size(1)\n        return torch.tril(torch.ones(batch_size, max_len, max_len)).eq(0).to('cpu')\n    @staticmethod\n    @torch.jit.script\n    def _pad_mask(mask: torch.Tensor) -> torch.Tensor:\n        return mask.eq(0).unsqueeze(1).repeat(1, mask.size(1), 1).to('cpu')", 'execution_count': 4}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
from typing import Iterator
from exp_env.data.data_maker_base import DataMaker, DataMakerSpec

@dataclass
class SpmTokenizerSpec(DataMakerSpec):
    model_name: str
    max_length: int

class SpmTokenizer(DataMaker):
    def __init__(self, spec: SpmTokenizerSpec):
        self.tokenizer = load_tokenizer(spec.model_name)
        self.max_length = spec.max_length

    def make(self, data: list[str]) -> tuple[torch.Tensor, torch.Tensor]:
        tokenized=self.tokenizer(data,  truncation=True, padding='max_length' , max_length=self.max_length, return_tensors='pt')
        return tokenized['input_ids'], tokenized['attention_mask'] # type: ignore
    
class TransformerTrainDataMaker(DataMaker):
    def __init__(self, src_spec: SpmTokenizerSpec, tgt_spec: SpmTokenizerSpec, masker: Masker):
        self.src_tokenizer = SpmTokenizer(src_spec)
        self.tgt_tokenizer = SpmTokenizer(tgt_spec)
        self.masker = masker
    @torch.no_grad()
    def make(self, src_texts: list[str], tgt_texts: list[str],is_train:bool=False) -> tuple[Iterator[tuple[torch.Tensor, torch.Tensor]], Iterator[tuple[torch.Tensor, torch.Tensor]]]:
        src_ids, encoder_mask = self.src_tokenizer.make(src_texts)
        encoder_mask = self.masker._pad_mask(encoder_mask)
        tgt_ids, decoder_mask = self.tgt_tokenizer.make(tgt_texts)
        decoder_mask = self.masker._pad_mask(decoder_mask)
        if is_train:
            seq_mask = self.masker._subsequent_mask(tgt_ids)
            decoder_mask = torch.logical_or(
                seq_mask,
                decoder_mask,
            )
        src=(src_ids, encoder_mask)
        tgt=(tgt_ids, decoder_mask)
        return src, tgt # type: ignore
max_length = 256
maker=TransformerTrainDataMaker(SpmTokenizerSpec('tokenizer/jesc_jp.model', max_length), SpmTokenizerSpec('tokenizer/jesc_en.model', max_length), Masker('cpu'))
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': "from typing import Iterator\nfrom exp_env.data.data_maker_base import DataMaker, DataMakerSpec\n\n@dataclass\nclass SpmTokenizerSpec(DataMakerSpec):\n    model_name: str\n    max_length: int\n\nclass SpmTokenizer(DataMaker):\n    def __init__(self, spec: SpmTokenizerSpec):\n        self.tokenizer = load_tokenizer(spec.model_name)\n        self.max_length = spec.max_length\n\n    def make(self, data: list[str]) -> tuple[torch.Tensor, torch.Tensor]:\n        tokenized=self.tokenizer(data,  truncation=True, padding='max_length' , max_length=self.max_length, return_tensors='pt')\n        return tokenized['input_ids'], tokenized['attention_mask'] # type: ignore\n    \nclass TransformerTrainDataMaker(DataMaker):\n    def __init__(self, src_spec: SpmTokenizerSpec, tgt_spec: SpmTokenizerSpec, masker: Masker):\n        self.src_tokenizer = SpmTokenizer(src_spec)\n        self.tgt_tokenizer = SpmTokenizer(tgt_spec)\n        self.masker = masker\n    @torch.no_grad()\n    def make(self, src_texts: list[str], tgt_texts: list[str],is_train:bool=False) -> tuple[Iterator[tuple[torch.Tensor, torch.Tensor]], Iterator[tuple[torch.Tensor, torch.Tensor]]]:\n        src_ids, encoder_mask = self.src_tokenizer.make(src_texts)\n        encoder_mask = self.masker._pad_mask(encoder_mask)\n        tgt_ids, decoder_mask = self.tgt_tokenizer.make(tgt_texts)\n        decoder_mask = self.masker._pad_mask(decoder_mask)\n        if is_train:\n            seq_mask = self.masker._subsequent_mask(tgt_ids)\n            decoder_mask = torch.logical_or(\n                seq_mask,\n                decoder_mask,\n            )\n        src=(src_ids, encoder_mask)\n        tgt=(tgt_ids, decoder_mask)\n        return src, tgt # type: ignore\nmax_length = 256\nmaker=TransformerTrainDataMaker(SpmTokenizerSpec('tokenizer/jesc_jp.model', max_length), SpmTokenizerSpec('tokenizer/jesc_en.model', max_length), Masker('cpu'))", 'execution_count': 5}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
from torch.utils import data as data

@dataclass
class BaseTrainPipelineConfig:
    batch_size:int=64
    max_epochs:int|None=2
    skip=0
    is_running:bool=True
    debug:bool=False
@dataclass
class BaseTrainGenKeys:
    epoch='epoch'
    train_loss='train_loss'
    test_loss='test_loss'
    batch_x='batch_x'
    batch_y='batch_y'
    batch_step='batch_step'
    max_batch_step='max_batch_step'

class BaseTrainPipeline:
    def __init__(self,config:BaseTrainPipelineConfig):
        self.config=config
    def epoch_gen(self):
        i=0
        while True:
            yield {BaseTrainGenKeys.epoch:i}
            i+=1
            if self.config.max_epochs is not None:
                if i>=self.config.max_epochs:
                    break
            if not self.config.is_running:
                break

    def batch_gen(self,G,dataset,scale=1,no_skip=False):
        dataloader=data.DataLoader(dataset,batch_size=self.config.batch_size*scale,shuffle=True,num_workers=6,pin_memory=True)
        all_step=0
        for g in G:
            for i,(batch_X,batch_y) in enumerate(dataloader):
                if all_step<self.config.skip and not no_skip:
                    all_step+=1
                    continue
                g[BaseTrainGenKeys.batch_x]=batch_X
                g[BaseTrainGenKeys.batch_y]=batch_y
                g[BaseTrainGenKeys.batch_step]=i+1
                g[BaseTrainGenKeys.max_batch_step]=len(dataloader)
                all_step+=1
                yield g
                if self.config.debug:
                    break
                gc.collect()
    
    def train_loop_gen(self,train_dataset):
        g=self.epoch_gen()
        g=self.batch_gen(g,train_dataset)
        return g
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': "from torch.utils import data as data\n\n@dataclass\nclass BaseTrainPipelineConfig:\n    batch_size:int=64\n    max_epochs:int|None=2\n    skip=0\n    is_running:bool=True\n    debug:bool=False\n@dataclass\nclass BaseTrainGenKeys:\n    epoch='epoch'\n    train_loss='train_loss'\n    test_loss='test_loss'\n    batch_x='batch_x'\n    batch_y='batch_y'\n    batch_step='batch_step'\n    max_batch_step='max_batch_step'\n\nclass BaseTrainPipeline:\n    def __init__(self,config:BaseTrainPipelineConfig):\n        self.config=config\n    def epoch_gen(self):\n        i=0\n        while True:\n            yield {BaseTrainGenKeys.epoch:i}\n            i+=1\n            if self.config.max_epochs is not None:\n                if i>=self.config.max_epochs:\n                    break\n            if not self.config.is_running:\n                break\n\n    def batch_gen(self,G,dataset,scale=1,no_skip=False):\n        dataloader=data.DataLoader(dataset,batch_size=self.config.batch_size*scale,shuffle=True,num_workers=6,pin_memory=True)\n        all_step=0\n        for g in G:\n            for i,(batch_X,batch_y) in enumerate(dataloader):\n                if all_step<self.config.skip and not no_skip:\n                    all_step+=1\n                    continue\n                g[BaseTrainGenKeys.batch_x]=batch_X\n                g[BaseTrainGenKeys.batch_y]=batch_y\n                g[BaseTrainGenKeys.batch_step]=i+1\n                g[BaseTrainGenKeys.max_batch_step]=len(dataloader)\n                all_step+=1\n                yield g\n                if self.config.debug:\n                    break\n                gc.collect()\n    \n    def train_loop_gen(self,train_dataset):\n        g=self.epoch_gen()\n        g=self.batch_gen(g,train_dataset)\n        return g", 'execution_count': 6}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:


class TransformerTrainDataLoader(data.Dataset):
    def __init__(self, data_maker: TransformerTrainDataMaker, src_texts: list[str], tgt_texts: list[str],is_train:bool=False):
        self.data_maker = data_maker
        self.src_texts = src_texts
        self.tgt_texts = tgt_texts
        self.is_train=is_train

    def __len__(self):
        return len(self.src_texts)

    def __getitem__(self, idx):
        return self.data_maker.make(self.src_texts[idx], self.tgt_texts[idx],is_train=self.is_train)

train_dataset = TransformerTrainDataLoader(maker, train_x, train_y,is_train=True)
test_dataset = TransformerTrainDataLoader(maker, test_x, test_y,is_train=False)
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '\n\nclass TransformerTrainDataLoader(data.Dataset):\n    def __init__(self, data_maker: TransformerTrainDataMaker, src_texts: list[str], tgt_texts: list[str],is_train:bool=False):\n        self.data_maker = data_maker\n        self.src_texts = src_texts\n        self.tgt_texts = tgt_texts\n        self.is_train=is_train\n\n    def __len__(self):\n        return len(self.src_texts)\n\n    def __getitem__(self, idx):\n        return self.data_maker.make(self.src_texts[idx], self.tgt_texts[idx],is_train=self.is_train)\n\ntrain_dataset = TransformerTrainDataLoader(maker, train_x, train_y,is_train=True)\ntest_dataset = TransformerTrainDataLoader(maker, test_x, test_y,is_train=False)', 'execution_count': 7}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Skipping non-executing cell 7
[NbConvertApp] Executing cell:
len(train_x)//128
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'len(train_x)//128', 'execution_count': 8}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '79139'}, 'metadata': {}, 'execution_count': 8}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
import math


class CosineAnnealingLR(torch.optim.lr_scheduler._LRScheduler):
    def __init__(
        self,
        optimizer: torch.optim.Optimizer,
        warmup_epochs: int,
        max_epochs: int,
        warmup_start_lr: float = 0.00001,
        eta_min: float = 0.00001,
        last_epoch: int = -1,
    ):
        """
        Args:
            optimizer (torch.optim.Optimizer):
                ÊúÄÈÅ©ÂåñÊâãÊ≥ï„Ç§„É≥„Çπ„Çø„É≥„Çπ
            warmup_epochs (int):
                linear warmup„ÇíË°å„ÅÜepochÊï∞
            max_epochs (int):
                cosineÊõ≤Á∑ö„ÅÆÁµÇ‰∫Ü„Å´Áî®„ÅÑ„Çã Â≠¶Áøí„ÅÆepochÊï∞
            warmup_start_lr (float):
                linear warmup 0 epochÁõÆ„ÅÆÂ≠¶ÁøíÁéá
            eta_min (float):
                cosineÊõ≤Á∑ö„ÅÆ‰∏ãÈôê
            last_epoch (int):
                cosineÊõ≤Á∑ö„ÅÆ‰ΩçÁõ∏„Ç™„Éï„Çª„ÉÉ„Éà
        Â≠¶ÁøíÁéá„Çímax_epochs„Å´Ëá≥„Çã„Åæ„Åß„Ç≥„Çµ„Ç§„É≥Êõ≤Á∑ö„Å´Ê≤ø„Å£„Å¶„Çπ„Ç±„Ç∏„É•„Éº„É´„Åô„Çã
        epoch 0„Åã„Çâwarmup_epochs„Åæ„Åß„ÅÆÂ≠¶ÁøíÊõ≤Á∑ö„ÅØÁ∑öÂΩ¢warmup„Åå„Åã„Åã„Çã
        https://pytorch-lightning-bolts.readthedocs.io/en/stable/schedulers/warmup_cosine_annealing.html
        """
        self.warmup_epochs = warmup_epochs
        self.max_epochs = max_epochs
        self.warmup_start_lr = warmup_start_lr
        self.eta_min = eta_min
        super().__init__(optimizer, last_epoch)
        return None

    def get_lr(self):
        if self.last_epoch == 0:
            return [self.warmup_start_lr] * len(self.base_lrs)
        if self.last_epoch < self.warmup_epochs:
            return [
                group["lr"] + (base_lr - self.warmup_start_lr) / (self.warmup_epochs - 1)
                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)
            ]
        if self.last_epoch == self.warmup_epochs:
            return self.base_lrs
        if (self.last_epoch - 1 - self.max_epochs) % (2 * (self.max_epochs - self.warmup_epochs)) == 0:
            return [
                group["lr"] + (base_lr - self.eta_min) * (1 - math.cos(math.pi / (self.max_epochs - self.warmup_epochs))) / 2
                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)
            ]

        return [
            (1 + math.cos(math.pi * (self.last_epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)))
            / (1 + math.cos(math.pi * (self.last_epoch - self.warmup_epochs - 1) / (self.max_epochs - self.warmup_epochs)))
            * (group["lr"] - self.eta_min)
            + self.eta_min
            for group in self.optimizer.param_groups
        ]
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'import math\n\n\nclass CosineAnnealingLR(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(\n        self,\n        optimizer: torch.optim.Optimizer,\n        warmup_epochs: int,\n        max_epochs: int,\n        warmup_start_lr: float = 0.00001,\n        eta_min: float = 0.00001,\n        last_epoch: int = -1,\n    ):\n        """\n        Args:\n            optimizer (torch.optim.Optimizer):\n                ÊúÄÈÅ©ÂåñÊâãÊ≥ï„Ç§„É≥„Çπ„Çø„É≥„Çπ\n            warmup_epochs (int):\n                linear warmup„ÇíË°å„ÅÜepochÊï∞\n            max_epochs (int):\n                cosineÊõ≤Á∑ö„ÅÆÁµÇ‰∫Ü„Å´Áî®„ÅÑ„Çã Â≠¶Áøí„ÅÆepochÊï∞\n            warmup_start_lr (float):\n                linear warmup 0 epochÁõÆ„ÅÆÂ≠¶ÁøíÁéá\n            eta_min (float):\n                cosineÊõ≤Á∑ö„ÅÆ‰∏ãÈôê\n            last_epoch (int):\n                cosineÊõ≤Á∑ö„ÅÆ‰ΩçÁõ∏„Ç™„Éï„Çª„ÉÉ„Éà\n        Â≠¶ÁøíÁéá„Çímax_epochs„Å´Ëá≥„Çã„Åæ„Åß„Ç≥„Çµ„Ç§„É≥Êõ≤Á∑ö„Å´Ê≤ø„Å£„Å¶„Çπ„Ç±„Ç∏„É•„Éº„É´„Åô„Çã\n        epoch 0„Åã„Çâwarmup_epochs„Åæ„Åß„ÅÆÂ≠¶ÁøíÊõ≤Á∑ö„ÅØÁ∑öÂΩ¢warmup„Åå„Åã„Åã„Çã\n        https://pytorch-lightning-bolts.readthedocs.io/en/stable/schedulers/warmup_cosine_annealing.html\n        """\n        self.warmup_epochs = warmup_epochs\n        self.max_epochs = max_epochs\n        self.warmup_start_lr = warmup_start_lr\n        self.eta_min = eta_min\n        super().__init__(optimizer, last_epoch)\n        return None\n\n    def get_lr(self):\n        if self.last_epoch == 0:\n            return [self.warmup_start_lr] * len(self.base_lrs)\n        if self.last_epoch < self.warmup_epochs:\n            return [\n                group["lr"] + (base_lr - self.warmup_start_lr) / (self.warmup_epochs - 1)\n                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)\n            ]\n        if self.last_epoch == self.warmup_epochs:\n            return self.base_lrs\n        if (self.last_epoch - 1 - self.max_epochs) % (2 * (self.max_epochs - self.warmup_epochs)) == 0:\n            return [\n                group["lr"] + (base_lr - self.eta_min) * (1 - math.cos(math.pi / (self.max_epochs - self.warmup_epochs))) / 2\n                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)\n            ]\n\n        return [\n            (1 + math.cos(math.pi * (self.last_epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)))\n            / (1 + math.cos(math.pi * (self.last_epoch - self.warmup_epochs - 1) / (self.max_epochs - self.warmup_epochs)))\n            * (group["lr"] - self.eta_min)\n            + self.eta_min\n            for group in self.optimizer.param_groups\n        ]', 'execution_count': 9}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
import matplotlib.pyplot as plt
sche=CosineAnnealingLR(torch.optim.Adam([torch.nn.Parameter(torch.tensor(1.0))]),warmup_epochs=10,max_epochs=100)
sche.last_epoch=999
lrs=[]
for i in range(300):
    sche.step()
    lrs.append(sche.get_lr()[0])
plt.plot(lrs)
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'import matplotlib.pyplot as plt\nsche=CosineAnnealingLR(torch.optim.Adam([torch.nn.Parameter(torch.tensor(1.0))]),warmup_epochs=10,max_epochs=100)\nsche.last_epoch=999\nlrs=[]\nfor i in range(300):\n    sche.step()\n    lrs.append(sche.get_lr()[0])\nplt.plot(lrs)', 'execution_count': 10}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': '/home/rain/exp_env/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "\n'}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '[<matplotlib.lines.Line2D at 0x7fb2f9b5ea90>]'}, 'metadata': {}, 'execution_count': 10}
[NbConvertApp] msg_type: display_data
[NbConvertApp] content: {'data': {'text/plain': '<Figure size 640x480 with 1 Axes>', 'image/png': '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'}, 'metadata': {}, 'transient': {}}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
import torch
class CossineLRDecay(torch.optim.lr_scheduler._LRScheduler):
    def __init__(self,optimizer,lr_max,lr_min,max_steps,warmup_steps):
        self.lr_max=lr_max
        self.lr_min=lr_min
        self.max_steps=max_steps
        self.warmup_steps=warmup_steps
        super().__init__(optimizer)
    def get_lr(self):
        if self.last_epoch<self.warmup_steps:
            return [self.lr_max/self.warmup_steps*(self.last_epoch+1) for _ in self.optimizer.param_groups]
        elif self.last_epoch<self.max_steps:
            return [self.lr_min+(self.lr_max-self.lr_min)*(1+math.cos(math.pi*(self.last_epoch-self.warmup_steps)/(self.max_steps-self.warmup_steps)))/2 for _ in self.optimizer.param_groups]
        else:
            return [self.lr_min for _ in self.optimizer.param_groups]

sche=CossineLRDecay(torch.optim.Adam([torch.nn.Parameter(torch.tensor(1.0))]),lr_max=0.1,lr_min=0.0001,max_steps=1000,warmup_steps=0)
sche.last_epoch=999
lrs=[]
for i in range(1000):
    sche.step()
    lrs.append(sche.get_lr()[0])
plt.plot(lrs)
    
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'import torch\nclass CossineLRDecay(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(self,optimizer,lr_max,lr_min,max_steps,warmup_steps):\n        self.lr_max=lr_max\n        self.lr_min=lr_min\n        self.max_steps=max_steps\n        self.warmup_steps=warmup_steps\n        super().__init__(optimizer)\n    def get_lr(self):\n        if self.last_epoch<self.warmup_steps:\n            return [self.lr_max/self.warmup_steps*(self.last_epoch+1) for _ in self.optimizer.param_groups]\n        elif self.last_epoch<self.max_steps:\n            return [self.lr_min+(self.lr_max-self.lr_min)*(1+math.cos(math.pi*(self.last_epoch-self.warmup_steps)/(self.max_steps-self.warmup_steps)))/2 for _ in self.optimizer.param_groups]\n        else:\n            return [self.lr_min for _ in self.optimizer.param_groups]\n\nsche=CossineLRDecay(torch.optim.Adam([torch.nn.Parameter(torch.tensor(1.0))]),lr_max=0.1,lr_min=0.0001,max_steps=1000,warmup_steps=0)\nsche.last_epoch=999\nlrs=[]\nfor i in range(1000):\n    sche.step()\n    lrs.append(sche.get_lr()[0])\nplt.plot(lrs)\n    ', 'execution_count': 11}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '[<matplotlib.lines.Line2D at 0x7fb3a9a9cf50>]'}, 'metadata': {}, 'execution_count': 11}
[NbConvertApp] msg_type: display_data
[NbConvertApp] content: {'data': {'text/plain': '<Figure size 640x480 with 1 Axes>', 'image/png': 'iVBORw0KGgoAAAANSUhEUgAAAk8AAAGdCAYAAAAL2ZfXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAtkUlEQVR4nO3df3BU9b3/8Xd+sLtB3Cw/zG6CCQSNRDGFQGQbf6Flx2Azluidqpk0Eyka9cIVLr0oXAvcaccmA1xnFFG0MxVnaokwKrT8cjJBTIPJgiH8CKGRK7nKRTcMhOwGhQDZ9/ePfjn1lAD5UCANPB8zO3HP53XOfs7HgfOak90lRlVVAAAA0COxvT0BAACAvoTyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYCC+tydwNYpGo/L111/L9ddfLzExMb09HQAA0AOqKh0dHZKSkiKxsee+v0R5ugy+/vprSU1N7e1pAACAi3DgwAG58cYbzzlOeboMrr/+ehH56+K73e5eng0AAOiJSCQiqamp1nX8XChPl8GZX9W53W7KEwAAfcyF3nLDG8YBAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMUJ4AAAAMXFR5Wrp0qQwfPlxcLpf4/X7ZunXrefOrVq2SzMxMcblckpWVJevXr7eNq6rMnz9fkpOTJSEhQQKBgOzbt8+WaWtrk6KiInG73eLxeGTq1Kly7Ngxa/zEiRPyxBNPSFZWlsTHx0tBQUG3c9m8ebOMHTtWnE6n3HzzzbJ8+fJzzru8vFxiYmJk5syZ5z0/AABw7TAuT++9957MmjVLFixYINu3b5fRo0dLXl6eHDp0qNv8p59+KoWFhTJ16lRpaGiQgoICKSgokMbGRiuzcOFCefXVV2XZsmUSDAbluuuuk7y8PDlx4oSVKSoqkj179khlZaWsXbtWqqurpbS01Brv6uqShIQEee655yQQCHQ7l5aWFsnPz5f7779fduzYITNnzpQnn3xSPvroo7Oy27ZtkzfffFN+8IMfmC4RAAC4mqmh8ePH67Rp06znXV1dmpKSomVlZd3mH330Uc3Pz7dt8/v9+vTTT6uqajQaVZ/Pp4sWLbLG29vb1el06ooVK1RVtampSUVEt23bZmU2bNigMTExevDgwbNes6SkRCdPnnzW9ueff15HjRpl2/bYY49pXl6ebVtHR4dmZGRoZWWlTpgwQWfMmNHtuZ1LOBxWEdFwOGy0HwAA6D09vX4b3Xk6efKk1NfX2+7sxMbGSiAQkNra2m73qa2tPetOUF5enpVvaWmRUChkyyQmJorf77cytbW14vF4JCcnx8oEAgGJjY2VYDDY4/lfaC5nTJs2TfLz8895B+vvdXZ2SiQSsT0AAMDVyag8HT58WLq6usTr9dq2e71eCYVC3e4TCoXOmz/z80KZpKQk23h8fLwMGjTonK9rMpdIJCLHjx8XEZGKigrZvn27lJWV9fi4ZWVlkpiYaD1SU1N7vC8AAOhb+LTd9xw4cEBmzJgh7777rrhcrh7vN3fuXAmHw9bjwIEDl3GWAACgN8WbhIcMGSJxcXHS2tpq297a2io+n6/bfXw+33nzZ362trZKcnKyLTNmzBgr8/dvSD99+rS0tbWd83VN5uJ2uyUhIUHq6+vl0KFDMnbsWGu8q6tLqqur5bXXXpPOzk6Ji4s767hOp1OcTmeP5wEAAPouoztPDodDxo0bJ1VVVda2aDQqVVVVkpub2+0+ubm5tryISGVlpZVPT08Xn89ny0QiEQkGg1YmNzdX2tvbpb6+3sps2rRJotGo+P3+Hs//QnOZOHGi7N69W3bs2GE9cnJypKioSHbs2NFtcQIAANcY03eiV1RUqNPp1OXLl2tTU5OWlpaqx+PRUCikqqrFxcU6Z84cK79lyxaNj4/XxYsX6969e3XBggXar18/3b17t5UpLy9Xj8eja9as0V27dunkyZM1PT1djx8/bmUmTZqk2dnZGgwGtaamRjMyMrSwsNA2tz179mhDQ4M+9NBDet9992lDQ4M2NDRY4/v379f+/fvr7Nmzde/evbp06VKNi4vTjRs3nvN8+bQdAADXhp5ev43Lk6rqkiVLNC0tTR0Oh44fP17r6uqssQkTJmhJSYktv3LlSr3lllvU4XDoqFGjdN26dbbxaDSq8+bNU6/Xq06nUydOnKjNzc22zJEjR7SwsFAHDBigbrdbp0yZoh0dHbbMsGHDVETOenzfxx9/rGPGjFGHw6EjRozQt99++7znSnkCAODa0NPrd4yqaq/d9rpKRSIRSUxMlHA4LG63u7enAwAAeqCn128+bQcAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGCA8gQAAGDgosrT0qVLZfjw4eJyucTv98vWrVvPm1+1apVkZmaKy+WSrKwsWb9+vW1cVWX+/PmSnJwsCQkJEggEZN++fbZMW1ubFBUVidvtFo/HI1OnTpVjx45Z4ydOnJAnnnhCsrKyJD4+XgoKCrqdy+bNm2Xs2LHidDrl5ptvluXLl9vGy8rK5I477pDrr79ekpKSpKCgQJqbm3u+OAAA4KpmXJ7ee+89mTVrlixYsEC2b98uo0ePlry8PDl06FC3+U8//VQKCwtl6tSp0tDQIAUFBVJQUCCNjY1WZuHChfLqq6/KsmXLJBgMynXXXSd5eXly4sQJK1NUVCR79uyRyspKWbt2rVRXV0tpaak13tXVJQkJCfLcc89JIBDodi4tLS2Sn58v999/v+zYsUNmzpwpTz75pHz00UdW5pNPPpFp06ZJXV2dVFZWyqlTp+SBBx6Qb7/91nSpAADA1UgNjR8/XqdNm2Y97+rq0pSUFC0rK+s2/+ijj2p+fr5tm9/v16efflpVVaPRqPp8Pl20aJE13t7erk6nU1esWKGqqk1NTSoium3bNiuzYcMGjYmJ0YMHD571miUlJTp58uSztj///PM6atQo27bHHntM8/Lyznm+hw4dUhHRTz755JyZvxcOh1VENBwO93gfAADQu3p6/Ta683Ty5Empr6+33dmJjY2VQCAgtbW13e5TW1t71p2gvLw8K9/S0iKhUMiWSUxMFL/fb2Vqa2vF4/FITk6OlQkEAhIbGyvBYLDH87/QXLoTDodFRGTQoEHnzHR2dkokErE9AADA1cmoPB0+fFi6urrE6/Xatnu9XgmFQt3uEwqFzps/8/NCmaSkJNt4fHy8DBo06JyvazKXSCQix48fPysfjUZl5syZctddd8ntt99+zuOWlZVJYmKi9UhNTe3xnAAAQN/Cp+3OY9q0adLY2CgVFRXnzc2dO1fC4bD1OHDgwBWaIQAAuNLiTcJDhgyRuLg4aW1ttW1vbW0Vn8/X7T4+n++8+TM/W1tbJTk52ZYZM2aMlfn7N6SfPn1a2trazvm6JnNxu92SkJBg2z59+nTrjek33njjeY/rdDrF6XT2eB4AAKDvMrrz5HA4ZNy4cVJVVWVti0ajUlVVJbm5ud3uk5uba8uLiFRWVlr59PR08fl8tkwkEpFgMGhlcnNzpb29Xerr663Mpk2bJBqNit/v7/H8LzQXkb9+bcL06dPlww8/lE2bNkl6enqPjw8AAK4Bpu9Er6ioUKfTqcuXL9empiYtLS1Vj8ejoVBIVVWLi4t1zpw5Vn7Lli0aHx+vixcv1r179+qCBQu0X79+unv3bitTXl6uHo9H16xZo7t27dLJkydrenq6Hj9+3MpMmjRJs7OzNRgMak1NjWZkZGhhYaFtbnv27NGGhgZ96KGH9L777tOGhgZtaGiwxvfv36/9+/fX2bNn6969e3Xp0qUaFxenGzdutDLPPvusJiYm6ubNm/Wbb76xHt99912P14hP2wEA0Pf09PptXJ5UVZcsWaJpaWnqcDh0/PjxWldXZ41NmDBBS0pKbPmVK1fqLbfcog6HQ0eNGqXr1q2zjUejUZ03b556vV51Op06ceJEbW5utmWOHDmihYWFOmDAAHW73TplyhTt6OiwZYYNG6Yictbj+z7++GMdM2aMOhwOHTFihL799tv2BelmfxE5K3c+lCcAAPqenl6/Y1RVe+GG11UtEolIYmKihMNhcbvdvT0dAADQAz29fvNpOwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAMXVZ6WLl0qw4cPF5fLJX6/X7Zu3Xre/KpVqyQzM1NcLpdkZWXJ+vXrbeOqKvPnz5fk5GRJSEiQQCAg+/bts2Xa2tqkqKhI3G63eDwemTp1qhw7dswaP3HihDzxxBOSlZUl8fHxUlBQ0O1cNm/eLGPHjhWn0yk333yzLF++/B8+PwAAcO0wLk/vvfeezJo1SxYsWCDbt2+X0aNHS15enhw6dKjb/KeffiqFhYUydepUaWhokIKCAikoKJDGxkYrs3DhQnn11Vdl2bJlEgwG5brrrpO8vDw5ceKElSkqKpI9e/ZIZWWlrF27Vqqrq6W0tNQa7+rqkoSEBHnuueckEAh0O5eWlhbJz8+X+++/X3bs2CEzZ86UJ598Uj766KOLPj8AAHBtiVFVNdnB7/fLHXfcIa+99pqIiESjUUlNTZV/+7d/kzlz5pyVf+yxx+Tbb7+VtWvXWtt++MMfypgxY2TZsmWiqpKSkiK/+MUv5D/+4z9ERCQcDovX65Xly5fL448/Lnv37pXbbrtNtm3bJjk5OSIisnHjRvnxj38s//d//ycpKSm213ziiSekvb1dVq9ebdv+wgsvyLp162zF7fHHH5f29nbZuHHjRZ1fdyKRiCQmJko4HBa3292jfS5EVeX4qa5LciwAAPq6hH5xEhMTc0mP2dPrd7zJQU+ePCn19fUyd+5ca1tsbKwEAgGpra3tdp/a2lqZNWuWbVteXp5VbFpaWiQUCtnuFiUmJorf75fa2lp5/PHHpba2Vjwej1WcREQCgYDExsZKMBiUhx9+uEfzr62tPeuuVF5ensycOfOiz09EpLOzUzo7O63nkUikR/MxcfxUl9w2/6MLBwEAuAY0/SpP+juMaswlY/Rru8OHD0tXV5d4vV7bdq/XK6FQqNt9QqHQefNnfl4ok5SUZBuPj4+XQYMGnfN1TeYSiUTk+PHjF3V+IiJlZWWSmJhoPVJTU3s8JwAA0Lf0TmW7ysydO9d2dy0SiVzyApXQL06afpV3SY8JAEBfldAvrtde26g8DRkyROLi4qS1tdW2vbW1VXw+X7f7+Hy+8+bP/GxtbZXk5GRbZsyYMVbm79+wffr0aWlrazvn65rMxe12S0JCgsTFxRmfn4iI0+kUp9PZ43lcjJiYmF67PQkAAP7G6Nd2DodDxo0bJ1VVVda2aDQqVVVVkpub2+0+ubm5tryISGVlpZVPT08Xn89ny0QiEQkGg1YmNzdX2tvbpb6+3sps2rRJotGo+P3+Hs//QnO5mPMDAADXGDVUUVGhTqdTly9frk1NTVpaWqoej0dDoZCqqhYXF+ucOXOs/JYtWzQ+Pl4XL16se/fu1QULFmi/fv109+7dVqa8vFw9Ho+uWbNGd+3apZMnT9b09HQ9fvy4lZk0aZJmZ2drMBjUmpoazcjI0MLCQtvc9uzZow0NDfrQQw/pfffdpw0NDdrQ0GCN79+/X/v376+zZ8/WvXv36tKlSzUuLk43btzY4/PriXA4rCKi4XC4x/sAAIDe1dPrt3F5UlVdsmSJpqWlqcPh0PHjx2tdXZ01NmHCBC0pKbHlV65cqbfccos6HA4dNWqUrlu3zjYejUZ13rx56vV61el06sSJE7W5udmWOXLkiBYWFuqAAQPU7XbrlClTtKOjw5YZNmyYishZj+/7+OOPdcyYMepwOHTEiBH69ttvG51fT1CeAADoe3p6/Tb+nidc2OX4nicAAHB59fT6zb9tBwAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYIDyBAAAYOCiytPSpUtl+PDh4nK5xO/3y9atW8+bX7VqlWRmZorL5ZKsrCxZv369bVxVZf78+ZKcnCwJCQkSCARk3759tkxbW5sUFRWJ2+0Wj8cjU6dOlWPHjtkyu3btknvuuUdcLpekpqbKwoULbeOnTp2SX/3qV3LTTTeJy+WS0aNHy8aNG22Zrq4umTdvnqSnp0tCQoLcdNNN8utf/1pU1XSZAADA1UgNVVRUqMPh0N/97ne6Z88efeqpp9Tj8Whra2u3+S1btmhcXJwuXLhQm5qa9Je//KX269dPd+/ebWXKy8s1MTFRV69erTt37tSf/OQnmp6ersePH7cykyZN0tGjR2tdXZ3++c9/1ptvvlkLCwut8XA4rF6vV4uKirSxsVFXrFihCQkJ+uabb1qZ559/XlNSUnTdunX6xRdf6Ouvv64ul0u3b99uZV566SUdPHiwrl27VltaWnTVqlU6YMAAfeWVV3q8RuFwWEVEw+Fwj/cBAAC9q6fXb+PyNH78eJ02bZr1vKurS1NSUrSsrKzb/KOPPqr5+fm2bX6/X59++mlVVY1Go+rz+XTRokXWeHt7uzqdTl2xYoWqqjY1NamI6LZt26zMhg0bNCYmRg8ePKiqqq+//roOHDhQOzs7rcwLL7ygI0eOtJ4nJyfra6+9ZpvLI488okVFRdbz/Px8/fnPf37ezIVQngAA6Ht6ev02+rXdyZMnpb6+XgKBgLUtNjZWAoGA1NbWdrtPbW2tLS8ikpeXZ+VbWlokFArZMomJieL3+61MbW2teDweycnJsTKBQEBiY2MlGAxamXvvvVccDoftdZqbm+Xo0aMiItLZ2Skul8s2l4SEBKmpqbGe33nnnVJVVSWff/65iIjs3LlTampq5MEHHzznunR2dkokErE9AADA1cmoPB0+fFi6urrE6/Xatnu9XgmFQt3uEwqFzps/8/NCmaSkJNt4fHy8DBo0yJbp7hjff428vDx5+eWXZd++fRKNRqWyslI++OAD+eabb6x95syZI48//rhkZmZKv379JDs7W2bOnClFRUXnXJeysjJJTEy0HqmpqefMAgCAvu2a+rTdK6+8IhkZGZKZmSkOh0OmT58uU6ZMkdjYvy3DypUr5d1335U//OEPsn37dnnnnXdk8eLF8s4775zzuHPnzpVwOGw9Dhw4cCVOBwAA9IJ4k/CQIUMkLi5OWltbbdtbW1vF5/N1u4/P5ztv/szP1tZWSU5OtmXGjBljZQ4dOmQ7xunTp6Wtrc12nO5e5/uvccMNN8jq1avlxIkTcuTIEUlJSZE5c+bIiBEjrH1mz55t3X0SEcnKypIvv/xSysrKpKSkpNtzdDqd4nQ6ux0DAABXF6M7Tw6HQ8aNGydVVVXWtmg0KlVVVZKbm9vtPrm5uba8iEhlZaWVT09PF5/PZ8tEIhEJBoNWJjc3V9rb26W+vt7KbNq0SaLRqPj9fitTXV0tp06dsr3OyJEjZeDAgbbXd7lcMnToUDl9+rS8//77MnnyZGvsu+++s92JEhGJi4uTaDR64QUCAABXP9N3oldUVKjT6dTly5drU1OTlpaWqsfj0VAopKqqxcXFOmfOHCu/ZcsWjY+P18WLF+vevXt1wYIF3X5Vgcfj0TVr1uiuXbt08uTJ3X5VQXZ2tgaDQa2pqdGMjAzbVxW0t7er1+vV4uJibWxs1IqKCu3fv7/tqwrq6ur0/fff1y+++EKrq6v1Rz/6kaanp+vRo0etTElJiQ4dOtT6qoIPPvhAhwwZos8//3yP14hP2wEA0Pdctq8qUFVdsmSJpqWlqcPh0PHjx2tdXZ01NmHCBC0pKbHlV65cqbfccos6HA4dNWqUrlu3zjYejUZ13rx56vV61el06sSJE7W5udmWOXLkiBYWFuqAAQPU7XbrlClTtKOjw5bZuXOn3n333ep0OnXo0KFaXl5uG9+8ebPeeuut6nQ6dfDgwVpcXGx91cEZkUhEZ8yYoWlpaepyuXTEiBH64osv2r4C4UIoTwAA9D09vX7HqPLV2ZdaJBKRxMRECYfD4na7e3s6AACgB3p6/b6mPm0HAADwj6I8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGKA8AQAAGLio8rR06VIZPny4uFwu8fv9snXr1vPmV61aJZmZmeJyuSQrK0vWr19vG1dVmT9/viQnJ0tCQoIEAgHZt2+fLdPW1iZFRUXidrvF4/HI1KlT5dixY7bMrl275J577hGXyyWpqamycOFC2/ipU6fkV7/6ldx0003icrlk9OjRsnHjxrPme/DgQfnZz34mgwcPloSEBMnKypLPPvvMZIkAAMBVyrg8vffeezJr1ixZsGCBbN++XUaPHi15eXly6NChbvOffvqpFBYWytSpU6WhoUEKCgqkoKBAGhsbrczChQvl1VdflWXLlkkwGJTrrrtO8vLy5MSJE1amqKhI9uzZI5WVlbJ27Vqprq6W0tJSazwSicgDDzwgw4YNk/r6elm0aJH813/9l7z11ltW5pe//KW8+eabsmTJEmlqapJnnnlGHn74YWloaLAyR48elbvuukv69esnGzZskKamJvnv//5vGThwoOlSAQCAq5EaGj9+vE6bNs163tXVpSkpKVpWVtZt/tFHH9X8/HzbNr/fr08//bSqqkajUfX5fLpo0SJrvL29XZ1Op65YsUJVVZuamlREdNu2bVZmw4YNGhMTowcPHlRV1ddff10HDhyonZ2dVuaFF17QkSNHWs+Tk5P1tddes83lkUce0aKiIts+d999d88W4xzC4bCKiIbD4X/oOAAA4Mrp6fXb6M7TyZMnpb6+XgKBgLUtNjZWAoGA1NbWdrtPbW2tLS8ikpeXZ+VbWlokFArZMomJieL3+61MbW2teDweycnJsTKBQEBiY2MlGAxamXvvvVccDoftdZqbm+Xo0aMiItLZ2Skul8s2l4SEBKmpqbGe//GPf5ScnBz56U9/KklJSZKdnS2//e1vz7sunZ2dEolEbA8AAHB1MipPhw8flq6uLvF6vbbtXq9XQqFQt/uEQqHz5s/8vFAmKSnJNh4fHy+DBg2yZbo7xvdfIy8vT15++WXZt2+fRKNRqayslA8++EC++eYba5/9+/fLG2+8IRkZGfLRRx/Js88+K88995y8884751yXsrIySUxMtB6pqannzAIAgL7tmvq03SuvvCIZGRmSmZkpDodDpk+fLlOmTJHY2L8tQzQalbFjx8pvfvMbyc7OltLSUnnqqadk2bJl5zzu3LlzJRwOW48DBw5cidMBAAC9wKg8DRkyROLi4qS1tdW2vbW1VXw+X7f7+Hy+8+bP/LxQ5u/fkH769Glpa2uzZbo7xvdf44YbbpDVq1fLt99+K19++aX85S9/kQEDBsiIESOsfZKTk+W2226zHefWW2+Vr776qtvzExFxOp3idrttDwAAcHUyKk8Oh0PGjRsnVVVV1rZoNCpVVVWSm5vb7T65ubm2vIhIZWWllU9PTxefz2fLRCIRCQaDViY3N1fa29ulvr7eymzatEmi0aj4/X4rU11dLadOnbK9zsiRI8/6pJzL5ZKhQ4fK6dOn5f3335fJkydbY3fddZc0Nzfb8p9//rkMGzbswgsEAACufqbvRK+oqFCn06nLly/XpqYmLS0tVY/Ho6FQSFVVi4uLdc6cOVZ+y5YtGh8fr4sXL9a9e/fqggULtF+/frp7924rU15erh6PR9esWaO7du3SyZMna3p6uh4/ftzKTJo0SbOzszUYDGpNTY1mZGRoYWGhNd7e3q5er1eLi4u1sbFRKyoqtH///vrmm29ambq6On3//ff1iy++0Orqav3Rj36k6enpevToUSuzdetWjY+P15deekn37dun7777rvbv319///vf93iN+LQdAAB9T0+v38blSVV1yZIlmpaWpg6HQ8ePH691dXXW2IQJE7SkpMSWX7lypd5yyy3qcDh01KhRum7dOtt4NBrVefPmqdfrVafTqRMnTtTm5mZb5siRI1pYWKgDBgxQt9utU6ZM0Y6ODltm586devfdd6vT6dShQ4dqeXm5bXzz5s166623qtPp1MGDB2txcbH1VQff96c//Ulvv/12dTqdmpmZqW+99ZbR+lCeAADoe3p6/Y5RVe3de19Xn0gkIomJiRIOh3n/EwAAfURPr9/X1KftAAAA/lGUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAOUJwAAAAPxvT2Bq5GqiohIJBLp5ZkAAICeOnPdPnMdPxfK02XQ0dEhIiKpqam9PBMAAGCqo6NDEhMTzzkeoxeqVzAWjUbl66+/luuvv15iYmIu2XEjkYikpqbKgQMHxO12X7Ljwo51vnJY6yuDdb4yWOcr53KttapKR0eHpKSkSGzsud/ZxJ2nyyA2NlZuvPHGy3Z8t9vNH8wrgHW+cljrK4N1vjJY5yvncqz1+e44ncEbxgEAAAxQngAAAAxQnvoQp9MpCxYsEKfT2dtTuaqxzlcOa31lsM5XBut85fT2WvOGcQAAAAPceQIAADBAeQIAADBAeQIAADBAeQIAADBAeepDli5dKsOHDxeXyyV+v1+2bt3a21PqM8rKyuSOO+6Q66+/XpKSkqSgoECam5ttmRMnTsi0adNk8ODBMmDAAPmXf/kXaW1ttWW++uoryc/Pl/79+0tSUpLMnj1bTp8+fSVPpU8pLy+XmJgYmTlzprWNdb50Dh48KD/72c9k8ODBkpCQIFlZWfLZZ59Z46oq8+fPl+TkZElISJBAICD79u2zHaOtrU2KiorE7XaLx+ORqVOnyrFjx670qfzT6urqknnz5kl6erokJCTITTfdJL/+9a9t//YZ63xxqqur5aGHHpKUlBSJiYmR1atX28Yv1bru2rVL7rnnHnG5XJKamioLFy78xyev6BMqKirU4XDo7373O92zZ48+9dRT6vF4tLW1tben1ifk5eXp22+/rY2Njbpjxw798Y9/rGlpaXrs2DEr88wzz2hqaqpWVVXpZ599pj/84Q/1zjvvtMZPnz6tt99+uwYCAW1oaND169frkCFDdO7cub1xSv/0tm7dqsOHD9cf/OAHOmPGDGs763xptLW16bBhw/SJJ57QYDCo+/fv148++kj/53/+x8qUl5drYmKirl69Wnfu3Kk/+clPND09XY8fP25lJk2apKNHj9a6ujr985//rDfffLMWFhb2xin9U3rppZd08ODBunbtWm1padFVq1bpgAED9JVXXrEyrPPFWb9+vb744ov6wQcfqIjohx9+aBu/FOsaDofV6/VqUVGRNjY26ooVKzQhIUHffPPNf2julKc+Yvz48Tpt2jTreVdXl6akpGhZWVkvzqrvOnTokIqIfvLJJ6qq2t7erv369dNVq1ZZmb1796qIaG1trar+9Q96bGyshkIhK/PGG2+o2+3Wzs7OK3sC/+Q6Ojo0IyNDKysrdcKECVZ5Yp0vnRdeeEHvvvvuc45Ho1H1+Xy6aNEia1t7e7s6nU5dsWKFqqo2NTWpiOi2bduszIYNGzQmJkYPHjx4+Sbfh+Tn5+vPf/5z27ZHHnlEi4qKVJV1vlT+vjxdqnV9/fXXdeDAgba/O1544QUdOXLkPzRffm3XB5w8eVLq6+slEAhY22JjYyUQCEhtbW0vzqzvCofDIiIyaNAgERGpr6+XU6dO2dY4MzNT0tLSrDWura2VrKws8Xq9ViYvL08ikYjs2bPnCs7+n9+0adMkPz/ftp4irPOl9Mc//lFycnLkpz/9qSQlJUl2drb89re/tcZbWlokFArZ1joxMVH8fr9trT0ej+Tk5FiZQCAgsbGxEgwGr9zJ/BO78847paqqSj7//HMREdm5c6fU1NTIgw8+KCKs8+Vyqda1trZW7r33XnE4HFYmLy9Pmpub5ejRoxc9P/5h4D7g8OHD0tXVZbuYiIh4vV75y1/+0kuz6rui0ajMnDlT7rrrLrn99ttFRCQUConD4RCPx2PLer1eCYVCVqa7/wdnxvBXFRUVsn37dtm2bdtZY6zzpbN//3554403ZNasWfKf//mfsm3bNnnuuefE4XBISUmJtVbdreX31zopKck2Hh8fL4MGDWKt/785c+ZIJBKRzMxMiYuLk66uLnnppZekqKhIRIR1vkwu1bqGQiFJT08/6xhnxgYOHHhR86M84Zozbdo0aWxslJqamt6eylXnwIEDMmPGDKmsrBSXy9Xb07mqRaNRycnJkd/85jciIpKdnS2NjY2ybNkyKSkp6eXZXT1Wrlwp7777rvzhD3+QUaNGyY4dO2TmzJmSkpLCOl/D+LVdHzBkyBCJi4s76xNJra2t4vP5emlWfdP06dNl7dq18vHHH8uNN95obff5fHLy5Elpb2+35b+/xj6fr9v/B2fG8Ndfyx06dEjGjh0r8fHxEh8fL5988om8+uqrEh8fL16vl3W+RJKTk+W2226zbbv11lvlq6++EpG/rdX5/t7w+Xxy6NAh2/jp06elra2Ntf7/Zs+eLXPmzJHHH39csrKypLi4WP793/9dysrKRIR1vlwu1bperr9PKE99gMPhkHHjxklVVZW1LRqNSlVVleTm5vbizPoOVZXp06fLhx9+KJs2bTrrNu64ceOkX79+tjVubm6Wr776ylrj3Nxc2b17t+0Pa2Vlpbjd7rMuYteqiRMnyu7du2XHjh3WIycnR4qKiqz/Zp0vjbvuuuusr9v4/PPPZdiwYSIikp6eLj6fz7bWkUhEgsGgba3b29ulvr7eymzatEmi0aj4/f4rcBb//L777juJjbVfKuPi4iQajYoI63y5XKp1zc3Nlerqajl16pSVqayslJEjR170r+xEhK8q6CsqKirU6XTq8uXLtampSUtLS9Xj8dg+kYRze/bZZzUxMVE3b96s33zzjfX47rvvrMwzzzyjaWlpumnTJv3ss880NzdXc3NzrfEzH6F/4IEHdMeOHbpx40a94YYb+Aj9BXz/03aqrPOlsnXrVo2Pj9eXXnpJ9+3bp++++672799ff//731uZ8vJy9Xg8umbNGt21a5dOnjy52496Z2dnazAY1JqaGs3IyLjmP0L/fSUlJTp06FDrqwo++OADHTJkiD7//PNWhnW+OB0dHdrQ0KANDQ0qIvryyy9rQ0ODfvnll6p6ada1vb1dvV6vFhcXa2Njo1ZUVGj//v35qoJryZIlSzQtLU0dDoeOHz9e6+rqentKfYaIdPt4++23rczx48f1X//1X3XgwIHav39/ffjhh/Wbb76xHed///d/9cEHH9SEhAQdMmSI/uIXv9BTp05d4bPpW/6+PLHOl86f/vQnvf3229XpdGpmZqa+9dZbtvFoNKrz5s1Tr9erTqdTJ06cqM3NzbbMkSNHtLCwUAcMGKBut1unTJmiHR0dV/I0/qlFIhGdMWOGpqWlqcvl0hEjRuiLL75o++g763xxPv74427/Xi4pKVHVS7euO3fu1LvvvludTqcOHTpUy8vL/+G5x6h+72tSAQAAcF685wkAAMAA5QkAAMAA5QkAAMAA5QkAAMAA5QkAAMAA5QkAAMAA5QkAAMAA5QkAAMAA5QkAAMAA5QkAAMAA5QkAAMAA5QkAAMDA/wOKVjPI1beDsgAAAABJRU5ErkJggg=='}, 'metadata': {}, 'transient': {}}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
import matplotlib.pyplot as plt
import math
max_lr = 6e-4
min_lr = max_lr * 0.1
warmup_steps = 0
max_steps = 840 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens
def get_lr(it):
    # 1) linear warmup for warmup_iters steps
    if it < warmup_steps:
        return max_lr * (it+1) / warmup_steps
    # 2) if it > lr_decay_iters, return min learning rate
    if it > max_steps:
        return min_lr
    # 3) in between, use cosine decay down to min learning rate
    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)
    assert 0 <= decay_ratio <= 1
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0
    return min_lr + coeff * (max_lr - min_lr)

plt.plot([get_lr(i) for i in range(840)])
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'import matplotlib.pyplot as plt\nimport math\nmax_lr = 6e-4\nmin_lr = max_lr * 0.1\nwarmup_steps = 0\nmax_steps = 840 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens\ndef get_lr(it):\n    # 1) linear warmup for warmup_iters steps\n    if it < warmup_steps:\n        return max_lr * (it+1) / warmup_steps\n    # 2) if it > lr_decay_iters, return min learning rate\n    if it > max_steps:\n        return min_lr\n    # 3) in between, use cosine decay down to min learning rate\n    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n    assert 0 <= decay_ratio <= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n    return min_lr + coeff * (max_lr - min_lr)\n\nplt.plot([get_lr(i) for i in range(840)])', 'execution_count': 12}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': '[<matplotlib.lines.Line2D at 0x7fb2f8813d90>]'}, 'metadata': {}, 'execution_count': 12}
[NbConvertApp] msg_type: display_data
[NbConvertApp] content: {'data': {'text/plain': '<Figure size 640x480 with 1 Axes>', 'image/png': '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'}, 'metadata': {}, 'transient': {}}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:


@dataclass
class TransformerTrainGenKeys(BaseTrainGenKeys):
    src_ids='src_ids'
    tgt_ids='tgt_ids'
    encoder_mask='encoder_mask'
    decoder_mask='decoder_mask'
    sampled_src='sampled_src'
    sampled_tgt='sampled_tgt'
    sampled_pred='sampled_pred'
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': "\n\n@dataclass\nclass TransformerTrainGenKeys(BaseTrainGenKeys):\n    src_ids='src_ids'\n    tgt_ids='tgt_ids'\n    encoder_mask='encoder_mask'\n    decoder_mask='decoder_mask'\n    sampled_src='sampled_src'\n    sampled_tgt='sampled_tgt'\n    sampled_pred='sampled_pred'", 'execution_count': 13}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
import sys
import wandb

class TrainLogger:
    def __init__(self,cfg):
        self.test_scores=[]
        wandb.init(project='transformer-jesc',config=cfg)
    
    def write(self,g):
        wandb.log({BaseTrainGenKeys.epoch:g[BaseTrainGenKeys.epoch],BaseTrainGenKeys.train_loss:g[BaseTrainGenKeys.train_loss],BaseTrainGenKeys.test_loss:g[BaseTrainGenKeys.test_loss],TransformerTrainGenKeys.sampled_src:g[TransformerTrainGenKeys.sampled_src],TransformerTrainGenKeys.sampled_tgt:g[TransformerTrainGenKeys.sampled_tgt],TransformerTrainGenKeys.sampled_pred:g[TransformerTrainGenKeys.sampled_pred]})


    def log(self, epoch, batch_step, max_batch_step, train_loss, test_loss: float|None):
        if batch_step!=0:
            print("\033[F\033[K",end="")
            print("\033[F\033[K",end="")
        if max_batch_step==0:
            max_batch_step=1
        progress=['=' for _ in range(int(batch_step/max_batch_step*50))]
        progress.append('>')
        progress.extend([' ' for _ in range(50-len(progress))])
        train_loss = f'{train_loss:.4f}'
        if test_loss is None:
            print(f'epoch:{epoch} batch_step:{batch_step}/{max_batch_step} [{"".join(progress)}] train_loss:{train_loss}')
        else:
            self.test_scores.append(test_loss)
            test_loss = f'{test_loss:.4f}'
            print(f'epoch:{epoch} batch_step:{batch_step}/{max_batch_step} [{"".join(progress)}] train_loss:{train_loss} test_loss:{test_loss}')

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'import sys\nimport wandb\n\nclass TrainLogger:\n    def __init__(self,cfg):\n        self.test_scores=[]\n        wandb.init(project=\'transformer-jesc\',config=cfg)\n    \n    def write(self,g):\n        wandb.log({BaseTrainGenKeys.epoch:g[BaseTrainGenKeys.epoch],BaseTrainGenKeys.train_loss:g[BaseTrainGenKeys.train_loss],BaseTrainGenKeys.test_loss:g[BaseTrainGenKeys.test_loss],TransformerTrainGenKeys.sampled_src:g[TransformerTrainGenKeys.sampled_src],TransformerTrainGenKeys.sampled_tgt:g[TransformerTrainGenKeys.sampled_tgt],TransformerTrainGenKeys.sampled_pred:g[TransformerTrainGenKeys.sampled_pred]})\n\n\n    def log(self, epoch, batch_step, max_batch_step, train_loss, test_loss: float|None):\n        if batch_step!=0:\n            print("\\033[F\\033[K",end="")\n            print("\\033[F\\033[K",end="")\n        if max_batch_step==0:\n            max_batch_step=1\n        progress=[\'=\' for _ in range(int(batch_step/max_batch_step*50))]\n        progress.append(\'>\')\n        progress.extend([\' \' for _ in range(50-len(progress))])\n        train_loss = f\'{train_loss:.4f}\'\n        if test_loss is None:\n            print(f\'epoch:{epoch} batch_step:{batch_step}/{max_batch_step} [{"".join(progress)}] train_loss:{train_loss}\')\n        else:\n            self.test_scores.append(test_loss)\n            test_loss = f\'{test_loss:.4f}\'\n            print(f\'epoch:{epoch} batch_step:{batch_step}/{max_batch_step} [{"".join(progress)}] train_loss:{train_loss} test_loss:{test_loss}\')\n', 'execution_count': 14}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:

from adabelief_pytorch import AdaBelief
class SchedulerType:
    CosineAnnealingLR="CosineAnnealingLR"
    CosineAnnealingWarmRestarts="CosineAnnealingWarmRestarts"
    CosineAnnealingWarmUp="CosineAnnealingWarmUp"
    CossineLRDecay="CossineLRDecay"
    StepLR="StepLR"
    NoneType="NoneType"
@dataclass
class CFG(BaseTrainPipelineConfig):
    batch_size:int=128
    skip=13920
    max_epochs:int|None=1
    is_running:bool=True
    debug:bool=False
    max_len: int = maker.src_tokenizer.max_length
    lr: float =0.0007 #0.0010466176# 2.79936e-05 #2.79936e-05*6#
    min_lr: float = 2.79936e-05
    accum_iter: int = 512//128//2
    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'
    embedding_dim: int = 512
    hidden_dim: int = 512
    num_heads: int = 12
    num_blocks: int = 3
    encode_vocab_size: int = maker.src_tokenizer.tokenizer.get_vocab().keys().__len__()
    decode_vocab_size: int = maker.tgt_tokenizer.tokenizer.get_vocab().keys().__len__()
    pad_idx:int=maker.src_tokenizer.tokenizer.pad_token_id # type: ignore
    tokenizer=maker
    optimizer=torch.optim.AdamW#AdaBelief#torch.optim.RAdam#
    default_dtype=torch.bfloat16
    eval_steps:int=60
    scheduler=SchedulerType.CossineLRDecay
    warmup_percent=0.0015

def cfg_to_dict(cfg):
    return {k:v for k,v in cfg.__dict__.items() if not k.startswith('_')}
logger=TrainLogger(cfg_to_dict(CFG()))
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '\nfrom adabelief_pytorch import AdaBelief\nclass SchedulerType:\n    CosineAnnealingLR="CosineAnnealingLR"\n    CosineAnnealingWarmRestarts="CosineAnnealingWarmRestarts"\n    CosineAnnealingWarmUp="CosineAnnealingWarmUp"\n    CossineLRDecay="CossineLRDecay"\n    StepLR="StepLR"\n    NoneType="NoneType"\n@dataclass\nclass CFG(BaseTrainPipelineConfig):\n    batch_size:int=128\n    skip=13920\n    max_epochs:int|None=1\n    is_running:bool=True\n    debug:bool=False\n    max_len: int = maker.src_tokenizer.max_length\n    lr: float =0.0007 #0.0010466176# 2.79936e-05 #2.79936e-05*6#\n    min_lr: float = 2.79936e-05\n    accum_iter: int = 512//128//2\n    device: str = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n    embedding_dim: int = 512\n    hidden_dim: int = 512\n    num_heads: int = 12\n    num_blocks: int = 3\n    encode_vocab_size: int = maker.src_tokenizer.tokenizer.get_vocab().keys().__len__()\n    decode_vocab_size: int = maker.tgt_tokenizer.tokenizer.get_vocab().keys().__len__()\n    pad_idx:int=maker.src_tokenizer.tokenizer.pad_token_id # type: ignore\n    tokenizer=maker\n    optimizer=torch.optim.AdamW#AdaBelief#torch.optim.RAdam#\n    default_dtype=torch.bfloat16\n    eval_steps:int=60\n    scheduler=SchedulerType.CossineLRDecay\n    warmup_percent=0.0015\n\ndef cfg_to_dict(cfg):\n    return {k:v for k,v in cfg.__dict__.items() if not k.startswith(\'_\')}\nlogger=TrainLogger(cfg_to_dict(CFG()))', 'execution_count': 15}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': '\x1b[34m\x1b[1mwandb\x1b[0m: Currently logged in as: \x1b[33myama-yeah\x1b[0m (\x1b[33mgrad-exp\x1b[0m). Use \x1b[1m`wandb login --relogin`\x1b[0m to force relogin\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': '\x1b[34m\x1b[1mwandb\x1b[0m: wandb version 0.17.4 is available!  To upgrade, please run:\n\x1b[34m\x1b[1mwandb\x1b[0m:  $ pip install wandb --upgrade\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': '\x1b[34m\x1b[1mwandb\x1b[0m: Tracking run with wandb version 0.17.3\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': '\x1b[34m\x1b[1mwandb\x1b[0m: Run data is saved locally in \x1b[35m\x1b[1m/home/rain/exp_env/src/work/translate/wandb/run-20240704_134805-n2rfl94h\x1b[0m\n\x1b[34m\x1b[1mwandb\x1b[0m: Run \x1b[1m`wandb offline`\x1b[0m to turn off syncing.\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': '\x1b[34m\x1b[1mwandb\x1b[0m: Syncing run \x1b[33mautumn-firefly-115\x1b[0m\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': '\x1b[34m\x1b[1mwandb\x1b[0m: ‚≠êÔ∏è View project at \x1b[34m\x1b[4mhttps://wandb.ai/grad-exp/transformer-jesc\x1b[0m\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': '\x1b[34m\x1b[1mwandb\x1b[0m: üöÄ View run at \x1b[34m\x1b[4mhttps://wandb.ai/grad-exp/transformer-jesc/runs/n2rfl94h\x1b[0m\n'}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:


class TransformerTrainPipeline(BaseTrainPipeline):
    def __init__(self,train_src,train_tgt,test_src,test_tgt,config:CFG=CFG()):
        super().__init__(config)
        self.encoder=transformer.Encoder(
            how_many_block=config.num_blocks,
            how_many_heads=config.num_heads,
            vocab_size=config.encode_vocab_size,
            embedding_dim=config.embedding_dim,
            hidden_dim=config.hidden_dim,
            pad_idx= config.pad_idx,
            max_seq_len=config.max_len,
            layer_norm_eps=1e-6,
        ).to(config.device)
        self.decoder=transformer.Decoder(
            how_many_block=config.num_blocks,
            how_many_heads=config.num_heads,
            vocab_size=config.decode_vocab_size,
            embedding_dim=config.embedding_dim,
            hidden_dim=config.hidden_dim,
            pad_idx= config.pad_idx,
            max_seq_len=config.max_len,
            layer_norm_eps=1e-6,
        ).to(config.device)
        self.optimizer=config.optimizer(list(self.encoder.parameters())[1:]+list(self.decoder.parameters())[1:],lr=config.lr,betas=(0.9,0.95),eps=1e-8,fused=True)#,eps=1e-16, betas=(0.9,0.999), weight_decouple = True, rectify = True)
        self.optimizer2=torch.optim.RAdam(list(self.encoder.parameters())[0:1]+list(self.decoder.parameters())[0:1],lr=config.min_lr,betas=(0.9,0.95),eps=1e-8)
        if config.scheduler==SchedulerType.CosineAnnealingLR:
            self.scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer,int(2000/config.batch_size*20*5/config.accum_iter),config.min_lr)
        elif config.scheduler==SchedulerType.CosineAnnealingWarmUp:
            self.scheduler=CosineAnnealingLR(self.optimizer,int(2000/config.batch_size*10*5/config.accum_iter),int(2000/config.batch_size*20*5/config.accum_iter),config.min_lr,config.min_lr)
        elif config.scheduler==SchedulerType.CossineLRDecay:
            self.scheduler=CossineLRDecay(self.optimizer,lr_max=config.lr,lr_min=config.min_lr,max_steps=len(train_src)//config.batch_size*config.max_epochs//config.accum_iter,warmup_steps=int(len(train_src)//config.batch_size*config.max_epochs//config.accum_iter*config.warmup_percent))
        elif config.scheduler==SchedulerType.StepLR:
            self.scheduler=torch.optim.lr_scheduler.StepLR(self.optimizer,step_size=1,gamma=6)
        if config.scheduler!=SchedulerType.NoneType:
            self.scheduler.last_step=self.config.skip-1
        #self.scheduler=CossineLRDecay(self.optimizer,lr_max=config.lr,lr_min=2.79936e-05,max_steps=840//2,warmup_steps=0)
        #self.scheduler=torch.optim.lr_scheduler.StepLR(self.optimizer,step_size=1,gamma=6)
        ## type: ignore
        self.config:CFG
            
    def id_mask_split(self,G):
        for g in G:
            g[TransformerTrainGenKeys.src_ids]=g[BaseTrainGenKeys.batch_x][0].squeeze(1)
            #print(maker.src_tokenizer.tokenizer.decode(g[BaseTrainGenKeys.batch_x][0].squeeze(1)[0].tolist()))
            g[TransformerTrainGenKeys.encoder_mask]=g[BaseTrainGenKeys.batch_x][1].squeeze(1)
            g[TransformerTrainGenKeys.tgt_ids]=g[BaseTrainGenKeys.batch_y][0].squeeze(1)
            #print(maker.tgt_tokenizer.tokenizer.decode(g[BaseTrainGenKeys.batch_y][0].squeeze(1)[0].tolist()))
            g[TransformerTrainGenKeys.decoder_mask]=g[BaseTrainGenKeys.batch_y][1].squeeze(1)
            yield g
    
    def forward(self,G):
        self.encoder.train()
        self.decoder.train()
        for g in G:
            with torch.set_grad_enabled(True):
                loss=None
                self.optimizer.zero_grad()
                y=self.encoder(g[TransformerTrainGenKeys.src_ids].to(self.config.device),g[TransformerTrainGenKeys.encoder_mask].to(self.config.device))
                y=self.decoder(g[TransformerTrainGenKeys.tgt_ids].to(self.config.device),y,g[TransformerTrainGenKeys.decoder_mask].to(self.config.device))
                tgt_ids=g[TransformerTrainGenKeys.tgt_ids][:,1:].to(self.config.device)
                # one hot encode
                y=y[:,:-1]
                tgt_ids=tgt_ids.contiguous().view(-1)
                y=y.contiguous().view(-1,self.config.decode_vocab_size)
                loss = torch.nn.CrossEntropyLoss(
                    ignore_index=self.config.pad_idx
                )(y, tgt_ids)/self.config.accum_iter
                del tgt_ids,y
                gc.collect()
                torch.cuda.empty_cache()
                loss.backward()
                if g[BaseTrainGenKeys.batch_step] % self.config.accum_iter == 0 or g[BaseTrainGenKeys.batch_step] == g[BaseTrainGenKeys.max_batch_step]:
                    # print(loss.item())
                    torch.nn.utils.clip_grad_norm_(list(self.encoder.parameters())+list(self.decoder.parameters()), 1.0)
                    self.optimizer.step()
                    self.optimizer2.step()
                    if self.scheduler is not None and self.config.scheduler!=SchedulerType.StepLR:
                        self.scheduler.step()
                
                g[BaseTrainGenKeys.train_loss]=loss.item()*self.config.accum_iter
                
            yield g
    def test(self,G):
        for g in G:
            
            with torch.no_grad():
                self.encoder.eval()
                self.decoder.eval()
                src_ids=g[TransformerTrainGenKeys.src_ids].to(self.config.device)
                encoder_mask=g[TransformerTrainGenKeys.encoder_mask].to(self.config.device)
                tgt_ids=g[TransformerTrainGenKeys.tgt_ids].to(self.config.device)
                decoder_mask=g[TransformerTrainGenKeys.decoder_mask].to(self.config.device)
                y=self.encoder(src_ids,encoder_mask)
                y=self.decoder(tgt_ids,y,decoder_mask)
                tgt_ids=tgt_ids[:,1:]
                y=y[:,:-1]
                if g[BaseTrainGenKeys.batch_step]==g[BaseTrainGenKeys.max_batch_step]:
                    how_many=3
                    src_text=maker.src_tokenizer.tokenizer.batch_decode(src_ids[:how_many].tolist())
                    src_text=[text.replace('[PAD]','') for text in src_text]
                    tgt_text=maker.tgt_tokenizer.tokenizer.batch_decode(tgt_ids[:how_many].tolist())
                    tgt_text=[text.replace('[PAD]','') for text in tgt_text]
                    pred_text=maker.tgt_tokenizer.tokenizer.batch_decode(torch.argmax(y,dim=-1)[:how_many].tolist())
                    pred_text=[text.replace('[PAD]','') for text in pred_text]
                    #EOS„Åæ„Åß„ÅÆ„ÅøË°®Á§∫
                    pred_text=[text[:text.find('[EOS]')+5] for text in pred_text]
                    print(src_text)
                    print(tgt_text)
                    print(pred_text)
                    g[TransformerTrainGenKeys.sampled_src]=src_text[0]
                    g[TransformerTrainGenKeys.sampled_tgt]=tgt_text[0]
                    g[TransformerTrainGenKeys.sampled_pred]=pred_text[0]
                tgt_ids=tgt_ids.contiguous().view(-1)
                y=y.contiguous().view(-1,self.config.decode_vocab_size)
                loss = torch.nn.CrossEntropyLoss(ignore_index=self.config.pad_idx)(y, tgt_ids).cpu()
                g[BaseTrainGenKeys.test_loss]=loss.detach().item()
            yield g
    def run_after_epoch(self,G):
        for g in G:
            logger.log(g[BaseTrainGenKeys.epoch],g[BaseTrainGenKeys.batch_step],g[BaseTrainGenKeys.max_batch_step],g[BaseTrainGenKeys.train_loss],g.get(BaseTrainGenKeys.test_loss,None))
            if g[BaseTrainGenKeys.batch_step] == g[BaseTrainGenKeys.max_batch_step]:
                if self.config.scheduler==SchedulerType.StepLR:
                    self.scheduler.step()
                    print(self.optimizer.param_groups[0]['lr'])
                yield g
    def run_eval_each_step(self,G):
        train_loss=0
        cnt=0
        for g in G:
            train_loss+=g[BaseTrainGenKeys.train_loss]
            cnt+=1
            logger.log(g[BaseTrainGenKeys.epoch],g[BaseTrainGenKeys.batch_step],g[BaseTrainGenKeys.max_batch_step],g[BaseTrainGenKeys.train_loss],g.get(BaseTrainGenKeys.test_loss,None))
            if g[BaseTrainGenKeys.batch_step]% (self.config.accum_iter*self.config.eval_steps)==0 or g[BaseTrainGenKeys.batch_step] == g[BaseTrainGenKeys.max_batch_step]:
                g[BaseTrainGenKeys.train_loss]=train_loss/cnt
                train_loss=0
                cnt=0
                yield g
    def run_after_test(self,G):
        loss=0
        for g in G:
            if g[BaseTrainGenKeys.batch_step] == g[BaseTrainGenKeys.max_batch_step]:
                g[BaseTrainGenKeys.test_loss]+=loss
                g[BaseTrainGenKeys.test_loss]/=g[BaseTrainGenKeys.max_batch_step]
                loss=0
                yield g
            else:
                loss+=g[BaseTrainGenKeys.test_loss]
    def reduce_mem_gen(self,G):
        for g in G:
            torch.cuda.empty_cache()
            gc.collect()
            yield g
config=CFG()
#test
pipeline=TransformerTrainPipeline(train_x,train_y,test_x,test_y,config)
# pipeline.encoder.embedding.load_state_dict(torch.load('/home/rain/exp_env/src/work/translate/model/fasttext_jp.pth'))
# pipeline.decoder.embedding.load_state_dict(torch.load('/home/rain/exp_env/src/work/translate/model/fasttext_en.pth'))
pipeline.encoder.load_state_dict(torch.load('encoder.pth'))
pipeline.decoder.load_state_dict(torch.load('decoder.pth'))
# compile
pipeline.encoder=torch.compile(pipeline.encoder,mode="reduce-overhead")
pipeline.decoder=torch.compile(pipeline.decoder,mode="reduce-overhead")
# pipeline.encoder.embedding.requires_grad=False
# pipeline.decoder.embedding.requires_grad=False

[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': '\n\nclass TransformerTrainPipeline(BaseTrainPipeline):\n    def __init__(self,train_src,train_tgt,test_src,test_tgt,config:CFG=CFG()):\n        super().__init__(config)\n        self.encoder=transformer.Encoder(\n            how_many_block=config.num_blocks,\n            how_many_heads=config.num_heads,\n            vocab_size=config.encode_vocab_size,\n            embedding_dim=config.embedding_dim,\n            hidden_dim=config.hidden_dim,\n            pad_idx= config.pad_idx,\n            max_seq_len=config.max_len,\n            layer_norm_eps=1e-6,\n        ).to(config.device)\n        self.decoder=transformer.Decoder(\n            how_many_block=config.num_blocks,\n            how_many_heads=config.num_heads,\n            vocab_size=config.decode_vocab_size,\n            embedding_dim=config.embedding_dim,\n            hidden_dim=config.hidden_dim,\n            pad_idx= config.pad_idx,\n            max_seq_len=config.max_len,\n            layer_norm_eps=1e-6,\n        ).to(config.device)\n        self.optimizer=config.optimizer(list(self.encoder.parameters())[1:]+list(self.decoder.parameters())[1:],lr=config.lr,betas=(0.9,0.95),eps=1e-8,fused=True)#,eps=1e-16, betas=(0.9,0.999), weight_decouple = True, rectify = True)\n        self.optimizer2=torch.optim.RAdam(list(self.encoder.parameters())[0:1]+list(self.decoder.parameters())[0:1],lr=config.min_lr,betas=(0.9,0.95),eps=1e-8)\n        if config.scheduler==SchedulerType.CosineAnnealingLR:\n            self.scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer,int(2000/config.batch_size*20*5/config.accum_iter),config.min_lr)\n        elif config.scheduler==SchedulerType.CosineAnnealingWarmUp:\n            self.scheduler=CosineAnnealingLR(self.optimizer,int(2000/config.batch_size*10*5/config.accum_iter),int(2000/config.batch_size*20*5/config.accum_iter),config.min_lr,config.min_lr)\n        elif config.scheduler==SchedulerType.CossineLRDecay:\n            self.scheduler=CossineLRDecay(self.optimizer,lr_max=config.lr,lr_min=config.min_lr,max_steps=len(train_src)//config.batch_size*config.max_epochs//config.accum_iter,warmup_steps=int(len(train_src)//config.batch_size*config.max_epochs//config.accum_iter*config.warmup_percent))\n        elif config.scheduler==SchedulerType.StepLR:\n            self.scheduler=torch.optim.lr_scheduler.StepLR(self.optimizer,step_size=1,gamma=6)\n        if config.scheduler!=SchedulerType.NoneType:\n            self.scheduler.last_step=self.config.skip-1\n        #self.scheduler=CossineLRDecay(self.optimizer,lr_max=config.lr,lr_min=2.79936e-05,max_steps=840//2,warmup_steps=0)\n        #self.scheduler=torch.optim.lr_scheduler.StepLR(self.optimizer,step_size=1,gamma=6)\n        ## type: ignore\n        self.config:CFG\n            \n    def id_mask_split(self,G):\n        for g in G:\n            g[TransformerTrainGenKeys.src_ids]=g[BaseTrainGenKeys.batch_x][0].squeeze(1)\n            #print(maker.src_tokenizer.tokenizer.decode(g[BaseTrainGenKeys.batch_x][0].squeeze(1)[0].tolist()))\n            g[TransformerTrainGenKeys.encoder_mask]=g[BaseTrainGenKeys.batch_x][1].squeeze(1)\n            g[TransformerTrainGenKeys.tgt_ids]=g[BaseTrainGenKeys.batch_y][0].squeeze(1)\n            #print(maker.tgt_tokenizer.tokenizer.decode(g[BaseTrainGenKeys.batch_y][0].squeeze(1)[0].tolist()))\n            g[TransformerTrainGenKeys.decoder_mask]=g[BaseTrainGenKeys.batch_y][1].squeeze(1)\n            yield g\n    \n    def forward(self,G):\n        self.encoder.train()\n        self.decoder.train()\n        for g in G:\n            with torch.set_grad_enabled(True):\n                loss=None\n                self.optimizer.zero_grad()\n                y=self.encoder(g[TransformerTrainGenKeys.src_ids].to(self.config.device),g[TransformerTrainGenKeys.encoder_mask].to(self.config.device))\n                y=self.decoder(g[TransformerTrainGenKeys.tgt_ids].to(self.config.device),y,g[TransformerTrainGenKeys.decoder_mask].to(self.config.device))\n                tgt_ids=g[TransformerTrainGenKeys.tgt_ids][:,1:].to(self.config.device)\n                # one hot encode\n                y=y[:,:-1]\n                tgt_ids=tgt_ids.contiguous().view(-1)\n                y=y.contiguous().view(-1,self.config.decode_vocab_size)\n                loss = torch.nn.CrossEntropyLoss(\n                    ignore_index=self.config.pad_idx\n                )(y, tgt_ids)/self.config.accum_iter\n                del tgt_ids,y\n                gc.collect()\n                torch.cuda.empty_cache()\n                loss.backward()\n                if g[BaseTrainGenKeys.batch_step] % self.config.accum_iter == 0 or g[BaseTrainGenKeys.batch_step] == g[BaseTrainGenKeys.max_batch_step]:\n                    # print(loss.item())\n                    torch.nn.utils.clip_grad_norm_(list(self.encoder.parameters())+list(self.decoder.parameters()), 1.0)\n                    self.optimizer.step()\n                    self.optimizer2.step()\n                    if self.scheduler is not None and self.config.scheduler!=SchedulerType.StepLR:\n                        self.scheduler.step()\n                \n                g[BaseTrainGenKeys.train_loss]=loss.item()*self.config.accum_iter\n                \n            yield g\n    def test(self,G):\n        for g in G:\n            \n            with torch.no_grad():\n                self.encoder.eval()\n                self.decoder.eval()\n                src_ids=g[TransformerTrainGenKeys.src_ids].to(self.config.device)\n                encoder_mask=g[TransformerTrainGenKeys.encoder_mask].to(self.config.device)\n                tgt_ids=g[TransformerTrainGenKeys.tgt_ids].to(self.config.device)\n                decoder_mask=g[TransformerTrainGenKeys.decoder_mask].to(self.config.device)\n                y=self.encoder(src_ids,encoder_mask)\n                y=self.decoder(tgt_ids,y,decoder_mask)\n                tgt_ids=tgt_ids[:,1:]\n                y=y[:,:-1]\n                if g[BaseTrainGenKeys.batch_step]==g[BaseTrainGenKeys.max_batch_step]:\n                    how_many=3\n                    src_text=maker.src_tokenizer.tokenizer.batch_decode(src_ids[:how_many].tolist())\n                    src_text=[text.replace(\'[PAD]\',\'\') for text in src_text]\n                    tgt_text=maker.tgt_tokenizer.tokenizer.batch_decode(tgt_ids[:how_many].tolist())\n                    tgt_text=[text.replace(\'[PAD]\',\'\') for text in tgt_text]\n                    pred_text=maker.tgt_tokenizer.tokenizer.batch_decode(torch.argmax(y,dim=-1)[:how_many].tolist())\n                    pred_text=[text.replace(\'[PAD]\',\'\') for text in pred_text]\n                    #EOS„Åæ„Åß„ÅÆ„ÅøË°®Á§∫\n                    pred_text=[text[:text.find(\'[EOS]\')+5] for text in pred_text]\n                    print(src_text)\n                    print(tgt_text)\n                    print(pred_text)\n                    g[TransformerTrainGenKeys.sampled_src]=src_text[0]\n                    g[TransformerTrainGenKeys.sampled_tgt]=tgt_text[0]\n                    g[TransformerTrainGenKeys.sampled_pred]=pred_text[0]\n                tgt_ids=tgt_ids.contiguous().view(-1)\n                y=y.contiguous().view(-1,self.config.decode_vocab_size)\n                loss = torch.nn.CrossEntropyLoss(ignore_index=self.config.pad_idx)(y, tgt_ids).cpu()\n                g[BaseTrainGenKeys.test_loss]=loss.detach().item()\n            yield g\n    def run_after_epoch(self,G):\n        for g in G:\n            logger.log(g[BaseTrainGenKeys.epoch],g[BaseTrainGenKeys.batch_step],g[BaseTrainGenKeys.max_batch_step],g[BaseTrainGenKeys.train_loss],g.get(BaseTrainGenKeys.test_loss,None))\n            if g[BaseTrainGenKeys.batch_step] == g[BaseTrainGenKeys.max_batch_step]:\n                if self.config.scheduler==SchedulerType.StepLR:\n                    self.scheduler.step()\n                    print(self.optimizer.param_groups[0][\'lr\'])\n                yield g\n    def run_eval_each_step(self,G):\n        train_loss=0\n        cnt=0\n        for g in G:\n            train_loss+=g[BaseTrainGenKeys.train_loss]\n            cnt+=1\n            logger.log(g[BaseTrainGenKeys.epoch],g[BaseTrainGenKeys.batch_step],g[BaseTrainGenKeys.max_batch_step],g[BaseTrainGenKeys.train_loss],g.get(BaseTrainGenKeys.test_loss,None))\n            if g[BaseTrainGenKeys.batch_step]% (self.config.accum_iter*self.config.eval_steps)==0 or g[BaseTrainGenKeys.batch_step] == g[BaseTrainGenKeys.max_batch_step]:\n                g[BaseTrainGenKeys.train_loss]=train_loss/cnt\n                train_loss=0\n                cnt=0\n                yield g\n    def run_after_test(self,G):\n        loss=0\n        for g in G:\n            if g[BaseTrainGenKeys.batch_step] == g[BaseTrainGenKeys.max_batch_step]:\n                g[BaseTrainGenKeys.test_loss]+=loss\n                g[BaseTrainGenKeys.test_loss]/=g[BaseTrainGenKeys.max_batch_step]\n                loss=0\n                yield g\n            else:\n                loss+=g[BaseTrainGenKeys.test_loss]\n    def reduce_mem_gen(self,G):\n        for g in G:\n            torch.cuda.empty_cache()\n            gc.collect()\n            yield g\nconfig=CFG()\n#test\npipeline=TransformerTrainPipeline(train_x,train_y,test_x,test_y,config)\n# pipeline.encoder.embedding.load_state_dict(torch.load(\'/home/rain/exp_env/src/work/translate/model/fasttext_jp.pth\'))\n# pipeline.decoder.embedding.load_state_dict(torch.load(\'/home/rain/exp_env/src/work/translate/model/fasttext_en.pth\'))\npipeline.encoder.load_state_dict(torch.load(\'encoder.pth\'))\npipeline.decoder.load_state_dict(torch.load(\'decoder.pth\'))\n# compile\npipeline.encoder=torch.compile(pipeline.encoder,mode="reduce-overhead")\npipeline.decoder=torch.compile(pipeline.decoder,mode="reduce-overhead")\n# pipeline.encoder.embedding.requires_grad=False\n# pipeline.decoder.embedding.requires_grad=False\n', 'execution_count': 16}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
cfg_to_dict(pipeline.config)
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': 'cfg_to_dict(pipeline.config)', 'execution_count': 17}
[NbConvertApp] msg_type: execute_result
[NbConvertApp] content: {'data': {'text/plain': "{'batch_size': 128,\n 'max_epochs': 1,\n 'is_running': True,\n 'debug': False,\n 'max_len': 256,\n 'lr': 0.0007,\n 'min_lr': 2.79936e-05,\n 'accum_iter': 2,\n 'device': 'cuda',\n 'embedding_dim': 512,\n 'hidden_dim': 512,\n 'num_heads': 12,\n 'num_blocks': 3,\n 'encode_vocab_size': 30004,\n 'decode_vocab_size': 30004,\n 'pad_idx': 30003,\n 'eval_steps': 60}"}, 'metadata': {}, 'execution_count': 17}
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'idle'}
[NbConvertApp] Executing cell:
g=pipeline.train_loop_gen(train_dataset)
g=pipeline.id_mask_split(g)
g=pipeline.forward(g)
g=pipeline.reduce_mem_gen(g)
g=pipeline.run_eval_each_step(g)
# g=[{}]
g=pipeline.batch_gen(g,test_dataset,scale=1,no_skip=True)
g=pipeline.id_mask_split(g)
g=pipeline.test(g)
g=pipeline.reduce_mem_gen(g)
g=pipeline.run_after_test(g)
best=100
for i in g:
    if i[TransformerTrainGenKeys.test_loss]<best:
        best=i[TransformerTrainGenKeys.test_loss]
        torch.save(pipeline.encoder.state_dict(), 'encoder1.pth')
        torch.save(pipeline.decoder.state_dict(), 'decoder1.pth')
    logger.log(i[BaseTrainGenKeys.epoch],i[BaseTrainGenKeys.batch_step],i[BaseTrainGenKeys.max_batch_step],i[BaseTrainGenKeys.train_loss],i.get(BaseTrainGenKeys.test_loss,None))
    logger.write(i)
    pipeline.encoder.train()
    pipeline.decoder.train()
[NbConvertApp] msg_type: status
[NbConvertApp] content: {'execution_state': 'busy'}
[NbConvertApp] msg_type: execute_input
[NbConvertApp] content: {'code': "g=pipeline.train_loop_gen(train_dataset)\ng=pipeline.id_mask_split(g)\ng=pipeline.forward(g)\ng=pipeline.reduce_mem_gen(g)\ng=pipeline.run_eval_each_step(g)\n# g=[{}]\ng=pipeline.batch_gen(g,test_dataset,scale=1,no_skip=True)\ng=pipeline.id_mask_split(g)\ng=pipeline.test(g)\ng=pipeline.reduce_mem_gen(g)\ng=pipeline.run_after_test(g)\nbest=100\nfor i in g:\n    if i[TransformerTrainGenKeys.test_loss]<best:\n        best=i[TransformerTrainGenKeys.test_loss]\n        torch.save(pipeline.encoder.state_dict(), 'encoder1.pth')\n        torch.save(pipeline.decoder.state_dict(), 'decoder1.pth')\n    logger.log(i[BaseTrainGenKeys.epoch],i[BaseTrainGenKeys.batch_step],i[BaseTrainGenKeys.max_batch_step],i[BaseTrainGenKeys.train_loss],i.get(BaseTrainGenKeys.test_loss,None))\n    logger.write(i)\n    pipeline.encoder.train()\n    pipeline.decoder.train()", 'execution_count': 18}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': '/home/rain/exp_env/.venv/lib/python3.11/site-packages/torch/_inductor/lowering.py:1611: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.\n  warnings.warn(\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': "skipping cudagraphs due to ['non-cuda device in graph']\n"}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': "skipping cudagraphs due to ['non-cuda device in graph']\n"}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13921/79140 [========>                                         ] train_loss:3.8281\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13922/79140 [========>                                         ] train_loss:3.8594\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13923/79140 [========>                                         ] train_loss:4.0938\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13924/79140 [========>                                         ] train_loss:3.8438\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13925/79140 [========>                                         ] train_loss:3.7969\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13926/79140 [========>                                         ] train_loss:3.8594\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13927/79140 [========>                                         ] train_loss:4.1875\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13928/79140 [========>                                         ] train_loss:3.7500\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13929/79140 [========>                                         ] train_loss:3.9062\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13930/79140 [========>                                         ] train_loss:3.9688\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13931/79140 [========>                                         ] train_loss:3.7188\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13932/79140 [========>                                         ] train_loss:3.7500\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13933/79140 [========>                                         ] train_loss:3.7344\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13934/79140 [========>                                         ] train_loss:3.9844\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13935/79140 [========>                                         ] train_loss:3.8438\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13936/79140 [========>                                         ] train_loss:4.0000\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13937/79140 [========>                                         ] train_loss:3.7969\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13938/79140 [========>                                         ] train_loss:3.8750\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13939/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13940/79140 [========>                                         ] train_loss:3.5938\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13941/79140 [========>                                         ] train_loss:3.8906\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13942/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13943/79140 [========>                                         ] train_loss:3.8438\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13944/79140 [========>                                         ] train_loss:3.8906\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13945/79140 [========>                                         ] train_loss:3.8125\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13946/79140 [========>                                         ] train_loss:4.0938\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13947/79140 [========>                                         ] train_loss:3.7031\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13948/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13949/79140 [========>                                         ] train_loss:3.6875\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13950/79140 [========>                                         ] train_loss:3.9375\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13951/79140 [========>                                         ] train_loss:3.5625\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13952/79140 [========>                                         ] train_loss:3.6562\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13953/79140 [========>                                         ] train_loss:3.8125\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13954/79140 [========>                                         ] train_loss:3.6094\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13955/79140 [========>                                         ] train_loss:3.9531\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13956/79140 [========>                                         ] train_loss:3.6875\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13957/79140 [========>                                         ] train_loss:3.7969\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13958/79140 [========>                                         ] train_loss:3.7500\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13959/79140 [========>                                         ] train_loss:3.8438\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13960/79140 [========>                                         ] train_loss:3.7969\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13961/79140 [========>                                         ] train_loss:3.9531\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13962/79140 [========>                                         ] train_loss:3.9062\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13963/79140 [========>                                         ] train_loss:3.7500\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13964/79140 [========>                                         ] train_loss:3.8281\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13965/79140 [========>                                         ] train_loss:3.6250\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13966/79140 [========>                                         ] train_loss:3.6406\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13967/79140 [========>                                         ] train_loss:3.5781\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13968/79140 [========>                                         ] train_loss:3.9844\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13969/79140 [========>                                         ] train_loss:3.7031\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13970/79140 [========>                                         ] train_loss:3.6719\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13971/79140 [========>                                         ] train_loss:3.7500\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13972/79140 [========>                                         ] train_loss:3.6875\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13973/79140 [========>                                         ] train_loss:4.0312\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13974/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13975/79140 [========>                                         ] train_loss:3.6875\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13976/79140 [========>                                         ] train_loss:3.9844\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13977/79140 [========>                                         ] train_loss:3.9219\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13978/79140 [========>                                         ] train_loss:3.9688\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13979/79140 [========>                                         ] train_loss:3.8750\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13980/79140 [========>                                         ] train_loss:3.7188\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13981/79140 [========>                                         ] train_loss:4.0000\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13982/79140 [========>                                         ] train_loss:3.9219\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13983/79140 [========>                                         ] train_loss:3.7188\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13984/79140 [========>                                         ] train_loss:3.7344\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13985/79140 [========>                                         ] train_loss:3.7500\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13986/79140 [========>                                         ] train_loss:3.7344\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13987/79140 [========>                                         ] train_loss:3.8750\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13988/79140 [========>                                         ] train_loss:3.6875\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13989/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13990/79140 [========>                                         ] train_loss:4.0000\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13991/79140 [========>                                         ] train_loss:3.5938\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13992/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13993/79140 [========>                                         ] train_loss:3.8594\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13994/79140 [========>                                         ] train_loss:3.7344\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13995/79140 [========>                                         ] train_loss:3.5938\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13996/79140 [========>                                         ] train_loss:3.8438\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13997/79140 [========>                                         ] train_loss:3.8125\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13998/79140 [========>                                         ] train_loss:3.8594\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:13999/79140 [========>                                         ] train_loss:3.9375\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14000/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14001/79140 [========>                                         ] train_loss:4.0312\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14002/79140 [========>                                         ] train_loss:3.8906\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14003/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14004/79140 [========>                                         ] train_loss:3.7031\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14005/79140 [========>                                         ] train_loss:3.7344\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14006/79140 [========>                                         ] train_loss:3.5781\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14007/79140 [========>                                         ] train_loss:3.9688\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14008/79140 [========>                                         ] train_loss:3.8594\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14009/79140 [========>                                         ] train_loss:3.8906\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14010/79140 [========>                                         ] train_loss:3.7656\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14011/79140 [========>                                         ] train_loss:4.0312\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14012/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14013/79140 [========>                                         ] train_loss:3.9375\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14014/79140 [========>                                         ] train_loss:3.9062\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14015/79140 [========>                                         ] train_loss:3.8125\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14016/79140 [========>                                         ] train_loss:3.8281\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14017/79140 [========>                                         ] train_loss:3.5938\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14018/79140 [========>                                         ] train_loss:3.7969\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14019/79140 [========>                                         ] train_loss:3.8281\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14020/79140 [========>                                         ] train_loss:4.0625\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14021/79140 [========>                                         ] train_loss:4.0625\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14022/79140 [========>                                         ] train_loss:3.8750\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14023/79140 [========>                                         ] train_loss:3.8750\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14024/79140 [========>                                         ] train_loss:3.9688\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14025/79140 [========>                                         ] train_loss:3.8906\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14026/79140 [========>                                         ] train_loss:3.5312\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14027/79140 [========>                                         ] train_loss:3.8281\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14028/79140 [========>                                         ] train_loss:3.8906\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14029/79140 [========>                                         ] train_loss:3.8438\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14030/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14031/79140 [========>                                         ] train_loss:3.7500\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14032/79140 [========>                                         ] train_loss:3.8594\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14033/79140 [========>                                         ] train_loss:3.7188\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14034/79140 [========>                                         ] train_loss:3.6719\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14035/79140 [========>                                         ] train_loss:3.9219\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14036/79140 [========>                                         ] train_loss:3.7812\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14037/79140 [========>                                         ] train_loss:4.0312\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14038/79140 [========>                                         ] train_loss:3.7188\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14039/79140 [========>                                         ] train_loss:3.7969\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stdout', 'text': '\x1b[F\x1b[K\x1b[F\x1b[Kepoch:0 batch_step:14040/79140 [========>                                         ] train_loss:3.6562\n'}
[NbConvertApp] msg_type: stream
[NbConvertApp] content: {'name': 'stderr', 'text': "skipping cudagraphs due to ['non-cuda device in graph']\n"}
[NbConvertApp] ERROR | Kernel died while waiting for execute reply.
[NbConvertApp] Destroying zmq context for <jupyter_client.asynchronous.client.AsyncKernelClient object at 0x7fc3f554cfd0>
Traceback (most recent call last):
  File "/home/rain/exp_env/.venv/bin/jupyter-nbconvert", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/jupyter_core/application.py", line 283, in launch_instance
    super().launch_instance(argv=argv, **kwargs)
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/traitlets/config/application.py", line 1075, in launch_instance
    app.start()
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 420, in start
    self.convert_notebooks()
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 597, in convert_notebooks
    self.convert_single_notebook(notebook_filename)
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 563, in convert_single_notebook
    output, resources = self.export_single_notebook(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 487, in export_single_notebook
    output, resources = self.exporter.from_filename(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 201, in from_filename
    return self.from_file(f, resources=resources, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 220, in from_file
    return self.from_notebook_node(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/exporters/notebook.py", line 36, in from_notebook_node
    nb_copy, resources = super().from_notebook_node(nb, resources, **kw)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 154, in from_notebook_node
    nb_copy, resources = self._preprocess(nb_copy, resources)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 353, in _preprocess
    nbc, resc = preprocessor(nbc, resc)
                ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/preprocessors/base.py", line 48, in __call__
    return self.preprocess(nb, resources)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/preprocessors/execute.py", line 103, in preprocess
    self.preprocess_cell(cell, resources, index)
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbconvert/preprocessors/execute.py", line 124, in preprocess_cell
    cell = self.execute_cell(cell, index, store_history=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/jupyter_core/utils/__init__.py", line 165, in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rain/.rye/py/cpython@3.11.6/lib/python3.11/asyncio/base_events.py", line 653, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/rain/exp_env/.venv/lib/python3.11/site-packages/nbclient/client.py", line 1009, in async_execute_cell
    raise DeadKernelError("Kernel died") from None
nbclient.exceptions.DeadKernelError: Kernel died
